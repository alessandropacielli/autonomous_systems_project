{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 2 -- State space dimensions: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessandro/projects/uni/autonomous/.venv/lib64/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "state_space_dimensions = env.observation_space.shape[0]\n",
    "\n",
    "print('Actions: {} -- State space dimensions: {}'.format(num_actions, state_space_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CartPoleDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_space_dimensions, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        #self.fc3 = nn.Linear(128, 64)\n",
    "        self.head = nn.Linear(64, num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        #x = F.leaky_relu(self.fc3(x))\n",
    "        x = F.leaky_relu(self.head(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.position = 0\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.done = []\n",
    "        \n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.states) < self.capacity:\n",
    "            self.states.append(None)\n",
    "            self.actions.append(None)\n",
    "            self.rewards.append(None)\n",
    "            self.next_states.append(None)\n",
    "            self.done.append(None)\n",
    "            \n",
    "        self.states[self.position] = state\n",
    "        self.actions[self.position] = action\n",
    "        self.rewards[self.position] = reward\n",
    "        self.next_states[self.position] = next_state\n",
    "        self.done[self.position] = done\n",
    "        \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(range(len(self.states)), size=batch_size)\n",
    "        state_sample = [self.states[i] for i in indices]\n",
    "        action_sample = [self.actions[i] for i in indices]\n",
    "        reward_sample = [self.rewards[i] for i in indices]\n",
    "        next_state_sample = [self.next_states[i] for i in indices]\n",
    "        done_sample = [self.done[i] for i in indices]\n",
    "        \n",
    "        return state_sample, action_sample, reward_sample, next_state_sample, done_sample\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, env, model, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return model(state).argmax().item()\n",
    "    else:\n",
    "        return env.action_space.sample()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_epsilon(epsilon_start, epsilon_end, epsilon_steps, total_steps):\n",
    "    return epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * total_steps / epsilon_steps)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(policy_net, target_net, optimizer, memory, batch_size, gamma):\n",
    "    state_batch, action_batch, reward_batch, next_state_batch, done_batch = memory.sample(batch_size)\n",
    "        \n",
    "    state_batch = torch.tensor(state_batch).float().view((batch_size, -1))\n",
    "    action_batch = torch.tensor(action_batch, dtype=torch.int64).view((batch_size))\n",
    "    reward_batch = torch.tensor(reward_batch).float().view((batch_size))\n",
    "    \n",
    "    non_final_next_states = torch.tensor([s for s in next_state_batch if s is not None]).float()\n",
    "    non_final_mask = torch.tensor(list(map(lambda s: s is not None, next_state_batch)), dtype=torch.bool)\n",
    "        \n",
    "    state_action_values = policy_net(state_batch)\n",
    "    state_action_values = state_action_values.gather(1, action_batch.reshape((batch_size, 1)))\n",
    "    \n",
    "    next_state_values = torch.zeros(batch_size)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(dim=1)[0].float().detach()\n",
    "        \n",
    "    expected_state_action_values = reward_batch + gamma * next_state_values\n",
    "    \n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "    \n",
    "\n",
    "def train_dqn(env, policy_net, target_net, optimizer, memory, target_update=10, batch_size=32, episodes=100, gamma=0.99, epsilon_start=0.9, epsilon_end=0.05, epsilon_steps=1000000):\n",
    "    \n",
    "    total_rewards = []\n",
    "    total_steps = 0\n",
    "    \n",
    "    epsilon = epsilon_start\n",
    "        \n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        \n",
    "        total_rewards.append(0)\n",
    "        loss = 0\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            env.render()\n",
    "            state_tensor = torch.tensor(state).float()\n",
    "            action = select_action(state_tensor, env, policy_net, epsilon)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if done:\n",
    "                next_state = None\n",
    "            \n",
    "            total_rewards[episode] += reward\n",
    "            \n",
    "            \n",
    "            memory.push(state, action, reward, next_state, done)\n",
    "            \n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if len(memory) >= batch_size:                 \n",
    "                loss = optimize_model(policy_net, target_net, optimizer, memory, batch_size, gamma)\n",
    "        \n",
    "            if total_steps % target_update == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "                \n",
    "            total_steps += 1\n",
    "            epsilon = update_epsilon(epsilon_start, epsilon_end, epsilon_steps, total_steps)\n",
    "            \n",
    "        \n",
    "        print('{}/{} Total steps: {} Episode reward: {} Average reward: {} Loss: {} Epsilon: {}'.format(episode, episodes, total_steps, total_rewards[episode], np.mean(total_rewards), loss, epsilon))\n",
    "\n",
    "            \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1000 Total steps: 20 Episode reward: 20.0 Average reward: 20.0 Loss: 0 Epsilon: 0.8984015989338665\n",
      "1/1000 Total steps: 31 Episode reward: 11.0 Average reward: 15.5 Loss: 0 Epsilon: 0.8975238400309432\n",
      "2/1000 Total steps: 47 Episode reward: 16.0 Average reward: 15.666666666666666 Loss: 0 Epsilon: 0.8962488221731837\n",
      "3/1000 Total steps: 69 Episode reward: 22.0 Average reward: 17.25 Loss: 0 Epsilon: 0.894499000274253\n",
      "4/1000 Total steps: 101 Episode reward: 32.0 Average reward: 20.2 Loss: 0 Epsilon: 0.8919606669727019\n",
      "5/1000 Total steps: 120 Episode reward: 19.0 Average reward: 20.0 Loss: 0 Epsilon: 0.8904573702895444\n",
      "6/1000 Total steps: 142 Episode reward: 22.0 Average reward: 20.285714285714285 Loss: 0.5645555853843689 Epsilon: 0.8887202755797164\n",
      "7/1000 Total steps: 162 Episode reward: 20.0 Average reward: 20.25 Loss: 0.10816188156604767 Epsilon: 0.8871444114180067\n",
      "8/1000 Total steps: 210 Episode reward: 48.0 Average reward: 23.333333333333332 Loss: 0.029264872893691063 Epsilon: 0.8833751716555677\n",
      "9/1000 Total steps: 223 Episode reward: 13.0 Average reward: 22.3 Loss: 0.02344335801899433 Epsilon: 0.8823574455976828\n",
      "10/1000 Total steps: 238 Episode reward: 15.0 Average reward: 21.636363636363637 Loss: 0.017370842397212982 Epsilon: 0.8811847891415016\n",
      "11/1000 Total steps: 276 Episode reward: 38.0 Average reward: 23.0 Loss: 0.006270647048950195 Epsilon: 0.8782219199595279\n",
      "12/1000 Total steps: 294 Episode reward: 18.0 Average reward: 22.615384615384617 Loss: 0.01106784027069807 Epsilon: 0.8768223804670198\n",
      "13/1000 Total steps: 323 Episode reward: 29.0 Average reward: 23.071428571428573 Loss: 0.018872004002332687 Epsilon: 0.8745728589464097\n",
      "14/1000 Total steps: 335 Episode reward: 12.0 Average reward: 22.333333333333332 Loss: 0.008389548398554325 Epsilon: 0.8736439289851223\n",
      "15/1000 Total steps: 349 Episode reward: 14.0 Average reward: 21.8125 Loss: 0.015218083746731281 Epsilon: 0.8725615853019043\n",
      "16/1000 Total steps: 373 Episode reward: 24.0 Average reward: 21.941176470588236 Loss: 0.008723403327167034 Epsilon: 0.8707096606956309\n",
      "17/1000 Total steps: 419 Episode reward: 46.0 Average reward: 23.27777777777778 Loss: 0.010986989364027977 Epsilon: 0.8671725378760404\n",
      "18/1000 Total steps: 430 Episode reward: 11.0 Average reward: 22.63157894736842 Loss: 0.00494134146720171 Epsilon: 0.8663291120536245\n",
      "19/1000 Total steps: 471 Episode reward: 41.0 Average reward: 23.55 Loss: 0.006710932124406099 Epsilon: 0.8631935948967121\n",
      "20/1000 Total steps: 488 Episode reward: 17.0 Average reward: 23.238095238095237 Loss: 0.00785941444337368 Epsilon: 0.8618972679754695\n",
      "21/1000 Total steps: 512 Episode reward: 24.0 Average reward: 23.272727272727273 Loss: 0.005149931646883488 Epsilon: 0.8600709070421015\n",
      "22/1000 Total steps: 537 Episode reward: 25.0 Average reward: 23.347826086956523 Loss: 0.009967825375497341 Epsilon: 0.8581731030179659\n",
      "23/1000 Total steps: 562 Episode reward: 25.0 Average reward: 23.416666666666668 Loss: 0.005262694787234068 Epsilon: 0.8562800375781923\n",
      "24/1000 Total steps: 630 Episode reward: 68.0 Average reward: 25.2 Loss: 0.0034109270200133324 Epsilon: 0.8511547789513066\n",
      "25/1000 Total steps: 665 Episode reward: 35.0 Average reward: 25.576923076923077 Loss: 0.002495982451364398 Epsilon: 0.8485303326850646\n",
      "26/1000 Total steps: 689 Episode reward: 24.0 Average reward: 25.51851851851852 Loss: 0.004822877701371908 Epsilon: 0.846736013930399\n",
      "27/1000 Total steps: 738 Episode reward: 49.0 Average reward: 26.357142857142858 Loss: 0.0037873294204473495 Epsilon: 0.8430859574037821\n",
      "28/1000 Total steps: 771 Episode reward: 33.0 Average reward: 26.586206896551722 Loss: 0.009367714636027813 Epsilon: 0.8406378154003438\n",
      "29/1000 Total steps: 786 Episode reward: 15.0 Average reward: 26.2 Loss: 0.007955101318657398 Epsilon: 0.8395276914783331\n",
      "30/1000 Total steps: 814 Episode reward: 28.0 Average reward: 26.258064516129032 Loss: 0.0037338584661483765 Epsilon: 0.8374599101869519\n",
      "31/1000 Total steps: 824 Episode reward: 10.0 Average reward: 25.75 Loss: 0.008241379633545876 Epsilon: 0.8367228188838407\n",
      "32/1000 Total steps: 839 Episode reward: 15.0 Average reward: 25.424242424242426 Loss: 0.002076891716569662 Epsilon: 0.835618563054435\n",
      "33/1000 Total steps: 853 Episode reward: 14.0 Average reward: 25.08823529411765 Loss: 0.0072248694486916065 Epsilon: 0.8345894176360454\n",
      "34/1000 Total steps: 883 Episode reward: 30.0 Average reward: 25.228571428571428 Loss: 0.003270970657467842 Epsilon: 0.8323889517323421\n",
      "35/1000 Total steps: 906 Episode reward: 23.0 Average reward: 25.166666666666668 Loss: 0.006965483073145151 Epsilon: 0.8307063928278259\n",
      "36/1000 Total steps: 922 Episode reward: 16.0 Average reward: 24.91891891891892 Loss: 0.008709508925676346 Epsilon: 0.8295381974048547\n",
      "37/1000 Total steps: 944 Episode reward: 22.0 Average reward: 24.842105263157894 Loss: 0.007833964191377163 Epsilon: 0.8279349775590263\n",
      "38/1000 Total steps: 955 Episode reward: 11.0 Average reward: 24.487179487179485 Loss: 0.0034600452054291964 Epsilon: 0.827134689322937\n",
      "39/1000 Total steps: 982 Episode reward: 27.0 Average reward: 24.55 Loss: 0.005354594439268112 Epsilon: 0.8251740736839516\n",
      "40/1000 Total steps: 1006 Episode reward: 24.0 Average reward: 24.536585365853657 Loss: 0.0039313966408371925 Epsilon: 0.8234357427386433\n",
      "41/1000 Total steps: 1031 Episode reward: 25.0 Average reward: 24.547619047619047 Loss: 0.003820262383669615 Epsilon: 0.8216294122357223\n",
      "42/1000 Total steps: 1042 Episode reward: 11.0 Average reward: 24.232558139534884 Loss: 0.00571869220584631 Epsilon: 0.8208360563080199\n",
      "43/1000 Total steps: 1066 Episode reward: 24.0 Average reward: 24.227272727272727 Loss: 0.006671997718513012 Epsilon: 0.8191081241209126\n",
      "44/1000 Total steps: 1087 Episode reward: 21.0 Average reward: 24.155555555555555 Loss: 0.005686161573976278 Epsilon: 0.8175995815843115\n",
      "45/1000 Total steps: 1137 Episode reward: 50.0 Average reward: 24.717391304347824 Loss: 0.0022803726606070995 Epsilon: 0.8140205387398373\n",
      "46/1000 Total steps: 1169 Episode reward: 32.0 Average reward: 24.872340425531913 Loss: 0.002362811705097556 Epsilon: 0.8117393249046415\n",
      "47/1000 Total steps: 1185 Episode reward: 16.0 Average reward: 24.6875 Loss: 0.0020126551389694214 Epsilon: 0.8106014525254436\n",
      "48/1000 Total steps: 1224 Episode reward: 39.0 Average reward: 24.979591836734695 Loss: 0.0007162199472077191 Epsilon: 0.8078355039661239\n",
      "49/1000 Total steps: 1243 Episode reward: 19.0 Average reward: 24.86 Loss: 0.0033992633689194918 Epsilon: 0.8064918933428832\n",
      "50/1000 Total steps: 1260 Episode reward: 17.0 Average reward: 24.705882352941178 Loss: 0.00566850695759058 Epsilon: 0.8052918774267329\n",
      "51/1000 Total steps: 1286 Episode reward: 26.0 Average reward: 24.73076923076923 Loss: 0.004025809466838837 Epsilon: 0.8034605003672762\n",
      "52/1000 Total steps: 1299 Episode reward: 13.0 Average reward: 24.50943396226415 Loss: 0.002544606104493141 Epsilon: 0.8025465958834216\n",
      "53/1000 Total steps: 1332 Episode reward: 33.0 Average reward: 24.666666666666668 Loss: 0.0017902330728247762 Epsilon: 0.8002320132787871\n",
      "54/1000 Total steps: 1346 Episode reward: 14.0 Average reward: 24.472727272727273 Loss: 0.010861381888389587 Epsilon: 0.7992523743674426\n",
      "55/1000 Total steps: 1376 Episode reward: 30.0 Average reward: 24.571428571428573 Loss: 0.0025777516420930624 Epsilon: 0.7971577607357477\n",
      "56/1000 Total steps: 1403 Episode reward: 27.0 Average reward: 24.614035087719298 Loss: 0.0028507125098258257 Epsilon: 0.7952779736363159\n",
      "57/1000 Total steps: 1422 Episode reward: 19.0 Average reward: 24.517241379310345 Loss: 0.0009458356071263552 Epsilon: 0.7939581996687082\n",
      "58/1000 Total steps: 1463 Episode reward: 41.0 Average reward: 24.796610169491526 Loss: 0.004363343585282564 Epsilon: 0.7911187958055165\n",
      "59/1000 Total steps: 1480 Episode reward: 17.0 Average reward: 24.666666666666668 Loss: 0.0034667965956032276 Epsilon: 0.7899448919536364\n",
      "60/1000 Total steps: 1539 Episode reward: 59.0 Average reward: 25.229508196721312 Loss: 0.0025327270850539207 Epsilon: 0.7858862020000492\n",
      "61/1000 Total steps: 1560 Episode reward: 21.0 Average reward: 25.161290322580644 Loss: 0.003061413997784257 Epsilon: 0.7844473522968148\n",
      "62/1000 Total steps: 1573 Episode reward: 13.0 Average reward: 24.96825396825397 Loss: 0.0015719709917902946 Epsilon: 0.7835581488463013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/1000 Total steps: 1600 Episode reward: 27.0 Average reward: 25.0 Loss: 0.004694642033427954 Epsilon: 0.7817150311729691\n",
      "64/1000 Total steps: 1626 Episode reward: 26.0 Average reward: 25.015384615384615 Loss: 0.004895329475402832 Epsilon: 0.7799448742930516\n",
      "65/1000 Total steps: 1648 Episode reward: 22.0 Average reward: 24.96969696969697 Loss: 0.000617132696788758 Epsilon: 0.7784506398301905\n",
      "66/1000 Total steps: 1706 Episode reward: 58.0 Average reward: 25.46268656716418 Loss: 0.002474992536008358 Epsilon: 0.7745270156285804\n",
      "67/1000 Total steps: 1734 Episode reward: 28.0 Average reward: 25.5 Loss: 0.0018141667824238539 Epsilon: 0.7726409816645787\n",
      "68/1000 Total steps: 1772 Episode reward: 38.0 Average reward: 25.681159420289855 Loss: 0.0019884021021425724 Epsilon: 0.7700897962564545\n",
      "69/1000 Total steps: 1793 Episode reward: 21.0 Average reward: 25.614285714285714 Loss: 0.0018971215467900038 Epsilon: 0.7686840841985758\n",
      "70/1000 Total steps: 1839 Episode reward: 46.0 Average reward: 25.901408450704224 Loss: 0.0016658378299325705 Epsilon: 0.7656152012534977\n",
      "71/1000 Total steps: 1857 Episode reward: 18.0 Average reward: 25.791666666666668 Loss: 0.0043871100060641766 Epsilon: 0.7644181715411804\n",
      "72/1000 Total steps: 1903 Episode reward: 46.0 Average reward: 26.068493150684933 Loss: 0.002713285619392991 Epsilon: 0.7613688667300955\n",
      "73/1000 Total steps: 1954 Episode reward: 51.0 Average reward: 26.405405405405407 Loss: 0.001119578955695033 Epsilon: 0.7580044720086341\n",
      "74/1000 Total steps: 1976 Episode reward: 22.0 Average reward: 26.346666666666668 Loss: 0.0010318448767066002 Epsilon: 0.7565584533739407\n",
      "75/1000 Total steps: 1986 Episode reward: 10.0 Average reward: 26.13157894736842 Loss: 0.0028773052617907524 Epsilon: 0.7559022230903943\n",
      "76/1000 Total steps: 2008 Episode reward: 22.0 Average reward: 26.07792207792208 Loss: 0.003216938814148307 Epsilon: 0.7544608243196075\n",
      "77/1000 Total steps: 2039 Episode reward: 31.0 Average reward: 26.141025641025642 Loss: 0.005964314565062523 Epsilon: 0.7524351372014874\n",
      "78/1000 Total steps: 2070 Episode reward: 31.0 Average reward: 26.20253164556962 Loss: 0.0035026425030082464 Epsilon: 0.7504157199900569\n",
      "79/1000 Total steps: 2108 Episode reward: 38.0 Average reward: 26.35 Loss: 0.002729111583903432 Epsilon: 0.7479488303129709\n",
      "80/1000 Total steps: 2135 Episode reward: 27.0 Average reward: 26.358024691358025 Loss: 0.002687986707314849 Epsilon: 0.7462017281204503\n",
      "81/1000 Total steps: 2147 Episode reward: 12.0 Average reward: 26.182926829268293 Loss: 0.0011090772459283471 Epsilon: 0.7454267511258997\n",
      "82/1000 Total steps: 2162 Episode reward: 15.0 Average reward: 26.048192771084338 Loss: 0.0006258759531192482 Epsilon: 0.7444593367413894\n",
      "83/1000 Total steps: 2202 Episode reward: 40.0 Average reward: 26.214285714285715 Loss: 0.005471695680171251 Epsilon: 0.7418866482017535\n",
      "84/1000 Total steps: 2218 Episode reward: 16.0 Average reward: 26.094117647058823 Loss: 0.004434250760823488 Epsilon: 0.7408604507415211\n",
      "85/1000 Total steps: 2235 Episode reward: 17.0 Average reward: 25.988372093023255 Loss: 0.0007318559801205993 Epsilon: 0.7397719134940769\n",
      "86/1000 Total steps: 2246 Episode reward: 11.0 Average reward: 25.816091954022987 Loss: 0.0034328054171055555 Epsilon: 0.7390685513093573\n",
      "87/1000 Total steps: 2262 Episode reward: 16.0 Average reward: 25.704545454545453 Loss: 0.0031982886139303446 Epsilon: 0.7380468591989117\n",
      "88/1000 Total steps: 2307 Episode reward: 45.0 Average reward: 25.921348314606742 Loss: 0.0009593694703653455 Epsilon: 0.7351820988775212\n",
      "89/1000 Total steps: 2345 Episode reward: 38.0 Average reward: 26.055555555555557 Loss: 0.002432753797620535 Epsilon: 0.7327729871131028\n",
      "90/1000 Total steps: 2361 Episode reward: 16.0 Average reward: 25.945054945054945 Loss: 0.004590651486068964 Epsilon: 0.731761359851345\n",
      "91/1000 Total steps: 2379 Episode reward: 18.0 Average reward: 25.858695652173914 Loss: 0.0036239249166101217 Epsilon: 0.7306252122432197\n",
      "92/1000 Total steps: 2394 Episode reward: 15.0 Average reward: 25.741935483870968 Loss: 0.001639972673729062 Epsilon: 0.729679983523625\n",
      "93/1000 Total steps: 2422 Episode reward: 28.0 Average reward: 25.76595744680851 Loss: 0.004240301437675953 Epsilon: 0.7279193456131168\n",
      "94/1000 Total steps: 2431 Episode reward: 9.0 Average reward: 25.589473684210525 Loss: 0.003150657285004854 Epsilon: 0.727354472433125\n",
      "95/1000 Total steps: 2462 Episode reward: 31.0 Average reward: 25.645833333333332 Loss: 0.0026696582790464163 Epsilon: 0.7254126848943153\n",
      "96/1000 Total steps: 2476 Episode reward: 14.0 Average reward: 25.52577319587629 Loss: 0.0011050953762605786 Epsilon: 0.7245377197539725\n",
      "97/1000 Total steps: 2492 Episode reward: 16.0 Average reward: 25.428571428571427 Loss: 0.0021617859601974487 Epsilon: 0.7235392583844669\n",
      "98/1000 Total steps: 2511 Episode reward: 19.0 Average reward: 25.363636363636363 Loss: 0.001689633005298674 Epsilon: 0.7223556585694269\n",
      "99/1000 Total steps: 2523 Episode reward: 12.0 Average reward: 25.23 Loss: 0.004588864743709564 Epsilon: 0.7216092796960331\n",
      "100/1000 Total steps: 2543 Episode reward: 20.0 Average reward: 25.178217821782177 Loss: 0.002037493512034416 Epsilon: 0.7203673035268022\n",
      "101/1000 Total steps: 2582 Episode reward: 39.0 Average reward: 25.313725490196077 Loss: 0.0017203122843056917 Epsilon: 0.717952582809105\n",
      "102/1000 Total steps: 2597 Episode reward: 15.0 Average reward: 25.21359223300971 Loss: 0.00255831447429955 Epsilon: 0.717026348784079\n",
      "103/1000 Total steps: 2611 Episode reward: 14.0 Average reward: 25.10576923076923 Loss: 0.0024276897311210632 Epsilon: 0.7161631162995151\n",
      "104/1000 Total steps: 2634 Episode reward: 23.0 Average reward: 25.085714285714285 Loss: 0.0016569697763770819 Epsilon: 0.7147475696347109\n",
      "105/1000 Total steps: 2649 Episode reward: 15.0 Average reward: 24.99056603773585 Loss: 0.00460246903821826 Epsilon: 0.7138261395256088\n",
      "106/1000 Total steps: 2677 Episode reward: 28.0 Average reward: 25.018691588785046 Loss: 0.0013104937970638275 Epsilon: 0.71210983028919\n",
      "107/1000 Total steps: 2732 Episode reward: 55.0 Average reward: 25.296296296296298 Loss: 0.0019105878891423345 Epsilon: 0.7087524674337996\n",
      "108/1000 Total steps: 2750 Episode reward: 18.0 Average reward: 25.229357798165136 Loss: 0.0016501953359693289 Epsilon: 0.7076576985799748\n",
      "109/1000 Total steps: 2800 Episode reward: 50.0 Average reward: 25.454545454545453 Loss: 0.00130008056294173 Epsilon: 0.7046269931645804\n",
      "110/1000 Total steps: 2829 Episode reward: 29.0 Average reward: 25.486486486486488 Loss: 0.001338259200565517 Epsilon: 0.7028761148849823\n",
      "111/1000 Total steps: 2840 Episode reward: 11.0 Average reward: 25.357142857142858 Loss: 0.0028020006138831377 Epsilon: 0.702213315764957\n",
      "112/1000 Total steps: 2872 Episode reward: 32.0 Average reward: 25.41592920353982 Loss: 0.0015660622157156467 Epsilon: 0.7002893132004276\n",
      "113/1000 Total steps: 2911 Episode reward: 39.0 Average reward: 25.535087719298247 Loss: 0.0019805326592177153 Epsilon: 0.6979527441501945\n",
      "114/1000 Total steps: 2921 Episode reward: 10.0 Average reward: 25.4 Loss: 0.0030224774964153767 Epsilon: 0.6973550902827823\n",
      "115/1000 Total steps: 2959 Episode reward: 38.0 Average reward: 25.50862068965517 Loss: 0.0016230101464316249 Epsilon: 0.6950894483856341\n",
      "116/1000 Total steps: 2987 Episode reward: 28.0 Average reward: 25.52991452991453 Loss: 0.0027813122142106295 Epsilon: 0.6934255285050813\n",
      "117/1000 Total steps: 2999 Episode reward: 12.0 Average reward: 25.415254237288135 Loss: 0.003265934996306896 Epsilon: 0.6927138449664005\n",
      "118/1000 Total steps: 3025 Episode reward: 26.0 Average reward: 25.42016806722689 Loss: 0.00048605696065351367 Epsilon: 0.6911747906071554\n",
      "119/1000 Total steps: 3038 Episode reward: 13.0 Average reward: 25.316666666666666 Loss: 0.0009184930240735412 Epsilon: 0.6904067627056659\n",
      "120/1000 Total steps: 3061 Episode reward: 23.0 Average reward: 25.297520661157026 Loss: 0.0011604101164266467 Epsilon: 0.689050387580772\n",
      "121/1000 Total steps: 3122 Episode reward: 61.0 Average reward: 25.59016393442623 Loss: 0.0006025768234394491 Epsilon: 0.6854681172490573\n",
      "122/1000 Total steps: 3169 Episode reward: 47.0 Average reward: 25.764227642276424 Loss: 0.0027817110531032085 Epsilon: 0.6827228734743915\n",
      "123/1000 Total steps: 3207 Episode reward: 38.0 Average reward: 25.862903225806452 Loss: 0.0014756997115910053 Epsilon: 0.6805127284901993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/1000 Total steps: 3227 Episode reward: 20.0 Average reward: 25.816 Loss: 0.0023483755066990852 Epsilon: 0.6793528632850458\n",
      "125/1000 Total steps: 3263 Episode reward: 36.0 Average reward: 25.896825396825395 Loss: 0.0016700892010703683 Epsilon: 0.6772709426827774\n",
      "126/1000 Total steps: 3304 Episode reward: 41.0 Average reward: 26.015748031496063 Loss: 0.0028012029360979795 Epsilon: 0.6749089771558274\n",
      "127/1000 Total steps: 3316 Episode reward: 12.0 Average reward: 25.90625 Loss: 0.0012844223529100418 Epsilon: 0.6742195001521799\n",
      "128/1000 Total steps: 3371 Episode reward: 55.0 Average reward: 26.131782945736433 Loss: 0.0015378404641523957 Epsilon: 0.671069962070524\n",
      "129/1000 Total steps: 3401 Episode reward: 30.0 Average reward: 26.161538461538463 Loss: 0.001965863164514303 Epsilon: 0.6693593194312532\n",
      "130/1000 Total steps: 3427 Episode reward: 26.0 Average reward: 26.16030534351145 Loss: 0.0008754581795074046 Epsilon: 0.6678809079684719\n",
      "131/1000 Total steps: 3449 Episode reward: 22.0 Average reward: 26.12878787878788 Loss: 0.0004759959992952645 Epsilon: 0.6666329432354933\n",
      "132/1000 Total steps: 3531 Episode reward: 82.0 Average reward: 26.548872180451127 Loss: 0.0011999347480013967 Epsilon: 0.6620055513365382\n",
      "133/1000 Total steps: 3561 Episode reward: 30.0 Average reward: 26.574626865671643 Loss: 0.0010943576926365495 Epsilon: 0.6603220611803802\n",
      "134/1000 Total steps: 3607 Episode reward: 46.0 Average reward: 26.71851851851852 Loss: 0.0006163258804008365 Epsilon: 0.6577504988268835\n",
      "135/1000 Total steps: 3653 Episode reward: 46.0 Average reward: 26.860294117647058 Loss: 0.0014956895029172301 Epsilon: 0.6551907384947528\n",
      "136/1000 Total steps: 3678 Episode reward: 25.0 Average reward: 26.846715328467152 Loss: 0.0017434227047488093 Epsilon: 0.6538044951746678\n",
      "137/1000 Total steps: 3723 Episode reward: 45.0 Average reward: 26.97826086956522 Loss: 0.0038830817211419344 Epsilon: 0.6513179738154433\n",
      "138/1000 Total steps: 3771 Episode reward: 48.0 Average reward: 27.1294964028777 Loss: 0.0005373304011300206 Epsilon: 0.6486779885744773\n",
      "139/1000 Total steps: 3782 Episode reward: 11.0 Average reward: 27.014285714285716 Loss: 0.0020486037246882915 Epsilon: 0.6480747746155469\n",
      "140/1000 Total steps: 3799 Episode reward: 17.0 Average reward: 26.94326241134752 Loss: 0.0009196700993925333 Epsilon: 0.6471438390181585\n",
      "141/1000 Total steps: 3844 Episode reward: 45.0 Average reward: 27.070422535211268 Loss: 0.0006529164966195822 Epsilon: 0.6446872232735398\n",
      "142/1000 Total steps: 3858 Episode reward: 14.0 Average reward: 26.97902097902098 Loss: 0.0008313182042911649 Epsilon: 0.6439251947054193\n",
      "143/1000 Total steps: 3910 Episode reward: 52.0 Average reward: 27.15277777777778 Loss: 0.0011910944012925029 Epsilon: 0.6411041248314313\n",
      "144/1000 Total steps: 3970 Episode reward: 60.0 Average reward: 27.379310344827587 Loss: 0.0014139554696157575 Epsilon: 0.6378672205061258\n",
      "145/1000 Total steps: 3989 Episode reward: 19.0 Average reward: 27.32191780821918 Loss: 0.0011113949585705996 Epsilon: 0.6368462430229174\n",
      "146/1000 Total steps: 4005 Episode reward: 16.0 Average reward: 27.244897959183675 Loss: 0.0007764931069687009 Epsilon: 0.6359879758309311\n",
      "147/1000 Total steps: 4029 Episode reward: 24.0 Average reward: 27.222972972972972 Loss: 0.0008282447815872729 Epsilon: 0.6347031471001316\n",
      "148/1000 Total steps: 4049 Episode reward: 20.0 Average reward: 27.174496644295303 Loss: 0.0011049844324588776 Epsilon: 0.6336348094996443\n",
      "149/1000 Total steps: 4107 Episode reward: 58.0 Average reward: 27.38 Loss: 0.001843753969296813 Epsilon: 0.6305486860140825\n",
      "150/1000 Total steps: 4127 Episode reward: 20.0 Average reward: 27.33112582781457 Loss: 0.0004790648235939443 Epsilon: 0.6294886490323817\n",
      "151/1000 Total steps: 4144 Episode reward: 17.0 Average reward: 27.263157894736842 Loss: 0.0015572670381516218 Epsilon: 0.6285892830067458\n",
      "152/1000 Total steps: 4164 Episode reward: 20.0 Average reward: 27.215686274509803 Loss: 0.0011312367860227823 Epsilon: 0.6275331609148649\n",
      "153/1000 Total steps: 4179 Episode reward: 15.0 Average reward: 27.136363636363637 Loss: 0.0004206163575872779 Epsilon: 0.6267424543516724\n",
      "154/1000 Total steps: 4199 Episode reward: 20.0 Average reward: 27.09032258064516 Loss: 0.00043224371620453894 Epsilon: 0.6256900222259055\n",
      "155/1000 Total steps: 4215 Episode reward: 16.0 Average reward: 27.01923076923077 Loss: 0.0014612410450354218 Epsilon: 0.624849590714845\n",
      "156/1000 Total steps: 4244 Episode reward: 29.0 Average reward: 27.03184713375796 Loss: 0.0022009697277098894 Epsilon: 0.6233297317624207\n",
      "157/1000 Total steps: 4336 Episode reward: 92.0 Average reward: 27.443037974683545 Loss: 0.0010421788319945335 Epsilon: 0.6185371777819496\n",
      "158/1000 Total steps: 4359 Episode reward: 23.0 Average reward: 27.41509433962264 Loss: 0.0014426902635022998 Epsilon: 0.6173459127529837\n",
      "159/1000 Total steps: 4375 Episode reward: 16.0 Average reward: 27.34375 Loss: 0.0006951004615984857 Epsilon: 0.6165188211423136\n",
      "160/1000 Total steps: 4437 Episode reward: 62.0 Average reward: 27.559006211180126 Loss: 0.0030302912928164005 Epsilon: 0.613326311457919\n",
      "161/1000 Total steps: 4493 Episode reward: 56.0 Average reward: 27.734567901234566 Loss: 0.0017954114591702819 Epsilon: 0.6104597180666105\n",
      "162/1000 Total steps: 4507 Episode reward: 14.0 Average reward: 27.650306748466257 Loss: 0.0011634128168225288 Epsilon: 0.6097455744784723\n",
      "163/1000 Total steps: 4522 Episode reward: 15.0 Average reward: 27.573170731707318 Loss: 0.0012931758537888527 Epsilon: 0.6089815292939016\n",
      "164/1000 Total steps: 4609 Episode reward: 87.0 Average reward: 27.933333333333334 Loss: 0.0016174237243831158 Epsilon: 0.6045725966553299\n",
      "165/1000 Total steps: 4639 Episode reward: 30.0 Average reward: 27.94578313253012 Loss: 0.0008100953418761492 Epsilon: 0.6030611471731742\n",
      "166/1000 Total steps: 4699 Episode reward: 60.0 Average reward: 28.137724550898202 Loss: 0.0013866206863895059 Epsilon: 0.6000518173077156\n",
      "167/1000 Total steps: 4735 Episode reward: 36.0 Average reward: 28.18452380952381 Loss: 0.001691085984930396 Epsilon: 0.5982548672162782\n",
      "168/1000 Total steps: 4748 Episode reward: 13.0 Average reward: 28.09467455621302 Loss: 0.0018276909831911325 Epsilon: 0.5976075567318747\n",
      "169/1000 Total steps: 4770 Episode reward: 22.0 Average reward: 28.058823529411764 Loss: 0.0007316170958802104 Epsilon: 0.5965140234347499\n",
      "170/1000 Total steps: 4801 Episode reward: 31.0 Average reward: 28.076023391812864 Loss: 0.0005828926805406809 Epsilon: 0.5949772132486192\n",
      "171/1000 Total steps: 4818 Episode reward: 17.0 Average reward: 28.011627906976745 Loss: 0.0030761766247451305 Epsilon: 0.5941364668230381\n",
      "172/1000 Total steps: 4832 Episode reward: 14.0 Average reward: 27.930635838150287 Loss: 0.001062273164279759 Epsilon: 0.5934451597973174\n",
      "173/1000 Total steps: 4848 Episode reward: 16.0 Average reward: 27.862068965517242 Loss: 0.0007048082770779729 Epsilon: 0.5926562788147223\n",
      "174/1000 Total steps: 4878 Episode reward: 30.0 Average reward: 27.874285714285715 Loss: 0.0010900036431849003 Epsilon: 0.5911805247162413\n",
      "175/1000 Total steps: 4916 Episode reward: 38.0 Average reward: 27.931818181818183 Loss: 0.0006735223578289151 Epsilon: 0.5893175805579626\n",
      "176/1000 Total steps: 4942 Episode reward: 26.0 Average reward: 27.92090395480226 Loss: 0.000720792158972472 Epsilon: 0.5880470073094911\n",
      "177/1000 Total steps: 4970 Episode reward: 28.0 Average reward: 27.921348314606742 Loss: 0.0033330372534692287 Epsilon: 0.586682387048941\n",
      "178/1000 Total steps: 4994 Episode reward: 24.0 Average reward: 27.899441340782122 Loss: 0.001003831042908132 Epsilon: 0.5855157498446545\n",
      "179/1000 Total steps: 5023 Episode reward: 29.0 Average reward: 27.905555555555555 Loss: 0.0008291315170936286 Epsilon: 0.5841097937917226\n",
      "180/1000 Total steps: 5039 Episode reward: 16.0 Average reward: 27.83977900552486 Loss: 0.002610744908452034 Epsilon: 0.5833358374518384\n",
      "181/1000 Total steps: 5053 Episode reward: 14.0 Average reward: 27.763736263736263 Loss: 0.0010662581771612167 Epsilon: 0.5826596407275583\n",
      "182/1000 Total steps: 5127 Episode reward: 74.0 Average reward: 28.016393442622952 Loss: 0.0031554766464978456 Epsilon: 0.5791011420698087\n",
      "183/1000 Total steps: 5139 Episode reward: 12.0 Average reward: 27.929347826086957 Loss: 0.001924701500684023 Epsilon: 0.5785265655142074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/1000 Total steps: 5167 Episode reward: 28.0 Average reward: 27.92972972972973 Loss: 0.00249171513132751 Epsilon: 0.5771885652053602\n",
      "185/1000 Total steps: 5184 Episode reward: 17.0 Average reward: 27.870967741935484 Loss: 0.0005824746331200004 Epsilon: 0.5763780337914158\n",
      "186/1000 Total steps: 5224 Episode reward: 40.0 Average reward: 27.93582887700535 Loss: 0.0018738191574811935 Epsilon: 0.5744763276042322\n",
      "187/1000 Total steps: 5282 Episode reward: 58.0 Average reward: 28.095744680851062 Loss: 0.0004663001745939255 Epsilon: 0.571732330188967\n",
      "188/1000 Total steps: 5295 Episode reward: 13.0 Average reward: 28.015873015873016 Loss: 0.002071532653644681 Epsilon: 0.5711194766008638\n",
      "189/1000 Total steps: 5366 Episode reward: 71.0 Average reward: 28.242105263157896 Loss: 0.0011119355913251638 Epsilon: 0.5677863748300772\n",
      "190/1000 Total steps: 5380 Episode reward: 14.0 Average reward: 28.167539267015705 Loss: 0.00139649398624897 Epsilon: 0.567131932122103\n",
      "191/1000 Total steps: 5393 Episode reward: 13.0 Average reward: 28.088541666666668 Loss: 0.000935302407015115 Epsilon: 0.5665250551658344\n",
      "192/1000 Total steps: 5408 Episode reward: 15.0 Average reward: 28.020725388601036 Loss: 0.001004861667752266 Epsilon: 0.5658257921614508\n",
      "193/1000 Total steps: 5430 Episode reward: 22.0 Average reward: 27.989690721649485 Loss: 0.0009771636687219143 Epsilon: 0.5648021018908815\n",
      "194/1000 Total steps: 5443 Episode reward: 13.0 Average reward: 27.912820512820513 Loss: 0.0005764641100540757 Epsilon: 0.5641982517460596\n",
      "195/1000 Total steps: 5465 Episode reward: 22.0 Average reward: 27.882653061224488 Loss: 0.0011899590026587248 Epsilon: 0.5631781381286433\n",
      "196/1000 Total steps: 5502 Episode reward: 37.0 Average reward: 27.928934010152282 Loss: 0.0015719396760687232 Epsilon: 0.5614675455653101\n",
      "197/1000 Total steps: 5521 Episode reward: 19.0 Average reward: 27.883838383838384 Loss: 0.00228154007345438 Epsilon: 0.5605915896503719\n",
      "198/1000 Total steps: 5535 Episode reward: 14.0 Average reward: 27.814070351758794 Loss: 0.000562418601475656 Epsilon: 0.5599472125940491\n",
      "199/1000 Total steps: 5551 Episode reward: 16.0 Average reward: 27.755 Loss: 0.0010420733597129583 Epsilon: 0.5592118854724656\n",
      "200/1000 Total steps: 5572 Episode reward: 21.0 Average reward: 27.72139303482587 Loss: 0.0020284035708755255 Epsilon: 0.5582485523667593\n",
      "201/1000 Total steps: 5589 Episode reward: 17.0 Average reward: 27.668316831683168 Loss: 0.0026960375253111124 Epsilon: 0.5574701916218243\n",
      "202/1000 Total steps: 5612 Episode reward: 23.0 Average reward: 27.645320197044335 Loss: 0.0014944719150662422 Epsilon: 0.5564192192626107\n",
      "203/1000 Total steps: 5672 Episode reward: 60.0 Average reward: 27.80392156862745 Loss: 0.0006032591336406767 Epsilon: 0.553688903086507\n",
      "204/1000 Total steps: 5694 Episode reward: 22.0 Average reward: 27.775609756097563 Loss: 0.0006098169251345098 Epsilon: 0.5526918846221582\n",
      "205/1000 Total steps: 5721 Episode reward: 27.0 Average reward: 27.771844660194176 Loss: 0.0017377684125676751 Epsilon: 0.551471265111544\n",
      "206/1000 Total steps: 5761 Episode reward: 40.0 Average reward: 27.830917874396135 Loss: 0.0007465947419404984 Epsilon: 0.5496689870103371\n",
      "207/1000 Total steps: 5828 Episode reward: 67.0 Average reward: 28.01923076923077 Loss: 0.0009731531608849764 Epsilon: 0.546666275114854\n",
      "208/1000 Total steps: 5845 Episode reward: 17.0 Average reward: 27.966507177033492 Loss: 0.0012017538538202643 Epsilon: 0.5459075875143364\n",
      "209/1000 Total steps: 5862 Episode reward: 17.0 Average reward: 27.914285714285715 Loss: 0.0009055424015969038 Epsilon: 0.5451501885870571\n",
      "210/1000 Total steps: 5887 Episode reward: 25.0 Average reward: 27.900473933649288 Loss: 0.0012797318631783128 Epsilon: 0.5440387030514077\n",
      "211/1000 Total steps: 5921 Episode reward: 34.0 Average reward: 27.92924528301887 Loss: 0.001238799188286066 Epsilon: 0.5425315350984578\n",
      "212/1000 Total steps: 5939 Episode reward: 18.0 Average reward: 27.88262910798122 Loss: 0.0015868782065808773 Epsilon: 0.5417356948064204\n",
      "213/1000 Total steps: 5962 Episode reward: 23.0 Average reward: 27.85981308411215 Loss: 0.0006909726653248072 Epsilon: 0.5407208702040268\n",
      "214/1000 Total steps: 6025 Episode reward: 63.0 Average reward: 28.023255813953487 Loss: 0.000529350945726037 Epsilon: 0.53795305648948\n",
      "215/1000 Total steps: 6045 Episode reward: 20.0 Average reward: 27.98611111111111 Loss: 0.0015644419472664595 Epsilon: 0.5370780256989685\n",
      "216/1000 Total steps: 6069 Episode reward: 24.0 Average reward: 27.967741935483872 Loss: 0.00047468586126342416 Epsilon: 0.5360302962155811\n",
      "217/1000 Total steps: 6103 Episode reward: 34.0 Average reward: 27.995412844036696 Loss: 0.0005557658150792122 Epsilon: 0.5345503106096974\n",
      "218/1000 Total steps: 6141 Episode reward: 38.0 Average reward: 28.041095890410958 Loss: 0.0017601201543584466 Epsilon: 0.5329021529122882\n",
      "219/1000 Total steps: 6157 Episode reward: 16.0 Average reward: 27.986363636363638 Loss: 0.0015187516110017896 Epsilon: 0.5322100632869745\n",
      "220/1000 Total steps: 6181 Episode reward: 24.0 Average reward: 27.968325791855204 Loss: 0.00036700651980936527 Epsilon: 0.5311740029048533\n",
      "221/1000 Total steps: 6209 Episode reward: 28.0 Average reward: 27.96846846846847 Loss: 0.0006927705835551023 Epsilon: 0.5299684043223928\n",
      "222/1000 Total steps: 6222 Episode reward: 13.0 Average reward: 27.90134529147982 Loss: 0.0006300430395640433 Epsilon: 0.5294098085626864\n",
      "223/1000 Total steps: 6247 Episode reward: 25.0 Average reward: 27.888392857142858 Loss: 0.0008002339163795114 Epsilon: 0.5283376248293753\n",
      "224/1000 Total steps: 6299 Episode reward: 52.0 Average reward: 27.995555555555555 Loss: 0.0007519167847931385 Epsilon: 0.5261160502800365\n",
      "225/1000 Total steps: 6325 Episode reward: 26.0 Average reward: 27.986725663716815 Loss: 0.0014247752260416746 Epsilon: 0.5250095875741334\n",
      "226/1000 Total steps: 6354 Episode reward: 29.0 Average reward: 27.991189427312776 Loss: 0.0014428865397349 Epsilon: 0.5237788452091428\n",
      "227/1000 Total steps: 6402 Episode reward: 48.0 Average reward: 28.07894736842105 Loss: 0.0008539063856005669 Epsilon: 0.5217495808827083\n",
      "228/1000 Total steps: 6445 Episode reward: 43.0 Average reward: 28.14410480349345 Loss: 0.0005440806271508336 Epsilon: 0.5199399511771167\n",
      "229/1000 Total steps: 6498 Episode reward: 53.0 Average reward: 28.252173913043478 Loss: 0.0010608192533254623 Epsilon: 0.517720157086384\n",
      "230/1000 Total steps: 6534 Episode reward: 36.0 Average reward: 28.285714285714285 Loss: 0.0005730509874410927 Epsilon: 0.5162190681022202\n",
      "231/1000 Total steps: 6558 Episode reward: 24.0 Average reward: 28.267241379310345 Loss: 0.0017344753723591566 Epsilon: 0.5152213400912974\n",
      "232/1000 Total steps: 6604 Episode reward: 46.0 Average reward: 28.34334763948498 Loss: 0.0003091650432907045 Epsilon: 0.5133157082403974\n",
      "233/1000 Total steps: 6624 Episode reward: 20.0 Average reward: 28.307692307692307 Loss: 0.0004125329141970724 Epsilon: 0.5124899029045209\n",
      "234/1000 Total steps: 6677 Episode reward: 53.0 Average reward: 28.41276595744681 Loss: 0.0009116876753978431 Epsilon: 0.5103094896183172\n",
      "235/1000 Total steps: 6698 Episode reward: 21.0 Average reward: 28.38135593220339 Loss: 0.0010375428246334195 Epsilon: 0.509448743789563\n",
      "236/1000 Total steps: 6730 Episode reward: 32.0 Average reward: 28.39662447257384 Loss: 0.0014493681956082582 Epsilon: 0.5081406019526562\n",
      "237/1000 Total steps: 6758 Episode reward: 28.0 Average reward: 28.394957983193276 Loss: 0.0025073308497667313 Epsilon: 0.5069994066861427\n",
      "238/1000 Total steps: 6787 Episode reward: 29.0 Average reward: 28.397489539748953 Loss: 0.0018335964996367693 Epsilon: 0.505820818186072\n",
      "239/1000 Total steps: 6812 Episode reward: 25.0 Average reward: 28.383333333333333 Loss: 0.0010352530516684055 Epsilon: 0.5048075332744988\n",
      "240/1000 Total steps: 6836 Episode reward: 24.0 Average reward: 28.365145228215766 Loss: 0.0005424003466032445 Epsilon: 0.5038371601082187\n",
      "241/1000 Total steps: 6882 Episode reward: 46.0 Average reward: 28.43801652892562 Loss: 0.0008132210350595415 Epsilon: 0.5019837752250862\n",
      "242/1000 Total steps: 6904 Episode reward: 22.0 Average reward: 28.411522633744855 Loss: 0.001624454976990819 Epsilon: 0.5011003830073321\n",
      "243/1000 Total steps: 6937 Episode reward: 33.0 Average reward: 28.43032786885246 Loss: 0.0013312280643731356 Epsilon: 0.49977893333458323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/1000 Total steps: 6999 Episode reward: 62.0 Average reward: 28.56734693877551 Loss: 0.0016017527086660266 Epsilon: 0.49730797184383835\n",
      "245/1000 Total steps: 7017 Episode reward: 18.0 Average reward: 28.524390243902438 Loss: 0.00048610856174491346 Epsilon: 0.49659346074742416\n",
      "246/1000 Total steps: 7055 Episode reward: 38.0 Average reward: 28.562753036437247 Loss: 0.0006822230643592775 Epsilon: 0.49508926537783415\n",
      "247/1000 Total steps: 7099 Episode reward: 44.0 Average reward: 28.625 Loss: 0.0005923499120399356 Epsilon: 0.49335469147121136\n",
      "248/1000 Total steps: 7160 Episode reward: 61.0 Average reward: 28.755020080321284 Loss: 0.0009406101889908314 Epsilon: 0.4909625313592636\n",
      "249/1000 Total steps: 7208 Episode reward: 48.0 Average reward: 28.832 Loss: 0.001293109729886055 Epsilon: 0.4890904078995182\n",
      "250/1000 Total steps: 7280 Episode reward: 72.0 Average reward: 29.00398406374502 Loss: 0.0011418645735830069 Epsilon: 0.4862990180249839\n",
      "251/1000 Total steps: 7310 Episode reward: 30.0 Average reward: 29.00793650793651 Loss: 0.001541941543109715 Epsilon: 0.4851418575794474\n",
      "252/1000 Total steps: 7388 Episode reward: 78.0 Average reward: 29.201581027667984 Loss: 0.0005646228673867881 Epsilon: 0.4821494367033031\n",
      "253/1000 Total steps: 7424 Episode reward: 36.0 Average reward: 29.228346456692915 Loss: 0.0012485989136621356 Epsilon: 0.48077617209059953\n",
      "254/1000 Total steps: 7457 Episode reward: 33.0 Average reward: 29.24313725490196 Loss: 0.0005027423030696809 Epsilon: 0.47952168177017895\n",
      "255/1000 Total steps: 7508 Episode reward: 51.0 Average reward: 29.328125 Loss: 0.0009399012196809053 Epsilon: 0.47759104849265455\n",
      "256/1000 Total steps: 7532 Episode reward: 24.0 Average reward: 29.30739299610895 Loss: 0.0010764545295387506 Epsilon: 0.4766859165690439\n",
      "257/1000 Total steps: 7575 Episode reward: 43.0 Average reward: 29.36046511627907 Loss: 0.0006186843966133893 Epsilon: 0.4750696446029291\n",
      "258/1000 Total steps: 7595 Episode reward: 20.0 Average reward: 29.324324324324323 Loss: 0.001707751303911209 Epsilon: 0.47432025495316954\n",
      "259/1000 Total steps: 7618 Episode reward: 23.0 Average reward: 29.3 Loss: 0.000503139803186059 Epsilon: 0.47346030768522873\n",
      "260/1000 Total steps: 7669 Episode reward: 51.0 Average reward: 29.38314176245211 Loss: 0.0017310481052845716 Epsilon: 0.4715605087212048\n",
      "261/1000 Total steps: 7720 Episode reward: 51.0 Average reward: 29.46564885496183 Loss: 0.0004711661022156477 Epsilon: 0.46967037406696\n",
      "262/1000 Total steps: 7751 Episode reward: 31.0 Average reward: 29.47148288973384 Loss: 0.003447221126407385 Epsilon: 0.46852617033944643\n",
      "263/1000 Total steps: 7806 Episode reward: 55.0 Average reward: 29.568181818181817 Loss: 0.0011737658642232418 Epsilon: 0.4665048401560179\n",
      "264/1000 Total steps: 7832 Episode reward: 26.0 Average reward: 29.554716981132074 Loss: 0.001257565338164568 Epsilon: 0.4655531652850545\n",
      "265/1000 Total steps: 7891 Episode reward: 59.0 Average reward: 29.665413533834588 Loss: 0.0012427333276718855 Epsilon: 0.46340275156832533\n",
      "266/1000 Total steps: 7951 Episode reward: 60.0 Average reward: 29.779026217228463 Loss: 0.002808511955663562 Epsilon: 0.4612288632455447\n",
      "267/1000 Total steps: 8058 Episode reward: 107.0 Average reward: 30.067164179104477 Loss: 0.0007427359232679009 Epsilon: 0.45738431939848134\n",
      "268/1000 Total steps: 8069 Episode reward: 11.0 Average reward: 29.99628252788104 Loss: 0.0013900946360081434 Epsilon: 0.45699141278539823\n",
      "269/1000 Total steps: 8093 Episode reward: 24.0 Average reward: 29.974074074074075 Loss: 0.0010318078566342592 Epsilon: 0.4561356607079672\n",
      "270/1000 Total steps: 8108 Episode reward: 15.0 Average reward: 29.918819188191883 Loss: 0.0010309749050065875 Epsilon: 0.4556018576692723\n",
      "271/1000 Total steps: 8184 Episode reward: 76.0 Average reward: 30.08823529411765 Loss: 0.0009410043712705374 Epsilon: 0.45290952736521195\n",
      "272/1000 Total steps: 8205 Episode reward: 21.0 Average reward: 30.054945054945055 Loss: 0.0010208528256043792 Epsilon: 0.4521691949788229\n",
      "273/1000 Total steps: 8216 Episode reward: 11.0 Average reward: 29.985401459854014 Loss: 0.0009163447539322078 Epsilon: 0.4517820218486077\n",
      "274/1000 Total steps: 8234 Episode reward: 18.0 Average reward: 29.94181818181818 Loss: 0.0014252978144213557 Epsilon: 0.4511493837543773\n",
      "275/1000 Total steps: 8290 Episode reward: 56.0 Average reward: 30.036231884057973 Loss: 0.001649274374358356 Epsilon: 0.4491884429641547\n",
      "276/1000 Total steps: 8307 Episode reward: 17.0 Average reward: 29.989169675090253 Loss: 0.0006393836811184883 Epsilon: 0.44859532690261006\n",
      "277/1000 Total steps: 8341 Episode reward: 34.0 Average reward: 30.003597122302157 Loss: 0.0014503083657473326 Epsilon: 0.4474121153905386\n",
      "278/1000 Total steps: 8393 Episode reward: 52.0 Average reward: 30.08243727598566 Loss: 0.0008322305511683226 Epsilon: 0.4456102612713937\n",
      "279/1000 Total steps: 8428 Episode reward: 35.0 Average reward: 30.1 Loss: 0.0008117459947243333 Epsilon: 0.4444027397522802\n",
      "280/1000 Total steps: 8477 Episode reward: 49.0 Average reward: 30.16725978647687 Loss: 0.0009700709488242865 Epsilon: 0.44271929413754285\n",
      "281/1000 Total steps: 8526 Episode reward: 49.0 Average reward: 30.23404255319149 Loss: 0.00023030542070046067 Epsilon: 0.4410440772295221\n",
      "282/1000 Total steps: 8565 Episode reward: 39.0 Average reward: 30.26501766784452 Loss: 0.001382146030664444 Epsilon: 0.4397165956000869\n",
      "283/1000 Total steps: 8660 Episode reward: 95.0 Average reward: 30.492957746478872 Loss: 0.0008735781884752214 Epsilon: 0.43650456922424996\n",
      "284/1000 Total steps: 8690 Episode reward: 30.0 Average reward: 30.49122807017544 Loss: 0.0010380916064605117 Epsilon: 0.4354965682740032\n",
      "285/1000 Total steps: 8735 Episode reward: 45.0 Average reward: 30.541958041958043 Loss: 0.0012873540399596095 Epsilon: 0.43399022552989697\n",
      "286/1000 Total steps: 8747 Episode reward: 12.0 Average reward: 30.477351916376307 Loss: 0.0007122137467376888 Epsilon: 0.43358967763606315\n",
      "287/1000 Total steps: 8764 Episode reward: 17.0 Average reward: 30.430555555555557 Loss: 0.00027759469230659306 Epsilon: 0.4330230569481277\n",
      "288/1000 Total steps: 8779 Episode reward: 15.0 Average reward: 30.377162629757784 Loss: 0.0010885329684242606 Epsilon: 0.43252389682638925\n",
      "289/1000 Total steps: 8797 Episode reward: 18.0 Average reward: 30.33448275862069 Loss: 0.0016763776075094938 Epsilon: 0.4319258921777468\n",
      "290/1000 Total steps: 8818 Episode reward: 21.0 Average reward: 30.302405498281786 Loss: 0.0005906245205551386 Epsilon: 0.4312295791887071\n",
      "291/1000 Total steps: 8828 Episode reward: 10.0 Average reward: 30.232876712328768 Loss: 0.0010341776069253683 Epsilon: 0.4308985151691168\n",
      "292/1000 Total steps: 8845 Episode reward: 17.0 Average reward: 30.187713310580204 Loss: 0.0006655686302110553 Epsilon: 0.43033646557084815\n",
      "293/1000 Total steps: 8892 Episode reward: 47.0 Average reward: 30.244897959183675 Loss: 0.003179306397214532 Epsilon: 0.42878752703955036\n",
      "294/1000 Total steps: 8926 Episode reward: 34.0 Average reward: 30.257627118644066 Loss: 0.0006846869364380836 Epsilon: 0.4276715476875741\n",
      "295/1000 Total steps: 9008 Episode reward: 82.0 Average reward: 30.43243243243243 Loss: 0.0004823342023883015 Epsilon: 0.4249956272643286\n",
      "296/1000 Total steps: 9054 Episode reward: 46.0 Average reward: 30.484848484848484 Loss: 0.0005433870828710496 Epsilon: 0.423504080566411\n",
      "297/1000 Total steps: 9075 Episode reward: 21.0 Average reward: 30.453020134228186 Loss: 0.0008909717435017228 Epsilon: 0.4228254348246526\n",
      "298/1000 Total steps: 9091 Episode reward: 16.0 Average reward: 30.40468227424749 Loss: 0.0007513607852160931 Epsilon: 0.42230932712519575\n",
      "299/1000 Total steps: 9117 Episode reward: 26.0 Average reward: 30.39 Loss: 0.0009823658037930727 Epsilon: 0.4214724113366579\n",
      "300/1000 Total steps: 9129 Episode reward: 12.0 Average reward: 30.32890365448505 Loss: 0.0010721776634454727 Epsilon: 0.42108687581063375\n",
      "301/1000 Total steps: 9149 Episode reward: 20.0 Average reward: 30.294701986754966 Loss: 0.0003542158519849181 Epsilon: 0.4204453438048622\n",
      "302/1000 Total steps: 9162 Episode reward: 13.0 Average reward: 30.237623762376238 Loss: 0.00109622604213655 Epsilon: 0.42002903551693316\n",
      "303/1000 Total steps: 9362 Episode reward: 200.0 Average reward: 30.79605263157895 Loss: 0.00032615059171803296 Epsilon: 0.4136920360333384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/1000 Total steps: 9415 Episode reward: 53.0 Average reward: 30.868852459016395 Loss: 0.0017822221852838993 Epsilon: 0.41203386627372185\n",
      "305/1000 Total steps: 9463 Episode reward: 48.0 Average reward: 30.924836601307188 Loss: 0.0006318922969512641 Epsilon: 0.41053969260123435\n",
      "306/1000 Total steps: 9547 Episode reward: 84.0 Average reward: 31.09771986970684 Loss: 0.00048242646153084934 Epsilon: 0.4079420844116982\n",
      "307/1000 Total steps: 9584 Episode reward: 37.0 Average reward: 31.116883116883116 Loss: 0.0019952706061303616 Epsilon: 0.4068048039656472\n",
      "308/1000 Total steps: 9615 Episode reward: 31.0 Average reward: 31.116504854368934 Loss: 0.0009133482235483825 Epsilon: 0.4058551817482797\n",
      "309/1000 Total steps: 9679 Episode reward: 64.0 Average reward: 31.22258064516129 Loss: 0.0004046832618769258 Epsilon: 0.4039039591575496\n",
      "310/1000 Total steps: 9793 Episode reward: 114.0 Average reward: 31.488745980707396 Loss: 0.0003094020066782832 Epsilon: 0.40045912687461904\n",
      "311/1000 Total steps: 9812 Episode reward: 19.0 Average reward: 31.44871794871795 Loss: 0.0004604125861078501 Epsilon: 0.3998887965189696\n",
      "312/1000 Total steps: 9845 Episode reward: 33.0 Average reward: 31.45367412140575 Loss: 0.0006453066016547382 Epsilon: 0.39890079459025085\n",
      "313/1000 Total steps: 9861 Episode reward: 16.0 Average reward: 31.404458598726116 Loss: 0.0013969240244477987 Epsilon: 0.3984229357079555\n",
      "314/1000 Total steps: 9958 Episode reward: 97.0 Average reward: 31.612698412698414 Loss: 0.0009780582040548325 Epsilon: 0.3955422272547061\n",
      "315/1000 Total steps: 10064 Episode reward: 106.0 Average reward: 31.848101265822784 Loss: 0.0009099795715883374 Epsilon: 0.3924260246973489\n",
      "316/1000 Total steps: 10077 Episode reward: 13.0 Average reward: 31.78864353312303 Loss: 0.00028268658206798136 Epsilon: 0.3920461178581912\n",
      "317/1000 Total steps: 10132 Episode reward: 55.0 Average reward: 31.861635220125788 Loss: 0.000534912571310997 Epsilon: 0.3904442733204311\n",
      "318/1000 Total steps: 10147 Episode reward: 15.0 Average reward: 31.808777429467085 Loss: 0.0005662805633619428 Epsilon: 0.3900089334969443\n",
      "319/1000 Total steps: 10162 Episode reward: 15.0 Average reward: 31.75625 Loss: 0.0007350498926825821 Epsilon: 0.3895742461936802\n",
      "320/1000 Total steps: 10221 Episode reward: 59.0 Average reward: 31.8411214953271 Loss: 0.0006595327286049724 Epsilon: 0.38787078828341737\n",
      "321/1000 Total steps: 10233 Episode reward: 12.0 Average reward: 31.779503105590063 Loss: 0.0015180817572399974 Epsilon: 0.3875255505215629\n",
      "322/1000 Total steps: 10261 Episode reward: 28.0 Average reward: 31.76780185758514 Loss: 0.0021199597977101803 Epsilon: 0.3867216050290364\n",
      "323/1000 Total steps: 10335 Episode reward: 74.0 Average reward: 31.89814814814815 Loss: 0.0010834159329533577 Epsilon: 0.3846076962607258\n",
      "324/1000 Total steps: 10360 Episode reward: 25.0 Average reward: 31.876923076923077 Loss: 0.0003324272984173149 Epsilon: 0.38389706567842197\n",
      "325/1000 Total steps: 10390 Episode reward: 30.0 Average reward: 31.87116564417178 Loss: 0.0004210082988720387 Epsilon: 0.38304665074160305\n",
      "326/1000 Total steps: 10415 Episode reward: 25.0 Average reward: 31.850152905198776 Loss: 0.0005391322774812579 Epsilon: 0.3823399178988923\n",
      "327/1000 Total steps: 10434 Episode reward: 19.0 Average reward: 31.8109756097561 Loss: 0.0010458567412570119 Epsilon: 0.38180398135582794\n",
      "328/1000 Total steps: 10456 Episode reward: 22.0 Average reward: 31.78115501519757 Loss: 0.0008304856601171196 Epsilon: 0.38118469406264677\n",
      "329/1000 Total steps: 10551 Episode reward: 95.0 Average reward: 31.972727272727273 Loss: 0.0004723263846244663 Epsilon: 0.37852608784349684\n",
      "330/1000 Total steps: 10634 Episode reward: 83.0 Average reward: 32.126888217522655 Loss: 0.002154079731553793 Epsilon: 0.3762238886575442\n",
      "331/1000 Total steps: 10713 Episode reward: 79.0 Average reward: 32.26807228915663 Loss: 0.0003547519736457616 Epsilon: 0.37405031685016177\n",
      "332/1000 Total steps: 10730 Episode reward: 17.0 Average reward: 32.22222222222222 Loss: 0.001195412827655673 Epsilon: 0.37358482708991814\n",
      "333/1000 Total steps: 10750 Episode reward: 20.0 Average reward: 32.18562874251497 Loss: 0.00034630903974175453 Epsilon: 0.37303820424079503\n",
      "334/1000 Total steps: 10762 Episode reward: 12.0 Average reward: 32.125373134328356 Loss: 0.0016635471256449819 Epsilon: 0.3727107549046016\n",
      "335/1000 Total steps: 10775 Episode reward: 13.0 Average reward: 32.06845238095238 Loss: 0.0005860969540663064 Epsilon: 0.37235646126398847\n",
      "336/1000 Total steps: 10799 Episode reward: 24.0 Average reward: 32.04451038575667 Loss: 0.0007991095772013068 Epsilon: 0.37170358951643034\n",
      "337/1000 Total steps: 10820 Episode reward: 21.0 Average reward: 32.01183431952663 Loss: 0.0012493643444031477 Epsilon: 0.37113361066570627\n",
      "338/1000 Total steps: 10852 Episode reward: 32.0 Average reward: 32.0117994100295 Loss: 0.0010380097664892673 Epsilon: 0.37026736983609543\n",
      "339/1000 Total steps: 10882 Episode reward: 30.0 Average reward: 32.00588235294118 Loss: 0.00038240733556449413 Epsilon: 0.36945778271445984\n",
      "340/1000 Total steps: 10896 Episode reward: 14.0 Average reward: 31.95307917888563 Loss: 0.0009772503981366754 Epsilon: 0.36908080576409785\n",
      "341/1000 Total steps: 10917 Episode reward: 21.0 Average reward: 31.92105263157895 Loss: 0.0009853389346972108 Epsilon: 0.3685163289800616\n",
      "342/1000 Total steps: 10967 Episode reward: 50.0 Average reward: 31.973760932944607 Loss: 0.0015213723527267575 Epsilon: 0.36717709820216904\n",
      "343/1000 Total steps: 11143 Episode reward: 176.0 Average reward: 32.39244186046512 Loss: 0.0005617360584437847 Epsilon: 0.36251591996224863\n",
      "344/1000 Total steps: 11179 Episode reward: 36.0 Average reward: 32.402898550724636 Loss: 0.0011880379170179367 Epsilon: 0.36157256171405794\n",
      "345/1000 Total steps: 11232 Episode reward: 53.0 Average reward: 32.46242774566474 Loss: 0.0011386702535673976 Epsilon: 0.3601898944418369\n",
      "346/1000 Total steps: 11412 Episode reward: 180.0 Average reward: 32.887608069164266 Loss: 0.0007589994929730892 Epsilon: 0.3555483753341918\n",
      "347/1000 Total steps: 11424 Episode reward: 12.0 Average reward: 32.827586206896555 Loss: 0.002714442787691951 Epsilon: 0.3552419012050452\n",
      "348/1000 Total steps: 11515 Episode reward: 91.0 Average reward: 32.99426934097421 Loss: 0.000616841425653547 Epsilon: 0.35292973621064705\n",
      "349/1000 Total steps: 11540 Episode reward: 25.0 Average reward: 32.97142857142857 Loss: 0.0029859289061278105 Epsilon: 0.3522982016172864\n",
      "350/1000 Total steps: 11554 Episode reward: 14.0 Average reward: 32.91737891737892 Loss: 0.0006584092625416815 Epsilon: 0.3519452312719157\n",
      "351/1000 Total steps: 11601 Episode reward: 47.0 Average reward: 32.95738636363637 Loss: 0.0007928250124678016 Epsilon: 0.35076386706551654\n",
      "352/1000 Total steps: 11672 Episode reward: 71.0 Average reward: 33.065155807365436 Loss: 0.0014309883117675781 Epsilon: 0.3489897491806101\n",
      "353/1000 Total steps: 11738 Episode reward: 66.0 Average reward: 33.15819209039548 Loss: 0.001160914427600801 Epsilon: 0.34735182792182195\n",
      "354/1000 Total steps: 11753 Episode reward: 15.0 Average reward: 33.10704225352113 Loss: 0.0011578545672819018 Epsilon: 0.34698107831166236\n",
      "355/1000 Total steps: 11784 Episode reward: 31.0 Average reward: 33.10112359550562 Loss: 0.000622153514996171 Epsilon: 0.3462166224876251\n",
      "356/1000 Total steps: 12066 Episode reward: 282.0 Average reward: 33.79831932773109 Loss: 0.0016382558969780803 Epsilon: 0.3393703005722213\n",
      "357/1000 Total steps: 12094 Episode reward: 28.0 Average reward: 33.78212290502793 Loss: 0.0006294063641689718 Epsilon: 0.33870100118703383\n",
      "358/1000 Total steps: 12106 Episode reward: 12.0 Average reward: 33.721448467966574 Loss: 0.0009706902783364058 Epsilon: 0.3384147317816051\n",
      "359/1000 Total steps: 12140 Episode reward: 34.0 Average reward: 33.72222222222222 Loss: 0.0010229905601590872 Epsilon: 0.33760549817024843\n",
      "360/1000 Total steps: 12191 Episode reward: 51.0 Average reward: 33.770083102493075 Loss: 0.0006960224127396941 Epsilon: 0.3363967949426736\n",
      "361/1000 Total steps: 12228 Episode reward: 37.0 Average reward: 33.77900552486188 Loss: 0.0023451577872037888 Epsilon: 0.33552374294359055\n",
      "362/1000 Total steps: 12285 Episode reward: 57.0 Average reward: 33.84297520661157 Loss: 0.00010063878289656714 Epsilon: 0.3341850764328055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/1000 Total steps: 12342 Episode reward: 57.0 Average reward: 33.90659340659341 Loss: 0.0006486999918706715 Epsilon: 0.33285401861575414\n",
      "364/1000 Total steps: 12379 Episode reward: 37.0 Average reward: 33.915068493150685 Loss: 0.0020903670229017735 Epsilon: 0.3319940506686579\n",
      "365/1000 Total steps: 12418 Episode reward: 39.0 Average reward: 33.92896174863388 Loss: 0.0009311420726589859 Epsilon: 0.3310910358944308\n",
      "366/1000 Total steps: 12431 Episode reward: 13.0 Average reward: 33.87193460490463 Loss: 0.00037622801028192043 Epsilon: 0.330790812735103\n",
      "367/1000 Total steps: 12575 Episode reward: 144.0 Average reward: 34.171195652173914 Loss: 0.0003499385202303529 Epsilon: 0.3274912389791975\n",
      "368/1000 Total steps: 12637 Episode reward: 62.0 Average reward: 34.24661246612466 Loss: 0.0010561095550656319 Epsilon: 0.3260851566568732\n",
      "369/1000 Total steps: 12652 Episode reward: 15.0 Average reward: 34.1945945945946 Loss: 0.000699564116075635 Epsilon: 0.32574628314056386\n",
      "370/1000 Total steps: 12724 Episode reward: 72.0 Average reward: 34.296495956873315 Loss: 0.0019675695803016424 Epsilon: 0.3241267472276275\n",
      "371/1000 Total steps: 12745 Episode reward: 21.0 Average reward: 34.26075268817204 Loss: 0.0018644649535417557 Epsilon: 0.32365657491216904\n",
      "372/1000 Total steps: 12834 Episode reward: 89.0 Average reward: 34.40750670241287 Loss: 0.0017658498836681247 Epsilon: 0.32167486309397386\n",
      "373/1000 Total steps: 12919 Episode reward: 85.0 Average reward: 34.5427807486631 Loss: 0.0003999257169198245 Epsilon: 0.31979861212089145\n",
      "374/1000 Total steps: 12992 Episode reward: 73.0 Average reward: 34.64533333333333 Loss: 0.0007115489570423961 Epsilon: 0.3181999245614995\n",
      "375/1000 Total steps: 13009 Episode reward: 17.0 Average reward: 34.598404255319146 Loss: 0.00040952221024781466 Epsilon: 0.3178292998100424\n",
      "376/1000 Total steps: 13047 Episode reward: 38.0 Average reward: 34.60742705570292 Loss: 0.00025516372988931835 Epsilon: 0.31700311920807844\n",
      "377/1000 Total steps: 13187 Episode reward: 140.0 Average reward: 34.886243386243386 Loss: 0.0005073391366750002 Epsilon: 0.3139862429484672\n",
      "378/1000 Total steps: 13213 Episode reward: 26.0 Average reward: 34.86279683377309 Loss: 0.002546692034229636 Epsilon: 0.3134306013638726\n",
      "379/1000 Total steps: 13364 Episode reward: 151.0 Average reward: 35.16842105263158 Loss: 0.00026601209538057446 Epsilon: 0.31023200942807466\n",
      "380/1000 Total steps: 13437 Episode reward: 73.0 Average reward: 35.267716535433074 Loss: 0.0007513759774155915 Epsilon: 0.30870290378534293\n",
      "381/1000 Total steps: 13462 Episode reward: 25.0 Average reward: 35.24083769633508 Loss: 0.00021590827964246273 Epsilon: 0.3081817981792963\n",
      "382/1000 Total steps: 13529 Episode reward: 67.0 Average reward: 35.32375979112272 Loss: 0.00035830613342113793 Epsilon: 0.3067916423538476\n",
      "383/1000 Total steps: 13627 Episode reward: 98.0 Average reward: 35.486979166666664 Loss: 0.0010746364714577794 Epsilon: 0.3047749820343245\n",
      "384/1000 Total steps: 13672 Episode reward: 45.0 Average reward: 35.51168831168831 Loss: 0.00042055637459270656 Epsilon: 0.3038555648553387\n",
      "385/1000 Total steps: 13693 Episode reward: 21.0 Average reward: 35.47409326424871 Loss: 0.0016960466746240854 Epsilon: 0.30342791735617713\n",
      "386/1000 Total steps: 13799 Episode reward: 106.0 Average reward: 35.656330749354005 Loss: 0.0013273033546283841 Epsilon: 0.30128296973839774\n",
      "387/1000 Total steps: 13808 Episode reward: 9.0 Average reward: 35.58762886597938 Loss: 0.0004704271850641817 Epsilon: 0.30110189656078556\n",
      "388/1000 Total steps: 13837 Episode reward: 29.0 Average reward: 35.5706940874036 Loss: 0.00037573929876089096 Epsilon: 0.30051954587738094\n",
      "389/1000 Total steps: 13907 Episode reward: 70.0 Average reward: 35.65897435897436 Loss: 0.0013515513855963945 Epsilon: 0.29912081034211147\n",
      "390/1000 Total steps: 13924 Episode reward: 17.0 Average reward: 35.611253196930946 Loss: 0.0011170239886268973 Epsilon: 0.2987825925311234\n",
      "391/1000 Total steps: 14017 Episode reward: 93.0 Average reward: 35.75765306122449 Loss: 0.0010010902769863605 Epsilon: 0.29694248418694613\n",
      "392/1000 Total steps: 14061 Episode reward: 44.0 Average reward: 35.778625954198475 Loss: 0.00039564198232255876 Epsilon: 0.29607784086678535\n",
      "393/1000 Total steps: 14148 Episode reward: 87.0 Average reward: 35.90862944162436 Loss: 0.00044698451529257 Epsilon: 0.2943793627442147\n",
      "394/1000 Total steps: 14213 Episode reward: 65.0 Average reward: 35.982278481012656 Loss: 0.0006345264264382422 Epsilon: 0.29311999426794855\n",
      "395/1000 Total steps: 14225 Episode reward: 12.0 Average reward: 35.92171717171717 Loss: 0.0005586830666288733 Epsilon: 0.292888389265621\n",
      "396/1000 Total steps: 14425 Episode reward: 200.0 Average reward: 36.33501259445844 Loss: 0.001413437887094915 Epsilon: 0.28906894325443877\n",
      "397/1000 Total steps: 14518 Episode reward: 93.0 Average reward: 36.47738693467337 Loss: 0.0006100476020947099 Epsilon: 0.28731875308095667\n",
      "398/1000 Total steps: 14527 Episode reward: 9.0 Average reward: 36.40852130325815 Loss: 0.00039129398646764457 Epsilon: 0.28715024204452466\n",
      "399/1000 Total steps: 14542 Episode reward: 15.0 Average reward: 36.355 Loss: 0.007856986485421658 Epsilon: 0.2868697271202476\n",
      "400/1000 Total steps: 14561 Episode reward: 19.0 Average reward: 36.31172069825436 Loss: 0.0003508036897983402 Epsilon: 0.2865150117250548\n",
      "401/1000 Total steps: 14708 Episode reward: 147.0 Average reward: 36.58706467661692 Loss: 0.0005607432103715837 Epsilon: 0.28379329468409065\n",
      "402/1000 Total steps: 14911 Episode reward: 203.0 Average reward: 37.0 Loss: 0.0009165636729449034 Epsilon: 0.28009990553505637\n",
      "403/1000 Total steps: 15041 Episode reward: 130.0 Average reward: 37.23019801980198 Loss: 0.001407946809194982 Epsilon: 0.27777375947230665\n",
      "404/1000 Total steps: 15120 Episode reward: 79.0 Average reward: 37.333333333333336 Loss: 0.0024409617763012648 Epsilon: 0.2763748796232127\n",
      "405/1000 Total steps: 15153 Episode reward: 33.0 Average reward: 37.32266009852217 Loss: 0.0005549424677155912 Epsilon: 0.2757938018261493\n",
      "406/1000 Total steps: 15200 Episode reward: 47.0 Average reward: 37.34643734643735 Loss: 0.0010832429397851229 Epsilon: 0.2749695095617718\n",
      "407/1000 Total steps: 15498 Episode reward: 298.0 Average reward: 37.98529411764706 Loss: 0.0015791973564773798 Epsilon: 0.2698323421334007\n",
      "408/1000 Total steps: 15560 Episode reward: 62.0 Average reward: 38.044009779951104 Loss: 0.0011729794787243009 Epsilon: 0.2687826390542657\n",
      "409/1000 Total steps: 15649 Episode reward: 89.0 Average reward: 38.16829268292683 Loss: 0.0003742845437955111 Epsilon: 0.26728713841606017\n",
      "410/1000 Total steps: 15665 Episode reward: 16.0 Average reward: 38.114355231143556 Loss: 0.0008579816785641015 Epsilon: 0.2670196930079759\n",
      "411/1000 Total steps: 15720 Episode reward: 55.0 Average reward: 38.15533980582524 Loss: 0.0008077722159214318 Epsilon: 0.26610360624433294\n",
      "412/1000 Total steps: 15830 Episode reward: 110.0 Average reward: 38.32929782082324 Loss: 0.00042111321818083525 Epsilon: 0.26428647909761405\n",
      "413/1000 Total steps: 15891 Episode reward: 61.0 Average reward: 38.38405797101449 Loss: 0.0006415091920644045 Epsilon: 0.2632873819195436\n",
      "414/1000 Total steps: 16223 Episode reward: 332.0 Average reward: 39.091566265060244 Loss: 0.0009450550423935056 Epsilon: 0.25795524409336523\n",
      "415/1000 Total steps: 16578 Episode reward: 355.0 Average reward: 39.85096153846154 Loss: 0.0009051580564118922 Epsilon: 0.25244619706535576\n",
      "416/1000 Total steps: 17073 Episode reward: 495.0 Average reward: 40.94244604316547 Loss: 0.00043300248216837645 Epsilon: 0.24508383208538667\n",
      "417/1000 Total steps: 17573 Episode reward: 500.0 Average reward: 42.04066985645933 Loss: 0.00020000241056550294 Epsilon: 0.2380080100989406\n",
      "418/1000 Total steps: 17867 Episode reward: 294.0 Average reward: 42.64200477326969 Loss: 0.000690706423483789 Epsilon: 0.23400963866071944\n",
      "419/1000 Total steps: 18118 Episode reward: 251.0 Average reward: 43.13809523809524 Loss: 0.0008147554472088814 Epsilon: 0.23068785945372844\n",
      "420/1000 Total steps: 18168 Episode reward: 50.0 Average reward: 43.15439429928741 Loss: 0.0006509758532047272 Epsilon: 0.23003605103543917\n",
      "421/1000 Total steps: 18179 Episode reward: 11.0 Average reward: 43.0781990521327 Loss: 0.000700215925462544 Epsilon: 0.22989309002227265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/1000 Total steps: 18581 Episode reward: 402.0 Average reward: 43.92671394799054 Loss: 0.0009712692117318511 Epsilon: 0.22477495162627092\n",
      "423/1000 Total steps: 18649 Episode reward: 68.0 Average reward: 43.98349056603774 Loss: 0.0009271241724491119 Epsilon: 0.22392936022432194\n",
      "424/1000 Total steps: 18740 Episode reward: 91.0 Average reward: 44.09411764705882 Loss: 0.0009259525104425848 Epsilon: 0.22280671881185787\n",
      "425/1000 Total steps: 18917 Episode reward: 177.0 Average reward: 44.406103286384976 Loss: 0.0003462258609943092 Epsilon: 0.2206521639491702\n",
      "426/1000 Total steps: 19075 Episode reward: 158.0 Average reward: 44.67213114754098 Loss: 0.0002151244698325172 Epsilon: 0.2187608405592211\n",
      "427/1000 Total steps: 19126 Episode reward: 51.0 Average reward: 44.68691588785047 Loss: 0.005123074632138014 Epsilon: 0.21815670213482075\n",
      "428/1000 Total steps: 19219 Episode reward: 93.0 Average reward: 44.7995337995338 Loss: 0.0006381546263583004 Epsilon: 0.21706293868828203\n",
      "429/1000 Total steps: 19255 Episode reward: 36.0 Average reward: 44.77906976744186 Loss: 0.0001528542343294248 Epsilon: 0.21664226976738415\n",
      "430/1000 Total steps: 19502 Episode reward: 247.0 Average reward: 45.24825986078886 Loss: 0.0002198677830165252 Epsilon: 0.21379649569399042\n",
      "431/1000 Total steps: 19650 Episode reward: 148.0 Average reward: 45.486111111111114 Loss: 0.0001861454511526972 Epsilon: 0.21212470929265229\n",
      "432/1000 Total steps: 19865 Episode reward: 215.0 Average reward: 45.877598152424945 Loss: 0.0005516430828720331 Epsilon: 0.20973975813737766\n",
      "433/1000 Total steps: 19934 Episode reward: 69.0 Average reward: 45.93087557603687 Loss: 0.000406199658755213 Epsilon: 0.2089851601631061\n",
      "434/1000 Total steps: 20024 Episode reward: 90.0 Average reward: 46.03218390804598 Loss: 0.000484722841065377 Epsilon: 0.20800869440866804\n",
      "435/1000 Total steps: 20035 Episode reward: 11.0 Average reward: 45.95183486238532 Loss: 0.0013985936529934406 Epsilon: 0.20788995016612533\n",
      "436/1000 Total steps: 20160 Episode reward: 125.0 Average reward: 46.13272311212815 Loss: 0.0005532930372282863 Epsilon: 0.2065497196804565\n",
      "437/1000 Total steps: 20471 Episode reward: 311.0 Average reward: 46.73744292237443 Loss: 0.00032842817017808557 Epsilon: 0.2032870213297152\n",
      "438/1000 Total steps: 20493 Episode reward: 22.0 Average reward: 46.68109339407745 Loss: 0.00038214633241295815 Epsilon: 0.20306003965418215\n",
      "439/1000 Total steps: 20723 Episode reward: 230.0 Average reward: 47.097727272727276 Loss: 0.00040091355913318694 Epsilon: 0.2007167103302185\n",
      "440/1000 Total steps: 20780 Episode reward: 57.0 Average reward: 47.12018140589569 Loss: 0.0005254964344203472 Epsilon: 0.20014425812004877\n",
      "441/1000 Total steps: 20916 Episode reward: 136.0 Average reward: 47.321266968325794 Loss: 0.0007001319900155067 Epsilon: 0.19879151570822193\n",
      "442/1000 Total steps: 20957 Episode reward: 41.0 Average reward: 47.306997742663654 Loss: 0.0018730465089902282 Epsilon: 0.1983872997028683\n",
      "443/1000 Total steps: 21038 Episode reward: 81.0 Average reward: 47.38288288288288 Loss: 0.0008281100890599191 Epsilon: 0.19759358147375264\n",
      "444/1000 Total steps: 21075 Episode reward: 37.0 Average reward: 47.359550561797754 Loss: 0.0003630549181252718 Epsilon: 0.19723315242722522\n",
      "445/1000 Total steps: 21189 Episode reward: 114.0 Average reward: 47.50896860986547 Loss: 0.0002729700063355267 Epsilon: 0.19613098875887114\n",
      "446/1000 Total steps: 21493 Episode reward: 304.0 Average reward: 48.082774049217 Loss: 0.000741337426006794 Epsilon: 0.19325258018337377\n",
      "447/1000 Total steps: 21591 Episode reward: 98.0 Average reward: 48.19419642857143 Loss: 0.000398891483200714 Epsilon: 0.19234316829414877\n",
      "448/1000 Total steps: 21645 Episode reward: 54.0 Average reward: 48.207126948775056 Loss: 0.00048546286416240036 Epsilon: 0.19184585912856814\n",
      "449/1000 Total steps: 21746 Episode reward: 101.0 Average reward: 48.324444444444445 Loss: 0.0011045187711715698 Epsilon: 0.19092288481767705\n",
      "450/1000 Total steps: 22132 Episode reward: 386.0 Average reward: 49.073170731707314 Loss: 0.0009551849798299372 Epsilon: 0.1874801340071595\n",
      "451/1000 Total steps: 22160 Episode reward: 28.0 Average reward: 49.02654867256637 Loss: 0.0008076599333435297 Epsilon: 0.187235532234228\n",
      "452/1000 Total steps: 22189 Episode reward: 29.0 Average reward: 48.98233995584989 Loss: 0.0005075809895060956 Epsilon: 0.18698291566182082\n",
      "453/1000 Total steps: 22313 Episode reward: 124.0 Average reward: 49.147577092511014 Loss: 0.0010297091212123632 Epsilon: 0.18591098719902507\n",
      "454/1000 Total steps: 22328 Episode reward: 15.0 Average reward: 49.072527472527476 Loss: 0.0005561475409194827 Epsilon: 0.18578221731978034\n",
      "455/1000 Total steps: 22465 Episode reward: 137.0 Average reward: 49.26535087719298 Loss: 0.0007124499534256756 Epsilon: 0.18461501453752485\n",
      "456/1000 Total steps: 22537 Episode reward: 72.0 Average reward: 49.31509846827134 Loss: 0.00018371030455455184 Epsilon: 0.1840079743997618\n",
      "457/1000 Total steps: 22590 Episode reward: 53.0 Average reward: 49.3231441048035 Loss: 0.002104056067764759 Epsilon: 0.18356390994572663\n",
      "458/1000 Total steps: 22771 Episode reward: 181.0 Average reward: 49.610021786492375 Loss: 0.003096239874139428 Epsilon: 0.1820650091489373\n",
      "459/1000 Total steps: 22946 Episode reward: 175.0 Average reward: 49.88260869565217 Loss: 0.0008020324748940766 Epsilon: 0.18064136471007794\n",
      "460/1000 Total steps: 23071 Episode reward: 125.0 Average reward: 50.04555314533623 Loss: 0.0011905485298484564 Epsilon: 0.17963962158920366\n",
      "461/1000 Total steps: 23084 Episode reward: 13.0 Average reward: 49.96536796536797 Loss: 0.0002504279837012291 Epsilon: 0.17953615734746606\n",
      "462/1000 Total steps: 23196 Episode reward: 112.0 Average reward: 50.09935205183585 Loss: 0.0003273557813372463 Epsilon: 0.1786503223212308\n",
      "463/1000 Total steps: 23223 Episode reward: 27.0 Average reward: 50.04956896551724 Loss: 0.000990870175883174 Epsilon: 0.17843825287355003\n",
      "464/1000 Total steps: 23236 Episode reward: 13.0 Average reward: 49.96989247311828 Loss: 0.0004612079355865717 Epsilon: 0.17833634939642595\n",
      "465/1000 Total steps: 23326 Episode reward: 90.0 Average reward: 50.05579399141631 Loss: 0.0003391397767700255 Epsilon: 0.17763448537751894\n",
      "466/1000 Total steps: 23385 Episode reward: 59.0 Average reward: 50.07494646680942 Loss: 0.0005095372325740755 Epsilon: 0.17717779048850912\n",
      "467/1000 Total steps: 23531 Episode reward: 146.0 Average reward: 50.27991452991453 Loss: 0.0005702426424250007 Epsilon: 0.1760591804706791\n",
      "468/1000 Total steps: 23649 Episode reward: 118.0 Average reward: 50.424307036247335 Loss: 0.00043413438834249973 Epsilon: 0.1751669566146226\n",
      "469/1000 Total steps: 23733 Episode reward: 84.0 Average reward: 50.49574468085106 Loss: 0.0004873016441706568 Epsilon: 0.17453819865956344\n",
      "470/1000 Total steps: 23747 Episode reward: 14.0 Average reward: 50.418259023354565 Loss: 0.0006724626291543245 Epsilon: 0.17443391819479787\n",
      "471/1000 Total steps: 23843 Episode reward: 96.0 Average reward: 50.514830508474574 Loss: 0.0015915361000224948 Epsilon: 0.17372277154564159\n",
      "472/1000 Total steps: 23960 Episode reward: 117.0 Average reward: 50.6553911205074 Loss: 0.00045725394738838077 Epsilon: 0.17286524145185847\n",
      "473/1000 Total steps: 23987 Episode reward: 27.0 Average reward: 50.60548523206751 Loss: 0.002043869812041521 Epsilon: 0.17266877065487038\n",
      "474/1000 Total steps: 23998 Episode reward: 11.0 Average reward: 50.5221052631579 Loss: 0.0006955122807994485 Epsilon: 0.17258887895564035\n",
      "475/1000 Total steps: 24021 Episode reward: 23.0 Average reward: 50.464285714285715 Loss: 0.00013056433817837387 Epsilon: 0.17242211638451366\n",
      "476/1000 Total steps: 24118 Episode reward: 97.0 Average reward: 50.56184486373166 Loss: 0.0017152864020317793 Epsilon: 0.1717230179644269\n",
      "477/1000 Total steps: 24179 Episode reward: 61.0 Average reward: 50.5836820083682 Loss: 0.000380547484382987 Epsilon: 0.17128683925243213\n",
      "478/1000 Total steps: 24233 Episode reward: 54.0 Average reward: 50.59081419624217 Loss: 0.001018776441924274 Epsilon: 0.17090292781425642\n",
      "479/1000 Total steps: 24421 Episode reward: 188.0 Average reward: 50.87708333333333 Loss: 0.00012588576646521688 Epsilon: 0.16958240458330115\n",
      "480/1000 Total steps: 24509 Episode reward: 88.0 Average reward: 50.95426195426195 Loss: 0.00023782964854035527 Epsilon: 0.16897276576795306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481/1000 Total steps: 24791 Episode reward: 282.0 Average reward: 51.433609958506224 Loss: 0.00039252976421266794 Epsilon: 0.16705490273712992\n",
      "482/1000 Total steps: 24874 Episode reward: 83.0 Average reward: 51.49896480331263 Loss: 0.0033228881657123566 Epsilon: 0.16650065037358724\n",
      "483/1000 Total steps: 25082 Episode reward: 208.0 Average reward: 51.82231404958678 Loss: 0.0002736929163802415 Epsilon: 0.1651317230440809\n",
      "484/1000 Total steps: 25177 Episode reward: 95.0 Average reward: 51.911340206185564 Loss: 0.00025438674492761493 Epsilon: 0.164515901459175\n",
      "485/1000 Total steps: 25315 Episode reward: 138.0 Average reward: 52.08847736625514 Loss: 0.0012416699901223183 Epsilon: 0.16363169706166047\n",
      "486/1000 Total steps: 25491 Episode reward: 176.0 Average reward: 52.3429158110883 Loss: 0.003005422418937087 Epsilon: 0.16252157690649438\n",
      "487/1000 Total steps: 25754 Episode reward: 263.0 Average reward: 52.77459016393443 Loss: 0.0002771475410554558 Epsilon: 0.16089869388878758\n",
      "488/1000 Total steps: 25885 Episode reward: 131.0 Average reward: 52.93456032719836 Loss: 0.001970968209207058 Epsilon: 0.16010612366817173\n",
      "489/1000 Total steps: 26074 Episode reward: 189.0 Average reward: 53.21224489795918 Loss: 0.0005839819204993546 Epsilon: 0.15898078587131698\n",
      "490/1000 Total steps: 26083 Episode reward: 9.0 Average reward: 53.122199592668025 Loss: 0.0010837021982297301 Epsilon: 0.15892772704408653\n",
      "491/1000 Total steps: 26176 Episode reward: 93.0 Average reward: 53.203252032520325 Loss: 0.00035643373848870397 Epsilon: 0.1583822396306439\n",
      "492/1000 Total steps: 26229 Episode reward: 53.0 Average reward: 53.20283975659229 Loss: 0.000455532775959 Epsilon: 0.15807363229244573\n",
      "493/1000 Total steps: 26381 Episode reward: 152.0 Average reward: 53.402834008097166 Loss: 0.0020060522947460413 Epsilon: 0.15719758788580107\n",
      "494/1000 Total steps: 26446 Episode reward: 65.0 Average reward: 53.42626262626263 Loss: 0.0003373137442395091 Epsilon: 0.15682700924985488\n",
      "495/1000 Total steps: 26527 Episode reward: 81.0 Average reward: 53.48185483870968 Loss: 0.0004087377164978534 Epsilon: 0.15636856966177848\n",
      "496/1000 Total steps: 26664 Episode reward: 137.0 Average reward: 53.64989939637827 Loss: 0.0004270189383532852 Epsilon: 0.15560158609109598\n",
      "497/1000 Total steps: 26737 Episode reward: 73.0 Average reward: 53.68875502008032 Loss: 0.00024342948745470494 Epsilon: 0.15519717241846817\n",
      "498/1000 Total steps: 27237 Episode reward: 500.0 Average reward: 54.58316633266533 Loss: 0.002436318900436163 Epsilon: 0.15250517455368617\n",
      "499/1000 Total steps: 27250 Episode reward: 13.0 Average reward: 54.5 Loss: 0.00023369910195469856 Epsilon: 0.1524369621744195\n",
      "500/1000 Total steps: 27378 Episode reward: 128.0 Average reward: 54.64670658682635 Loss: 0.0001452778378734365 Epsilon: 0.1517700464249815\n",
      "501/1000 Total steps: 27437 Episode reward: 59.0 Average reward: 54.655378486055774 Loss: 0.00015669799176976085 Epsilon: 0.1514655024392628\n",
      "502/1000 Total steps: 27533 Episode reward: 96.0 Average reward: 54.737574552683895 Loss: 0.0006028645439073443 Epsilon: 0.15097379757547957\n",
      "503/1000 Total steps: 27615 Episode reward: 82.0 Average reward: 54.791666666666664 Loss: 0.0005504500004462898 Epsilon: 0.15055752149980195\n",
      "504/1000 Total steps: 27750 Episode reward: 135.0 Average reward: 54.95049504950495 Loss: 0.0007639706018380821 Epsilon: 0.14987958135173876\n",
      "505/1000 Total steps: 27809 Episode reward: 59.0 Average reward: 54.958498023715414 Loss: 0.0006308691808953881 Epsilon: 0.14958615827102256\n",
      "506/1000 Total steps: 27911 Episode reward: 102.0 Average reward: 55.05128205128205 Loss: 0.0013979562791064382 Epsilon: 0.1490829501807252\n",
      "507/1000 Total steps: 28028 Episode reward: 117.0 Average reward: 55.173228346456696 Loss: 0.0007448950782418251 Epsilon: 0.1485120260823878\n",
      "508/1000 Total steps: 28263 Episode reward: 235.0 Average reward: 55.52652259332024 Loss: 0.0002814177132677287 Epsilon: 0.14738528453573257\n",
      "509/1000 Total steps: 28475 Episode reward: 212.0 Average reward: 55.833333333333336 Loss: 0.0034415738191455603 Epsilon: 0.146391290073007\n",
      "510/1000 Total steps: 28576 Episode reward: 101.0 Average reward: 55.92172211350294 Loss: 0.00023762638738844544 Epsilon: 0.1459250962849284\n",
      "511/1000 Total steps: 28594 Episode reward: 18.0 Average reward: 55.84765625 Loss: 8.64653629832901e-05 Epsilon: 0.1458425054656524\n",
      "512/1000 Total steps: 28680 Episode reward: 86.0 Average reward: 55.90643274853801 Loss: 0.0006253215833567083 Epsilon: 0.14544995032519692\n",
      "513/1000 Total steps: 28754 Episode reward: 74.0 Average reward: 55.94163424124513 Loss: 0.0050802081823349 Epsilon: 0.14511486204853224\n",
      "514/1000 Total steps: 28862 Episode reward: 108.0 Average reward: 56.04271844660194 Loss: 0.0012866462348029017 Epsilon: 0.14463024319072637\n",
      "515/1000 Total steps: 29024 Episode reward: 162.0 Average reward: 56.248062015503876 Loss: 0.0002574209647718817 Epsilon: 0.14391305813475852\n",
      "516/1000 Total steps: 29049 Episode reward: 25.0 Average reward: 56.18762088974855 Loss: 0.0015364346327260137 Epsilon: 0.14380341260344282\n",
      "517/1000 Total steps: 29098 Episode reward: 49.0 Average reward: 56.173745173745175 Loss: 0.0032335659489035606 Epsilon: 0.14358930088380076\n",
      "518/1000 Total steps: 29123 Episode reward: 25.0 Average reward: 56.113680154142585 Loss: 0.0011587749468162656 Epsilon: 0.14348046373471363\n",
      "519/1000 Total steps: 29227 Episode reward: 104.0 Average reward: 56.20576923076923 Loss: 0.001889707869850099 Epsilon: 0.14303061020490015\n",
      "520/1000 Total steps: 29434 Episode reward: 207.0 Average reward: 56.495201535508635 Loss: 0.00029194203671067953 Epsilon: 0.14214903238283239\n",
      "521/1000 Total steps: 29676 Episode reward: 242.0 Average reward: 56.85057471264368 Loss: 0.0018341366667300463 Epsilon: 0.14114126891881862\n",
      "522/1000 Total steps: 29757 Episode reward: 81.0 Average reward: 56.896749521988525 Loss: 0.00019131138105876744 Epsilon: 0.14080937064324411\n",
      "523/1000 Total steps: 29955 Episode reward: 198.0 Average reward: 57.166030534351144 Loss: 0.0003644262033049017 Epsilon: 0.14000929202126328\n",
      "524/1000 Total steps: 30153 Episode reward: 198.0 Average reward: 57.434285714285714 Loss: 0.0008539739064872265 Epsilon: 0.13922489915457004\n",
      "525/1000 Total steps: 30325 Episode reward: 172.0 Average reward: 57.652091254752854 Loss: 0.0002855357015505433 Epsilon: 0.1385559999131027\n",
      "526/1000 Total steps: 30527 Episode reward: 202.0 Average reward: 57.92599620493359 Loss: 0.0014668313087895513 Epsilon: 0.1377849822106465\n",
      "527/1000 Total steps: 30643 Episode reward: 116.0 Average reward: 58.03598484848485 Loss: 0.0013373933034017682 Epsilon: 0.13734920878930842\n",
      "528/1000 Total steps: 30786 Episode reward: 143.0 Average reward: 58.19659735349716 Loss: 0.00015115304267965257 Epsilon: 0.13681891573556038\n",
      "529/1000 Total steps: 30895 Episode reward: 109.0 Average reward: 58.29245283018868 Loss: 0.00042962434235960245 Epsilon: 0.136419768856413\n",
      "530/1000 Total steps: 30909 Episode reward: 14.0 Average reward: 58.2090395480226 Loss: 0.00035490680602379143 Epsilon: 0.13636881685473737\n",
      "531/1000 Total steps: 31174 Episode reward: 265.0 Average reward: 58.597744360902254 Loss: 0.00040801771683618426 Epsilon: 0.1354177011505988\n",
      "532/1000 Total steps: 31295 Episode reward: 121.0 Average reward: 58.71482176360225 Loss: 0.0006731825997121632 Epsilon: 0.1349917292936103\n",
      "533/1000 Total steps: 31318 Episode reward: 23.0 Average reward: 58.647940074906366 Loss: 0.00015028563211672008 Epsilon: 0.13491134079844236\n",
      "534/1000 Total steps: 31349 Episode reward: 31.0 Average reward: 58.59626168224299 Loss: 0.0002706238883547485 Epsilon: 0.13480328321775337\n",
      "535/1000 Total steps: 31499 Episode reward: 150.0 Average reward: 58.76679104477612 Loss: 0.0008389184367842972 Epsilon: 0.13428512983519572\n",
      "536/1000 Total steps: 31633 Episode reward: 134.0 Average reward: 58.906890130353815 Loss: 0.0028617242351174355 Epsilon: 0.13382877351136513\n",
      "537/1000 Total steps: 31646 Episode reward: 13.0 Average reward: 58.82156133828996 Loss: 0.00040292006451636553 Epsilon: 0.13378482467873104\n",
      "538/1000 Total steps: 31755 Episode reward: 109.0 Average reward: 58.91465677179963 Loss: 0.0003480232262518257 Epsilon: 0.1334185698050159\n",
      "539/1000 Total steps: 31821 Episode reward: 66.0 Average reward: 58.92777777777778 Loss: 0.00023138710821513087 Epsilon: 0.1331987335021076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540/1000 Total steps: 31938 Episode reward: 117.0 Average reward: 59.03512014787431 Loss: 0.0010837833397090435 Epsilon: 0.13281257177138753\n",
      "541/1000 Total steps: 32085 Episode reward: 147.0 Average reward: 59.19741697416974 Loss: 0.00028229167219251394 Epsilon: 0.1323337548926706\n",
      "542/1000 Total steps: 32150 Episode reward: 65.0 Average reward: 59.20810313075506 Loss: 0.00044968092697672546 Epsilon: 0.13212426705889924\n",
      "543/1000 Total steps: 32263 Episode reward: 113.0 Average reward: 59.306985294117645 Loss: 0.0016060794005170465 Epsilon: 0.1317633061114041\n",
      "544/1000 Total steps: 32416 Episode reward: 153.0 Average reward: 59.47889908256881 Loss: 0.0009706945857033134 Epsilon: 0.13128102637591138\n",
      "545/1000 Total steps: 32540 Episode reward: 124.0 Average reward: 59.5970695970696 Loss: 0.0007499767816625535 Epsilon: 0.1308955366247036\n",
      "546/1000 Total steps: 32665 Episode reward: 125.0 Average reward: 59.716636197440586 Loss: 0.00031287013553082943 Epsilon: 0.13051174610490293\n",
      "547/1000 Total steps: 33026 Episode reward: 361.0 Average reward: 60.26642335766423 Loss: 0.0003888795617967844 Epsilon: 0.12942991657850378\n",
      "548/1000 Total steps: 33097 Episode reward: 71.0 Average reward: 60.28597449908926 Loss: 0.00021140187163837254 Epsilon: 0.12922170419940696\n",
      "549/1000 Total steps: 33142 Episode reward: 45.0 Average reward: 60.25818181818182 Loss: 0.00193966762162745 Epsilon: 0.12909050195695884\n",
      "550/1000 Total steps: 33192 Episode reward: 50.0 Average reward: 60.23956442831216 Loss: 0.000387161155231297 Epsilon: 0.1289454124731532\n",
      "551/1000 Total steps: 33359 Episode reward: 167.0 Average reward: 60.43297101449275 Loss: 0.0006294400081969798 Epsilon: 0.1284660380026903\n",
      "552/1000 Total steps: 33456 Episode reward: 97.0 Average reward: 60.499095840867994 Loss: 0.00032086652936413884 Epsilon: 0.12819125229927128\n",
      "553/1000 Total steps: 33626 Episode reward: 170.0 Average reward: 60.69675090252708 Loss: 0.0026449174620211124 Epsilon: 0.12771605165997796\n",
      "554/1000 Total steps: 33740 Episode reward: 114.0 Average reward: 60.792792792792795 Loss: 0.00034752883948385715 Epsilon: 0.12740188283579304\n",
      "555/1000 Total steps: 33870 Episode reward: 130.0 Average reward: 60.91726618705036 Loss: 0.0002362416998948902 Epsilon: 0.12704796381689604\n",
      "556/1000 Total steps: 33888 Episode reward: 18.0 Average reward: 60.84021543985637 Loss: 0.0002309190749656409 Epsilon: 0.12699932127344823\n",
      "557/1000 Total steps: 34006 Episode reward: 118.0 Average reward: 60.9426523297491 Loss: 0.00032986191217787564 Epsilon: 0.12668260160346978\n",
      "558/1000 Total steps: 34042 Episode reward: 36.0 Average reward: 60.89803220035778 Loss: 0.00016279687406495214 Epsilon: 0.12658671693365836\n",
      "559/1000 Total steps: 34253 Episode reward: 211.0 Average reward: 61.16607142857143 Loss: 0.00026951191830448806 Epsilon: 0.12603161413550232\n",
      "560/1000 Total steps: 34609 Episode reward: 356.0 Average reward: 61.69162210338681 Loss: 0.00011524152068886906 Epsilon: 0.12512119036622957\n",
      "561/1000 Total steps: 34653 Episode reward: 44.0 Average reward: 61.66014234875445 Loss: 0.0004178081580903381 Epsilon: 0.12501089994547895\n",
      "562/1000 Total steps: 34808 Episode reward: 155.0 Average reward: 61.825932504440495 Loss: 0.0032381340861320496 Epsilon: 0.12462621996773443\n",
      "563/1000 Total steps: 34830 Episode reward: 22.0 Average reward: 61.755319148936174 Loss: 0.00026887451531365514 Epsilon: 0.12457210183557843\n",
      "564/1000 Total steps: 34946 Episode reward: 116.0 Average reward: 61.85132743362832 Loss: 0.0005494517390616238 Epsilon: 0.12428871229137639\n",
      "565/1000 Total steps: 35067 Episode reward: 121.0 Average reward: 61.95583038869258 Loss: 0.0005167989293113351 Epsilon: 0.12399658977798615\n",
      "566/1000 Total steps: 35209 Episode reward: 142.0 Average reward: 62.09700176366843 Loss: 0.000311599433189258 Epsilon: 0.1236582461283333\n",
      "567/1000 Total steps: 35226 Episode reward: 17.0 Average reward: 62.017605633802816 Loss: 0.0005918331444263458 Epsilon: 0.12361806127671686\n",
      "568/1000 Total steps: 35343 Episode reward: 117.0 Average reward: 62.114235500878735 Loss: 0.000390490546124056 Epsilon: 0.12334334021188195\n",
      "569/1000 Total steps: 35421 Episode reward: 78.0 Average reward: 62.142105263157895 Loss: 0.0001068589772330597 Epsilon: 0.12316197041996167\n",
      "570/1000 Total steps: 35460 Episode reward: 39.0 Average reward: 62.101576182136604 Loss: 0.0009959978051483631 Epsilon: 0.12307181465334115\n",
      "571/1000 Total steps: 35680 Episode reward: 220.0 Average reward: 62.37762237762238 Loss: 0.0014534731162711978 Epsilon: 0.12256977738954239\n",
      "572/1000 Total steps: 35779 Episode reward: 99.0 Average reward: 62.44153577661431 Loss: 0.00030914039234630764 Epsilon: 0.12234743898443715\n",
      "573/1000 Total steps: 35891 Episode reward: 112.0 Average reward: 62.52787456445993 Loss: 0.0005167719209566712 Epsilon: 0.1220985440810464\n",
      "574/1000 Total steps: 36103 Episode reward: 212.0 Average reward: 62.78782608695652 Loss: 0.0008582600858062506 Epsilon: 0.12163498602360151\n",
      "575/1000 Total steps: 36242 Episode reward: 139.0 Average reward: 62.920138888888886 Loss: 0.00014742786879651248 Epsilon: 0.12133634011536819\n",
      "576/1000 Total steps: 36369 Episode reward: 127.0 Average reward: 63.031195840554595 Loss: 0.00025744966114871204 Epsilon: 0.12106708200395416\n",
      "577/1000 Total steps: 36549 Episode reward: 180.0 Average reward: 63.233564013840834 Loss: 0.0011910265311598778 Epsilon: 0.1206912670097806\n",
      "578/1000 Total steps: 36565 Episode reward: 16.0 Average reward: 63.151986183074264 Loss: 0.0016042294446378946 Epsilon: 0.12065818745326713\n",
      "579/1000 Total steps: 36634 Episode reward: 69.0 Average reward: 63.16206896551724 Loss: 0.0019107997650280595 Epsilon: 0.12051613659887356\n",
      "580/1000 Total steps: 36778 Episode reward: 144.0 Average reward: 63.30120481927711 Loss: 0.00283022690564394 Epsilon: 0.12022282117140067\n",
      "581/1000 Total steps: 36803 Episode reward: 25.0 Average reward: 63.235395189003434 Loss: 0.001150760450400412 Epsilon: 0.12017232726215762\n",
      "582/1000 Total steps: 36828 Episode reward: 25.0 Average reward: 63.16981132075472 Loss: 0.002716081915423274 Epsilon: 0.12012195943002564\n",
      "583/1000 Total steps: 36869 Episode reward: 41.0 Average reward: 63.13184931506849 Loss: 0.0007610698812641203 Epsilon: 0.12003962829053066\n",
      "584/1000 Total steps: 36968 Episode reward: 99.0 Average reward: 63.193162393162396 Loss: 0.002005364978685975 Epsilon: 0.11984221477970522\n",
      "585/1000 Total steps: 37038 Episode reward: 70.0 Average reward: 63.20477815699659 Loss: 0.00010586017742753029 Epsilon: 0.11970380427817837\n",
      "586/1000 Total steps: 37067 Episode reward: 29.0 Average reward: 63.146507666098806 Loss: 0.00045998202404007316 Epsilon: 0.11964674602023401\n",
      "587/1000 Total steps: 37186 Episode reward: 119.0 Average reward: 63.24149659863946 Loss: 0.0013762125745415688 Epsilon: 0.11941433532884037\n",
      "588/1000 Total steps: 37249 Episode reward: 63.0 Average reward: 63.241086587436335 Loss: 0.003765396773815155 Epsilon: 0.11929240948594326\n",
      "589/1000 Total steps: 37633 Episode reward: 384.0 Average reward: 63.784745762711864 Loss: 0.0005183865432627499 Epsilon: 0.11856562453782657\n",
      "590/1000 Total steps: 37721 Episode reward: 88.0 Average reward: 63.82571912013536 Loss: 0.000360923761036247 Epsilon: 0.11840296379884781\n",
      "591/1000 Total steps: 37788 Episode reward: 67.0 Average reward: 63.83108108108108 Loss: 0.0006690968293696642 Epsilon: 0.11828007607497266\n",
      "592/1000 Total steps: 37956 Episode reward: 168.0 Average reward: 64.00674536256324 Loss: 0.00023617109400220215 Epsilon: 0.11797553609548743\n",
      "593/1000 Total steps: 38087 Episode reward: 131.0 Average reward: 64.11952861952862 Loss: 0.0011125117307528853 Epsilon: 0.1177415922504042\n",
      "594/1000 Total steps: 38194 Episode reward: 107.0 Average reward: 64.19159663865547 Loss: 0.0021311547607183456 Epsilon: 0.11755276921807349\n",
      "595/1000 Total steps: 38305 Episode reward: 111.0 Average reward: 64.27013422818791 Loss: 0.0004795386048499495 Epsilon: 0.11735901082822671\n",
      "596/1000 Total steps: 38412 Episode reward: 107.0 Average reward: 64.34170854271356 Loss: 0.0002066328888759017 Epsilon: 0.11717425959414439\n",
      "597/1000 Total steps: 38575 Episode reward: 163.0 Average reward: 64.5066889632107 Loss: 0.0005453533376567066 Epsilon: 0.11689658833139716\n",
      "598/1000 Total steps: 38716 Episode reward: 141.0 Average reward: 64.63439065108514 Loss: 0.0009226793190464377 Epsilon: 0.11666001817489091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599/1000 Total steps: 38739 Episode reward: 23.0 Average reward: 64.565 Loss: 0.0025192461907863617 Epsilon: 0.1166217441650724\n",
      "600/1000 Total steps: 38840 Episode reward: 101.0 Average reward: 64.62562396006656 Loss: 0.0004016577731817961 Epsilon: 0.11645470949402545\n",
      "601/1000 Total steps: 39056 Episode reward: 216.0 Average reward: 64.87707641196013 Loss: 0.00027495596441440284 Epsilon: 0.11610309883459251\n",
      "602/1000 Total steps: 39170 Episode reward: 114.0 Average reward: 64.95854063018243 Loss: 0.0007930509746074677 Epsilon: 0.11592056592230535\n",
      "603/1000 Total steps: 39221 Episode reward: 51.0 Average reward: 64.93543046357615 Loss: 0.0010893573053181171 Epsilon: 0.11583957773152989\n",
      "604/1000 Total steps: 39244 Episode reward: 23.0 Average reward: 64.86611570247933 Loss: 0.0038278468418866396 Epsilon: 0.11580318856632892\n",
      "605/1000 Total steps: 39422 Episode reward: 178.0 Average reward: 65.05280528052805 Loss: 0.0002587757771834731 Epsilon: 0.11552438056250408\n",
      "606/1000 Total steps: 39542 Episode reward: 120.0 Average reward: 65.14332784184514 Loss: 0.0022974673192948103 Epsilon: 0.11533920129351387\n",
      "607/1000 Total steps: 39564 Episode reward: 22.0 Average reward: 65.07236842105263 Loss: 0.0018966307397931814 Epsilon: 0.11530549214432828\n",
      "608/1000 Total steps: 39686 Episode reward: 122.0 Average reward: 65.16584564860428 Loss: 0.00020517683879006654 Epsilon: 0.11511989955691172\n",
      "609/1000 Total steps: 39696 Episode reward: 10.0 Average reward: 65.07540983606557 Loss: 0.0013455877779051661 Epsilon: 0.11510478721478525\n",
      "610/1000 Total steps: 39938 Episode reward: 242.0 Average reward: 65.36497545008183 Loss: 0.002610942581668496 Epsilon: 0.11474363888405956\n",
      "611/1000 Total steps: 40064 Episode reward: 126.0 Average reward: 65.4640522875817 Loss: 0.00023880822118371725 Epsilon: 0.11455903448414959\n",
      "612/1000 Total steps: 40084 Episode reward: 20.0 Average reward: 65.38988580750407 Loss: 0.00016202524420805275 Epsilon: 0.1145299455138479\n",
      "613/1000 Total steps: 40108 Episode reward: 24.0 Average reward: 65.32247557003258 Loss: 0.0013733579544350505 Epsilon: 0.11449511545740083\n",
      "614/1000 Total steps: 40264 Episode reward: 156.0 Average reward: 65.46991869918699 Loss: 0.00014881964307278395 Epsilon: 0.11427074628599072\n",
      "615/1000 Total steps: 40282 Episode reward: 18.0 Average reward: 65.39285714285714 Loss: 0.00034151709405705333 Epsilon: 0.11424508204742\n",
      "616/1000 Total steps: 40329 Episode reward: 47.0 Average reward: 65.36304700162074 Loss: 0.0005961082642897964 Epsilon: 0.11417828725252317\n",
      "617/1000 Total steps: 40491 Episode reward: 162.0 Average reward: 65.51941747572816 Loss: 0.00016432350093964487 Epsilon: 0.11395044946787847\n",
      "618/1000 Total steps: 40589 Episode reward: 98.0 Average reward: 65.5718901453958 Loss: 0.0002524708106648177 Epsilon: 0.11381440278068575\n",
      "619/1000 Total steps: 40677 Episode reward: 88.0 Average reward: 65.60806451612903 Loss: 9.12876712391153e-05 Epsilon: 0.1136933693643157\n",
      "620/1000 Total steps: 40814 Episode reward: 137.0 Average reward: 65.72302737520128 Loss: 0.0003520952886901796 Epsilon: 0.11350704940990256\n",
      "621/1000 Total steps: 40860 Episode reward: 46.0 Average reward: 65.69131832797427 Loss: 0.0006091947434470057 Epsilon: 0.11344505966833116\n",
      "622/1000 Total steps: 40890 Episode reward: 30.0 Average reward: 65.63402889245586 Loss: 0.00233558495528996 Epsilon: 0.11340478493163725\n",
      "623/1000 Total steps: 41020 Episode reward: 130.0 Average reward: 65.73717948717949 Loss: 0.000911140232346952 Epsilon: 0.11323165053937814\n",
      "624/1000 Total steps: 41138 Episode reward: 118.0 Average reward: 65.8208 Loss: 0.0012210909044370055 Epsilon: 0.11307643463785017\n",
      "625/1000 Total steps: 41215 Episode reward: 77.0 Average reward: 65.83865814696486 Loss: 0.0012506446801126003 Epsilon: 0.11297613274898527\n",
      "626/1000 Total steps: 41377 Episode reward: 162.0 Average reward: 65.99202551834131 Loss: 0.00026441531372256577 Epsilon: 0.11276761296897742\n",
      "627/1000 Total steps: 41440 Episode reward: 63.0 Average reward: 65.98726114649682 Loss: 9.065114863915369e-05 Epsilon: 0.11268742984930531\n",
      "628/1000 Total steps: 41501 Episode reward: 61.0 Average reward: 65.97933227344993 Loss: 0.00010592013859422877 Epsilon: 0.11261027209762037\n",
      "629/1000 Total steps: 41603 Episode reward: 102.0 Average reward: 66.03650793650793 Loss: 0.0004261568537913263 Epsilon: 0.11248230108390138\n",
      "630/1000 Total steps: 41783 Episode reward: 180.0 Average reward: 66.21711568938193 Loss: 0.0018465315224602818 Epsilon: 0.11225962971877171\n",
      "631/1000 Total steps: 41953 Episode reward: 170.0 Average reward: 66.38132911392405 Loss: 0.0012105355272069573 Epsilon: 0.11205297753397286\n",
      "632/1000 Total steps: 42039 Episode reward: 86.0 Average reward: 66.41232227488152 Loss: 0.00022774655371904373 Epsilon: 0.11194976637130419\n",
      "633/1000 Total steps: 42107 Episode reward: 68.0 Average reward: 66.41482649842271 Loss: 0.00042998159187845886 Epsilon: 0.11186878361340948\n",
      "634/1000 Total steps: 42132 Episode reward: 25.0 Average reward: 66.3496062992126 Loss: 0.00173083133995533 Epsilon: 0.11183914871343577\n",
      "635/1000 Total steps: 42226 Episode reward: 94.0 Average reward: 66.39308176100629 Loss: 0.00020412271260283887 Epsilon: 0.11172838213406257\n",
      "636/1000 Total steps: 42462 Episode reward: 236.0 Average reward: 66.65934065934066 Loss: 0.00028337552794255316 Epsilon: 0.11145483289295799\n",
      "637/1000 Total steps: 42478 Episode reward: 16.0 Average reward: 66.57993730407523 Loss: 0.0002758775372058153 Epsilon: 0.11143651981469865\n",
      "638/1000 Total steps: 42641 Episode reward: 163.0 Average reward: 66.73082942097027 Loss: 0.0019786630291491747 Epsilon: 0.11125161560494373\n",
      "639/1000 Total steps: 42687 Episode reward: 46.0 Average reward: 66.6984375 Loss: 0.00040294977952726185 Epsilon: 0.11119997703293259\n",
      "640/1000 Total steps: 42732 Episode reward: 45.0 Average reward: 66.66458658346333 Loss: 0.00015848393377382308 Epsilon: 0.11114969036614339\n",
      "641/1000 Total steps: 42845 Episode reward: 113.0 Average reward: 66.7367601246106 Loss: 0.0005873892805539072 Epsilon: 0.11102440804323588\n",
      "642/1000 Total steps: 42996 Episode reward: 151.0 Average reward: 66.86780715396579 Loss: 0.00037070331745781004 Epsilon: 0.11085919001714822\n",
      "643/1000 Total steps: 43113 Episode reward: 117.0 Average reward: 66.94565217391305 Loss: 0.0003774738288484514 Epsilon: 0.1107328778609639\n",
      "644/1000 Total steps: 43168 Episode reward: 55.0 Average reward: 66.92713178294574 Loss: 0.0012453714152798057 Epsilon: 0.11067400907030127\n",
      "645/1000 Total steps: 43187 Episode reward: 19.0 Average reward: 66.8529411764706 Loss: 0.002500394592061639 Epsilon: 0.11065374770745769\n",
      "646/1000 Total steps: 43233 Episode reward: 46.0 Average reward: 66.82071097372489 Loss: 0.000581642147153616 Epsilon: 0.1106048530120205\n",
      "647/1000 Total steps: 43398 Episode reward: 165.0 Average reward: 66.97222222222223 Loss: 0.0006544770440086722 Epsilon: 0.11043130861586109\n",
      "648/1000 Total steps: 43499 Episode reward: 101.0 Average reward: 67.0246533127889 Loss: 0.00023943051928654313 Epsilon: 0.11032648266101931\n",
      "649/1000 Total steps: 43578 Episode reward: 79.0 Average reward: 67.04307692307692 Loss: 0.0033129355870187283 Epsilon: 0.11024522483900218\n",
      "650/1000 Total steps: 43661 Episode reward: 83.0 Average reward: 67.06758832565284 Loss: 0.00034307377063669264 Epsilon: 0.11016054139528286\n",
      "651/1000 Total steps: 43734 Episode reward: 73.0 Average reward: 67.07668711656441 Loss: 0.0006707248976454139 Epsilon: 0.11008663951315273\n",
      "652/1000 Total steps: 43882 Episode reward: 148.0 Average reward: 67.20061255742726 Loss: 0.00018378111417405307 Epsilon: 0.10993845650742438\n",
      "653/1000 Total steps: 44030 Episode reward: 148.0 Average reward: 67.32415902140673 Loss: 0.0029767178930342197 Epsilon: 0.10979245046094575\n",
      "654/1000 Total steps: 44111 Episode reward: 81.0 Average reward: 67.34503816793894 Loss: 0.0005263626226224005 Epsilon: 0.10971345198795139\n",
      "655/1000 Total steps: 44157 Episode reward: 46.0 Average reward: 67.3125 Loss: 0.0002843427355401218 Epsilon: 0.1096688727197318\n",
      "656/1000 Total steps: 44230 Episode reward: 73.0 Average reward: 67.32115677321157 Loss: 0.0011624705512076616 Epsilon: 0.10959854695024115\n",
      "657/1000 Total steps: 44364 Episode reward: 134.0 Average reward: 67.4224924012158 Loss: 0.0003685997799038887 Epsilon: 0.1094707843423297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658/1000 Total steps: 44472 Episode reward: 108.0 Average reward: 67.48406676783004 Loss: 2.723698526096996e-05 Epsilon: 0.1093690502245224\n",
      "659/1000 Total steps: 44495 Episode reward: 23.0 Average reward: 67.41666666666667 Loss: 0.00052096345461905 Epsilon: 0.1093475261711559\n",
      "660/1000 Total steps: 44598 Episode reward: 103.0 Average reward: 67.47049924357034 Loss: 0.0002165471960324794 Epsilon: 0.10925174079311097\n",
      "661/1000 Total steps: 44801 Episode reward: 203.0 Average reward: 67.67522658610272 Loss: 0.0001536139752715826 Epsilon: 0.10906582389601369\n",
      "662/1000 Total steps: 45036 Episode reward: 235.0 Average reward: 67.92760180995475 Loss: 0.00032657990232110023 Epsilon: 0.10885526084055708\n",
      "663/1000 Total steps: 45150 Episode reward: 114.0 Average reward: 67.99698795180723 Loss: 0.0026180578861385584 Epsilon: 0.10875488410146528\n",
      "664/1000 Total steps: 45281 Episode reward: 131.0 Average reward: 68.09172932330827 Loss: 0.0006092258845455945 Epsilon: 0.10864094306298526\n",
      "665/1000 Total steps: 45333 Episode reward: 52.0 Average reward: 68.06756756756756 Loss: 0.0014389253919944167 Epsilon: 0.1085961267823733\n",
      "666/1000 Total steps: 45450 Episode reward: 117.0 Average reward: 68.14092953523239 Loss: 0.00022228423040360212 Epsilon: 0.10849613817300183\n",
      "667/1000 Total steps: 45495 Episode reward: 45.0 Average reward: 68.1062874251497 Loss: 0.00022307431208901107 Epsilon: 0.10845799144573225\n",
      "668/1000 Total steps: 45736 Episode reward: 241.0 Average reward: 68.36472346786248 Loss: 0.0019886111840605736 Epsilon: 0.10825659048139057\n",
      "669/1000 Total steps: 46079 Episode reward: 343.0 Average reward: 68.77462686567164 Loss: 0.002654933836311102 Epsilon: 0.10797819126833838\n",
      "670/1000 Total steps: 46566 Episode reward: 487.0 Average reward: 69.39791356184799 Loss: 0.002412085421383381 Epsilon: 0.10759896252167457\n",
      "671/1000 Total steps: 46668 Episode reward: 102.0 Average reward: 69.44642857142857 Loss: 0.00021417150855995715 Epsilon: 0.10752184706139081\n",
      "672/1000 Total steps: 46931 Episode reward: 263.0 Average reward: 69.73402674591382 Loss: 0.0006655547767877579 Epsilon: 0.10732660122048736\n",
      "673/1000 Total steps: 47022 Episode reward: 91.0 Average reward: 69.76557863501483 Loss: 0.0013830247335135937 Epsilon: 0.10726023158920837\n",
      "674/1000 Total steps: 47051 Episode reward: 29.0 Average reward: 69.70518518518519 Loss: 0.0010235515655949712 Epsilon: 0.10723920741738326\n",
      "675/1000 Total steps: 47196 Episode reward: 145.0 Average reward: 69.81656804733728 Loss: 0.0014597887638956308 Epsilon: 0.10713499626653467\n",
      "676/1000 Total steps: 47327 Episode reward: 131.0 Average reward: 69.9069423929099 Loss: 0.00014301319606602192 Epsilon: 0.10704213736917656\n",
      "677/1000 Total steps: 47495 Episode reward: 168.0 Average reward: 70.05162241887906 Loss: 0.0002854631165973842 Epsilon: 0.10692481770589149\n",
      "678/1000 Total steps: 47594 Episode reward: 99.0 Average reward: 70.09425625920471 Loss: 0.00033753359457477927 Epsilon: 0.10685660024420374\n",
      "679/1000 Total steps: 47696 Episode reward: 102.0 Average reward: 70.14117647058823 Loss: 0.00014549748448189348 Epsilon: 0.1067870183924305\n",
      "680/1000 Total steps: 47803 Episode reward: 107.0 Average reward: 70.19530102790014 Loss: 0.0024872857611626387 Epsilon: 0.10671478443646672\n",
      "681/1000 Total steps: 47915 Episode reward: 112.0 Average reward: 70.2565982404692 Loss: 0.001043133088387549 Epsilon: 0.10663999843415263\n",
      "682/1000 Total steps: 47941 Episode reward: 26.0 Average reward: 70.19180087847731 Loss: 0.000182580784894526 Epsilon: 0.10662275686198042\n",
      "683/1000 Total steps: 48055 Episode reward: 114.0 Average reward: 70.25584795321637 Loss: 0.001160627929493785 Epsilon: 0.10654768614982715\n",
      "684/1000 Total steps: 48186 Episode reward: 131.0 Average reward: 70.34452554744526 Loss: 0.00027115456759929657 Epsilon: 0.106462470840189\n",
      "685/1000 Total steps: 48461 Episode reward: 275.0 Average reward: 70.64285714285714 Loss: 0.0017924703424796462 Epsilon: 0.10628717426715983\n",
      "686/1000 Total steps: 48541 Episode reward: 80.0 Average reward: 70.65647743813683 Loss: 0.0022988533601164818 Epsilon: 0.10623707752716487\n",
      "687/1000 Total steps: 48720 Episode reward: 179.0 Average reward: 70.81395348837209 Loss: 0.0009748084121383727 Epsilon: 0.10612642711505939\n",
      "688/1000 Total steps: 48755 Episode reward: 35.0 Average reward: 70.76197387518143 Loss: 0.0004745229962281883 Epsilon: 0.10610502210078263\n",
      "689/1000 Total steps: 48834 Episode reward: 79.0 Average reward: 70.77391304347826 Loss: 0.0004859293403569609 Epsilon: 0.10605698243272133\n",
      "690/1000 Total steps: 48845 Episode reward: 11.0 Average reward: 70.68740955137481 Loss: 0.00031188802677206695 Epsilon: 0.10605032341517644\n",
      "691/1000 Total steps: 48934 Episode reward: 89.0 Average reward: 70.71387283236994 Loss: 0.0004452498396858573 Epsilon: 0.10599671445053738\n",
      "692/1000 Total steps: 49237 Episode reward: 303.0 Average reward: 71.04906204906204 Loss: 0.0006801357376389205 Epsilon: 0.10581773917091408\n",
      "693/1000 Total steps: 49361 Episode reward: 124.0 Average reward: 71.12536023054756 Loss: 0.0002619861625134945 Epsilon: 0.10574604462999215\n",
      "694/1000 Total steps: 49505 Episode reward: 144.0 Average reward: 71.23021582733813 Loss: 0.0007705752504989505 Epsilon: 0.10566389448789293\n",
      "695/1000 Total steps: 49595 Episode reward: 90.0 Average reward: 71.25718390804597 Loss: 0.0035892012529075146 Epsilon: 0.10561314813861106\n",
      "696/1000 Total steps: 49660 Episode reward: 65.0 Average reward: 71.24820659971306 Loss: 0.00042369216680526733 Epsilon: 0.105576780996963\n",
      "697/1000 Total steps: 49860 Episode reward: 200.0 Average reward: 71.432664756447 Loss: 0.00014569598715752363 Epsilon: 0.10546635333454546\n",
      "698/1000 Total steps: 50096 Episode reward: 236.0 Average reward: 71.6680972818312 Loss: 0.0002665255742613226 Epsilon: 0.10533885776105698\n",
      "699/1000 Total steps: 50127 Episode reward: 31.0 Average reward: 71.61 Loss: 0.0002359356585657224 Epsilon: 0.10532233292872145\n",
      "700/1000 Total steps: 50138 Episode reward: 11.0 Average reward: 71.52353780313837 Loss: 0.00018462697335053235 Epsilon: 0.10531648158133093\n",
      "701/1000 Total steps: 50201 Episode reward: 63.0 Average reward: 71.51139601139602 Loss: 0.0006221097428351641 Epsilon: 0.10528309303173233\n",
      "702/1000 Total steps: 50410 Episode reward: 209.0 Average reward: 71.70697012802276 Loss: 0.0004401251790113747 Epsilon: 0.10517382224461327\n",
      "703/1000 Total steps: 50569 Episode reward: 159.0 Average reward: 71.8309659090909 Loss: 0.0024393077474087477 Epsilon: 0.10509220901547503\n",
      "704/1000 Total steps: 50713 Episode reward: 144.0 Average reward: 71.93333333333334 Loss: 0.0035919335205107927 Epsilon: 0.10501940664077075\n",
      "705/1000 Total steps: 50837 Episode reward: 124.0 Average reward: 72.0070821529745 Loss: 0.0024402476847171783 Epsilon: 0.10495755030031986\n",
      "706/1000 Total steps: 50967 Episode reward: 130.0 Average reward: 72.08910891089108 Loss: 0.00011153243394801393 Epsilon: 0.10489351925001081\n",
      "707/1000 Total steps: 51101 Episode reward: 134.0 Average reward: 72.17655367231639 Loss: 0.0011494173668324947 Epsilon: 0.1048283834763893\n",
      "708/1000 Total steps: 51315 Episode reward: 214.0 Average reward: 72.37658674188998 Loss: 0.0015286147827282548 Epsilon: 0.10472615382862005\n",
      "709/1000 Total steps: 51401 Episode reward: 86.0 Average reward: 72.39577464788732 Loss: 0.0004380222235340625 Epsilon: 0.10468568317892141\n",
      "710/1000 Total steps: 51499 Episode reward: 98.0 Average reward: 72.43178621659634 Loss: 0.00021729833679273725 Epsilon: 0.10463998775705026\n",
      "711/1000 Total steps: 51561 Episode reward: 62.0 Average reward: 72.41713483146067 Loss: 0.001144733396358788 Epsilon: 0.10461130882950005\n",
      "712/1000 Total steps: 51675 Episode reward: 114.0 Average reward: 72.47545582047685 Loss: 0.000346108601661399 Epsilon: 0.10455903841628643\n",
      "713/1000 Total steps: 51897 Episode reward: 222.0 Average reward: 72.68487394957984 Loss: 0.0007038413314148784 Epsilon: 0.10445894293418388\n",
      "714/1000 Total steps: 52059 Episode reward: 162.0 Average reward: 72.80979020979021 Loss: 0.00014163521700538695 Epsilon: 0.10438729001434333\n",
      "715/1000 Total steps: 52199 Episode reward: 140.0 Average reward: 72.9036312849162 Loss: 0.00024385972938034683 Epsilon: 0.10432629590911292\n",
      "716/1000 Total steps: 52393 Episode reward: 194.0 Average reward: 73.07252440725244 Loss: 0.00014350333367474377 Epsilon: 0.1042431746516172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/1000 Total steps: 52429 Episode reward: 36.0 Average reward: 73.02089136490251 Loss: 0.0021011405624449253 Epsilon: 0.10422792668567786\n",
      "718/1000 Total steps: 52490 Episode reward: 61.0 Average reward: 73.00417246175243 Loss: 0.0004335020203143358 Epsilon: 0.10420221483377166\n",
      "719/1000 Total steps: 52620 Episode reward: 130.0 Average reward: 73.08333333333333 Loss: 0.0014096556697040796 Epsilon: 0.10414793959436293\n",
      "720/1000 Total steps: 52700 Episode reward: 80.0 Average reward: 73.09292649098474 Loss: 0.0014929927419871092 Epsilon: 0.10411488845842432\n",
      "721/1000 Total steps: 52806 Episode reward: 106.0 Average reward: 73.13850415512465 Loss: 0.0012001359136775136 Epsilon: 0.10407150100054226\n",
      "722/1000 Total steps: 52878 Episode reward: 72.0 Average reward: 73.13692946058092 Loss: 0.00019185642304364592 Epsilon: 0.1040422914738196\n",
      "723/1000 Total steps: 52957 Episode reward: 79.0 Average reward: 73.14502762430939 Loss: 0.00024832403869368136 Epsilon: 0.10401048317936898\n",
      "724/1000 Total steps: 53084 Episode reward: 127.0 Average reward: 73.21931034482759 Loss: 0.0004967425484210253 Epsilon: 0.10395987210357548\n",
      "725/1000 Total steps: 53115 Episode reward: 31.0 Average reward: 73.16115702479338 Loss: 0.00035907741403207183 Epsilon: 0.10394761550759365\n",
      "726/1000 Total steps: 53280 Episode reward: 165.0 Average reward: 73.28748280605227 Loss: 0.00013181426038499922 Epsilon: 0.1038830142775004\n",
      "727/1000 Total steps: 53318 Episode reward: 38.0 Average reward: 73.23901098901099 Loss: 0.00015113590052351356 Epsilon: 0.10386828682313122\n",
      "728/1000 Total steps: 53381 Episode reward: 63.0 Average reward: 73.2249657064472 Loss: 0.0002778168418444693 Epsilon: 0.10384399322134216\n",
      "729/1000 Total steps: 53482 Episode reward: 101.0 Average reward: 73.26301369863013 Loss: 0.0008281469345092773 Epsilon: 0.10380536429426586\n",
      "730/1000 Total steps: 53618 Episode reward: 136.0 Average reward: 73.34883720930233 Loss: 0.0006426110630854964 Epsilon: 0.10375396166999225\n",
      "731/1000 Total steps: 53628 Episode reward: 10.0 Average reward: 73.26229508196721 Loss: 0.0007163049886003137 Epsilon: 0.10375020958467758\n",
      "732/1000 Total steps: 53857 Episode reward: 229.0 Average reward: 73.47476125511596 Loss: 0.0010598859516903758 Epsilon: 0.103665305645631\n",
      "733/1000 Total steps: 53878 Episode reward: 21.0 Average reward: 73.40326975476839 Loss: 0.0002979258424602449 Epsilon: 0.1036576165801197\n",
      "734/1000 Total steps: 54100 Episode reward: 222.0 Average reward: 73.60544217687075 Loss: 0.0007121656672097743 Epsilon: 0.10357731216907867\n",
      "735/1000 Total steps: 54118 Episode reward: 18.0 Average reward: 73.52989130434783 Loss: 0.00038427475374192 Epsilon: 0.10357087879894446\n",
      "736/1000 Total steps: 54148 Episode reward: 30.0 Average reward: 73.4708276797829 Loss: 0.0002141631703125313 Epsilon: 0.10356018221544531\n",
      "737/1000 Total steps: 54264 Episode reward: 116.0 Average reward: 73.52845528455285 Loss: 0.00011378257477190346 Epsilon: 0.10351912270730627\n",
      "738/1000 Total steps: 54309 Episode reward: 45.0 Average reward: 73.48985115020298 Loss: 0.0003048617800232023 Epsilon: 0.1035033222328542\n",
      "739/1000 Total steps: 54439 Episode reward: 130.0 Average reward: 73.56621621621622 Loss: 8.974943193607032e-05 Epsilon: 0.10345807379591423\n",
      "740/1000 Total steps: 54554 Episode reward: 115.0 Average reward: 73.6221322537112 Loss: 0.00019323505694046617 Epsilon: 0.1034185337383556\n",
      "741/1000 Total steps: 54570 Episode reward: 16.0 Average reward: 73.544474393531 Loss: 0.0007105187978595495 Epsilon: 0.10341306845776464\n",
      "742/1000 Total steps: 54594 Episode reward: 24.0 Average reward: 73.47779273216689 Loss: 0.000270610413281247 Epsilon: 0.10340488691524416\n",
      "743/1000 Total steps: 54604 Episode reward: 10.0 Average reward: 73.39247311827957 Loss: 0.0001169208117062226 Epsilon: 0.10340148373020504\n",
      "744/1000 Total steps: 54682 Episode reward: 78.0 Average reward: 73.3986577181208 Loss: 0.0002436357899568975 Epsilon: 0.10337505536173815\n",
      "745/1000 Total steps: 54716 Episode reward: 34.0 Average reward: 73.34584450402144 Loss: 0.0007199711981229484 Epsilon: 0.10336359965923815\n",
      "746/1000 Total steps: 54862 Episode reward: 146.0 Average reward: 73.44310575635876 Loss: 0.0005557846743613482 Epsilon: 0.10331484785835116\n",
      "747/1000 Total steps: 54917 Episode reward: 55.0 Average reward: 73.41844919786097 Loss: 0.0016491144197061658 Epsilon: 0.10329666624041237\n",
      "748/1000 Total steps: 55044 Episode reward: 127.0 Average reward: 73.48998664886516 Loss: 0.0002890802570618689 Epsilon: 0.10325506331689989\n",
      "749/1000 Total steps: 55206 Episode reward: 162.0 Average reward: 73.608 Loss: 8.248422091128305e-05 Epsilon: 0.10320275612338689\n",
      "750/1000 Total steps: 55299 Episode reward: 93.0 Average reward: 73.63382157123834 Loss: 0.001661738264374435 Epsilon: 0.10317310856626447\n",
      "751/1000 Total steps: 55372 Episode reward: 73.0 Average reward: 73.63297872340425 Loss: 0.00044357689330354333 Epsilon: 0.1031500292158512\n",
      "752/1000 Total steps: 55502 Episode reward: 130.0 Average reward: 73.70783532536521 Loss: 0.000840857857838273 Epsilon: 0.10310934386381712\n",
      "753/1000 Total steps: 55521 Episode reward: 19.0 Average reward: 73.63527851458886 Loss: 0.00040782662108540535 Epsilon: 0.10310344171928873\n",
      "754/1000 Total steps: 55609 Episode reward: 88.0 Average reward: 73.65430463576159 Loss: 0.0013978335773572326 Epsilon: 0.1030762512457117\n",
      "755/1000 Total steps: 55654 Episode reward: 45.0 Average reward: 73.61640211640211 Loss: 0.00025381302111782134 Epsilon: 0.1030624392154818\n",
      "756/1000 Total steps: 55908 Episode reward: 254.0 Average reward: 73.85468956406869 Loss: 0.0013425671495497227 Epsilon: 0.10298563282982927\n",
      "757/1000 Total steps: 55933 Episode reward: 25.0 Average reward: 73.79023746701847 Loss: 0.001504914602264762 Epsilon: 0.10297817807008705\n",
      "758/1000 Total steps: 56090 Episode reward: 157.0 Average reward: 73.89986824769433 Loss: 0.0006491077365353703 Epsilon: 0.102931785806587\n",
      "759/1000 Total steps: 56250 Episode reward: 160.0 Average reward: 74.01315789473684 Loss: 0.0002974987437482923 Epsilon: 0.10288525050881259\n",
      "760/1000 Total steps: 56413 Episode reward: 163.0 Average reward: 74.13009198423127 Loss: 0.0004994047922082245 Epsilon: 0.10283860214253325\n",
      "761/1000 Total steps: 56486 Episode reward: 73.0 Average reward: 74.12860892388451 Loss: 0.0012950559612363577 Epsilon: 0.10281795579773816\n",
      "762/1000 Total steps: 56747 Episode reward: 261.0 Average reward: 74.37352555701179 Loss: 0.00028056735754944384 Epsilon: 0.10274535866510894\n",
      "763/1000 Total steps: 56779 Episode reward: 32.0 Average reward: 74.31806282722513 Loss: 0.00444030249491334 Epsilon: 0.10273658755863563\n",
      "764/1000 Total steps: 56980 Episode reward: 201.0 Average reward: 74.48366013071896 Loss: 0.0003762271662708372 Epsilon: 0.10268213126782452\n",
      "765/1000 Total steps: 57151 Episode reward: 171.0 Average reward: 74.60966057441253 Loss: 0.001573949004523456 Epsilon: 0.10263665673846593\n",
      "766/1000 Total steps: 57242 Episode reward: 91.0 Average reward: 74.63102998696219 Loss: 0.000185193755896762 Epsilon: 0.10261277200251881\n",
      "767/1000 Total steps: 57378 Episode reward: 136.0 Average reward: 74.7109375 Loss: 0.001191294053569436 Epsilon: 0.10257747884076805\n",
      "768/1000 Total steps: 57432 Episode reward: 54.0 Average reward: 74.68400520156047 Loss: 0.00014134477532934397 Epsilon: 0.10256359796711727\n",
      "769/1000 Total steps: 57447 Episode reward: 15.0 Average reward: 74.6064935064935 Loss: 0.0003519425808917731 Epsilon: 0.10255975545277282\n",
      "770/1000 Total steps: 57531 Episode reward: 84.0 Average reward: 74.61867704280155 Loss: 0.0005043724086135626 Epsilon: 0.10253834356280914\n",
      "771/1000 Total steps: 57544 Episode reward: 13.0 Average reward: 74.53886010362694 Loss: 0.0003285524435341358 Epsilon: 0.10253504586014864\n",
      "772/1000 Total steps: 57561 Episode reward: 17.0 Average reward: 74.46442432082794 Loss: 0.0005452842451632023 Epsilon: 0.10253073994325276\n",
      "773/1000 Total steps: 57576 Episode reward: 15.0 Average reward: 74.3875968992248 Loss: 0.0017343792133033276 Epsilon: 0.10252694667899731\n",
      "774/1000 Total steps: 57679 Episode reward: 103.0 Average reward: 74.42451612903226 Loss: 0.0012699781218543649 Epsilon: 0.10250105271106236\n",
      "775/1000 Total steps: 57846 Episode reward: 167.0 Average reward: 74.54381443298969 Loss: 0.0031815289985388517 Epsilon: 0.10245963195673462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/1000 Total steps: 58046 Episode reward: 200.0 Average reward: 74.7052767052767 Loss: 0.00031938086613081396 Epsilon: 0.10241092798081418\n",
      "777/1000 Total steps: 58118 Episode reward: 72.0 Average reward: 74.70179948586119 Loss: 0.00012918793072458357 Epsilon: 0.10239363164089614\n",
      "778/1000 Total steps: 58248 Episode reward: 130.0 Average reward: 74.77278562259306 Loss: 0.004722017329186201 Epsilon: 0.10236271581781116\n",
      "779/1000 Total steps: 58286 Episode reward: 38.0 Average reward: 74.72564102564102 Loss: 0.0004476332396734506 Epsilon: 0.10235375453492437\n",
      "780/1000 Total steps: 58428 Episode reward: 142.0 Average reward: 74.81177976952625 Loss: 0.00029100250685587525 Epsilon: 0.1023205674067907\n",
      "781/1000 Total steps: 58524 Episode reward: 96.0 Average reward: 74.8388746803069 Loss: 9.695022163214162e-05 Epsilon: 0.10229839655006968\n",
      "782/1000 Total steps: 58631 Episode reward: 107.0 Average reward: 74.87994891443168 Loss: 0.00014608909259550273 Epsilon: 0.10227393481067464\n",
      "783/1000 Total steps: 58643 Episode reward: 12.0 Average reward: 74.79974489795919 Loss: 0.00015020609134808183 Epsilon: 0.10227120772548019\n",
      "784/1000 Total steps: 58654 Episode reward: 11.0 Average reward: 74.71847133757962 Loss: 0.000749146391171962 Epsilon: 0.10226871077055914\n",
      "785/1000 Total steps: 58764 Episode reward: 110.0 Average reward: 74.76335877862596 Loss: 0.00011040082608815283 Epsilon: 0.10224389170718991\n",
      "786/1000 Total steps: 58822 Episode reward: 58.0 Average reward: 74.7420584498094 Loss: 0.00022051300038583577 Epsilon: 0.10223091480468403\n",
      "787/1000 Total steps: 59030 Episode reward: 208.0 Average reward: 74.91116751269035 Loss: 0.0006413591327145696 Epsilon: 0.1021849910395964\n",
      "788/1000 Total steps: 59223 Episode reward: 193.0 Average reward: 75.06083650190114 Loss: 0.00043268996523693204 Epsilon: 0.10214322505076744\n",
      "789/1000 Total steps: 59302 Episode reward: 79.0 Average reward: 75.06582278481012 Loss: 0.0003124610229860991 Epsilon: 0.10212636027643578\n",
      "790/1000 Total steps: 59404 Episode reward: 102.0 Average reward: 75.09987357774969 Loss: 0.0012338015949353576 Epsilon: 0.10210478163974968\n",
      "791/1000 Total steps: 59485 Episode reward: 81.0 Average reward: 75.10732323232324 Loss: 0.0017879367806017399 Epsilon: 0.10208780176977843\n",
      "792/1000 Total steps: 59607 Episode reward: 122.0 Average reward: 75.16645649432535 Loss: 0.0005538016557693481 Epsilon: 0.10206248533246219\n",
      "793/1000 Total steps: 59729 Episode reward: 122.0 Average reward: 75.22544080604534 Loss: 0.0008891609613783658 Epsilon: 0.10203747587927044\n",
      "794/1000 Total steps: 59740 Episode reward: 11.0 Average reward: 75.14465408805032 Loss: 0.0012961121974512935 Epsilon: 0.1020352358880243\n",
      "795/1000 Total steps: 59826 Episode reward: 86.0 Average reward: 75.15829145728644 Loss: 0.000412407738622278 Epsilon: 0.10201780790711949\n",
      "796/1000 Total steps: 59939 Episode reward: 113.0 Average reward: 75.20577164366374 Loss: 0.00046627086703665555 Epsilon: 0.10199513502083445\n",
      "797/1000 Total steps: 60144 Episode reward: 205.0 Average reward: 75.36842105263158 Loss: 0.0002464406716171652 Epsilon: 0.10195465113055238\n",
      "798/1000 Total steps: 60252 Episode reward: 108.0 Average reward: 75.40926157697122 Loss: 0.0009449977660551667 Epsilon: 0.10193365448431908\n",
      "799/1000 Total steps: 60384 Episode reward: 132.0 Average reward: 75.48 Loss: 0.0002721298660617322 Epsilon: 0.10190829796632042\n",
      "800/1000 Total steps: 60545 Episode reward: 161.0 Average reward: 75.58676654182273 Loss: 0.0016898345202207565 Epsilon: 0.10187782037203526\n",
      "801/1000 Total steps: 60585 Episode reward: 40.0 Average reward: 75.5423940149626 Loss: 0.00030207130475901067 Epsilon: 0.10187032409310003\n",
      "802/1000 Total steps: 60752 Episode reward: 167.0 Average reward: 75.65628891656289 Loss: 0.0010518399067223072 Epsilon: 0.10183934904230206\n",
      "803/1000 Total steps: 60886 Episode reward: 134.0 Average reward: 75.72885572139303 Loss: 0.0001821701880544424 Epsilon: 0.10181486616674577\n",
      "804/1000 Total steps: 61004 Episode reward: 118.0 Average reward: 75.78136645962734 Loss: 0.0008050938486121595 Epsilon: 0.10179357660144281\n",
      "805/1000 Total steps: 61073 Episode reward: 69.0 Average reward: 75.77295285359801 Loss: 0.003302437486127019 Epsilon: 0.101781243520952\n",
      "806/1000 Total steps: 61357 Episode reward: 284.0 Average reward: 76.03097893432466 Loss: 9.01075400179252e-05 Epsilon: 0.1017313677925689\n",
      "807/1000 Total steps: 61413 Episode reward: 56.0 Average reward: 76.00618811881188 Loss: 0.0007254309020936489 Epsilon: 0.1017216992301724\n",
      "808/1000 Total steps: 61433 Episode reward: 20.0 Average reward: 75.93695920889988 Loss: 2.4735880288062617e-05 Epsilon: 0.10171825927281605\n",
      "809/1000 Total steps: 61614 Episode reward: 181.0 Average reward: 76.06666666666666 Loss: 0.0005967277684248984 Epsilon: 0.10168743854895587\n",
      "810/1000 Total steps: 61796 Episode reward: 182.0 Average reward: 76.19728729963009 Loss: 0.0012243347009643912 Epsilon: 0.10165700495315075\n",
      "811/1000 Total steps: 61809 Episode reward: 13.0 Average reward: 76.11945812807882 Loss: 0.00040505087235942483 Epsilon: 0.1016548522462743\n",
      "812/1000 Total steps: 61835 Episode reward: 26.0 Average reward: 76.05781057810579 Loss: 0.00031843490432947874 Epsilon: 0.10165055521899011\n",
      "813/1000 Total steps: 62148 Episode reward: 313.0 Average reward: 76.34889434889435 Loss: 0.0007382739568129182 Epsilon: 0.1015996929869346\n",
      "814/1000 Total steps: 62393 Episode reward: 245.0 Average reward: 76.5558282208589 Loss: 0.0035980844404548407 Epsilon: 0.10156097671962977\n",
      "815/1000 Total steps: 62514 Episode reward: 121.0 Average reward: 76.61029411764706 Loss: 3.1018920708447695e-05 Epsilon: 0.1015422027131196\n",
      "816/1000 Total steps: 62724 Episode reward: 210.0 Average reward: 76.7735618115055 Loss: 0.00024963109171949327 Epsilon: 0.10151015414389719\n",
      "817/1000 Total steps: 62841 Episode reward: 117.0 Average reward: 76.82273838630807 Loss: 0.00014608957280870527 Epsilon: 0.10149258830097657\n",
      "818/1000 Total steps: 62865 Episode reward: 24.0 Average reward: 76.75824175824175 Loss: 0.0006322445115074515 Epsilon: 0.10148901038427167\n",
      "819/1000 Total steps: 62951 Episode reward: 86.0 Average reward: 76.76951219512195 Loss: 0.00021830947662238032 Epsilon: 0.10147625980106074\n",
      "820/1000 Total steps: 62984 Episode reward: 33.0 Average reward: 76.71619975639464 Loss: 0.0004957044147886336 Epsilon: 0.10147139617311708\n",
      "821/1000 Total steps: 63123 Episode reward: 139.0 Average reward: 76.7919708029197 Loss: 0.0009863695595413446 Epsilon: 0.10145108525421877\n",
      "822/1000 Total steps: 63345 Episode reward: 222.0 Average reward: 76.96840826245443 Loss: 0.0008599559077993035 Epsilon: 0.10141922610655858\n",
      "823/1000 Total steps: 63475 Episode reward: 130.0 Average reward: 77.03276699029126 Loss: 3.685545379994437e-05 Epsilon: 0.10140089557379059\n",
      "824/1000 Total steps: 63619 Episode reward: 144.0 Average reward: 77.11393939393939 Loss: 0.00013097705959808081 Epsilon: 0.10138086722770842\n",
      "825/1000 Total steps: 63654 Episode reward: 35.0 Average reward: 77.06295399515739 Loss: 0.00030537895509041846 Epsilon: 0.1013760426403644\n",
      "826/1000 Total steps: 63784 Episode reward: 130.0 Average reward: 77.12696493349456 Loss: 0.001275291433557868 Epsilon: 0.10135826985941512\n",
      "827/1000 Total steps: 63885 Episode reward: 101.0 Average reward: 77.15579710144928 Loss: 0.00021248142002150416 Epsilon: 0.10134462037973915\n",
      "828/1000 Total steps: 64145 Episode reward: 260.0 Average reward: 77.37635705669481 Loss: 0.0003003620950039476 Epsilon: 0.10131011081818289\n",
      "829/1000 Total steps: 64159 Episode reward: 14.0 Average reward: 77.3 Loss: 0.0009624117519706488 Epsilon: 0.1013082779463471\n",
      "830/1000 Total steps: 64284 Episode reward: 125.0 Average reward: 77.35740072202167 Loss: 0.0004752296954393387 Epsilon: 0.10129202625668812\n",
      "831/1000 Total steps: 64451 Episode reward: 167.0 Average reward: 77.46514423076923 Loss: 0.00012800251715816557 Epsilon: 0.10127062858604866\n",
      "832/1000 Total steps: 64621 Episode reward: 170.0 Average reward: 77.57623049219688 Loss: 0.0004541863454505801 Epsilon: 0.10124921046989031\n",
      "833/1000 Total steps: 64654 Episode reward: 33.0 Average reward: 77.52278177458034 Loss: 0.000644206942524761 Epsilon: 0.10124509486981471\n",
      "834/1000 Total steps: 64714 Episode reward: 60.0 Average reward: 77.50179640718562 Loss: 0.0010980020742863417 Epsilon: 0.10123764666754721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835/1000 Total steps: 64808 Episode reward: 94.0 Average reward: 77.52153110047847 Loss: 0.0013710581697523594 Epsilon: 0.10122606729717566\n",
      "836/1000 Total steps: 64988 Episode reward: 180.0 Average reward: 77.64396654719235 Loss: 0.00041708568460308015 Epsilon: 0.1012041955223348\n",
      "837/1000 Total steps: 65175 Episode reward: 187.0 Average reward: 77.77446300715991 Loss: 0.0005537440301850438 Epsilon: 0.10118188630733273\n",
      "838/1000 Total steps: 65304 Episode reward: 129.0 Average reward: 77.83551847437425 Loss: 0.00039102911250665784 Epsilon: 0.10116673789132148\n",
      "839/1000 Total steps: 65437 Episode reward: 133.0 Average reward: 77.90119047619048 Loss: 0.00017784991359803826 Epsilon: 0.10115132301353169\n",
      "840/1000 Total steps: 65627 Episode reward: 190.0 Average reward: 78.03448275862068 Loss: 0.0016989992000162601 Epsilon: 0.10112965438015249\n",
      "841/1000 Total steps: 65687 Episode reward: 60.0 Average reward: 78.01306413301663 Loss: 5.950188642600551e-05 Epsilon: 0.10112289674704379\n",
      "842/1000 Total steps: 65725 Episode reward: 38.0 Average reward: 77.9655990510083 Loss: 0.0012578608002513647 Epsilon: 0.10111863783646001\n",
      "843/1000 Total steps: 65785 Episode reward: 60.0 Average reward: 77.94431279620854 Loss: 0.00019993878959212452 Epsilon: 0.10111194610471168\n",
      "844/1000 Total steps: 65948 Episode reward: 163.0 Average reward: 78.04497041420119 Loss: 0.0021771499887108803 Epsilon: 0.10109396830035222\n",
      "845/1000 Total steps: 66118 Episode reward: 170.0 Average reward: 78.15366430260048 Loss: 8.379853534279391e-05 Epsilon: 0.10107552802568207\n",
      "846/1000 Total steps: 66144 Episode reward: 26.0 Average reward: 78.09208972845336 Loss: 0.00039064587326720357 Epsilon: 0.10107273528495149\n",
      "847/1000 Total steps: 66166 Episode reward: 22.0 Average reward: 78.02594339622641 Loss: 0.00011191328667337075 Epsilon: 0.10107037786144128\n",
      "848/1000 Total steps: 66395 Episode reward: 229.0 Average reward: 78.2037691401649 Loss: 0.0006243797834031284 Epsilon: 0.10104614473669116\n",
      "849/1000 Total steps: 66497 Episode reward: 102.0 Average reward: 78.23176470588236 Loss: 0.00028545851819217205 Epsilon: 0.10103552829626745\n",
      "850/1000 Total steps: 66672 Episode reward: 175.0 Average reward: 78.3454759106933 Loss: 0.00031334193772636354 Epsilon: 0.10101756419542166\n",
      "851/1000 Total steps: 66761 Episode reward: 89.0 Average reward: 78.35798122065728 Loss: 0.0003465494082774967 Epsilon: 0.10100854805541937\n",
      "852/1000 Total steps: 66973 Episode reward: 212.0 Average reward: 78.51465416178195 Loss: 0.00041577592492103577 Epsilon: 0.10098739188442026\n",
      "853/1000 Total steps: 67131 Episode reward: 158.0 Average reward: 78.60772833723654 Loss: 0.00013391190441325307 Epsilon: 0.10097191369236035\n",
      "854/1000 Total steps: 67143 Episode reward: 12.0 Average reward: 78.52982456140352 Loss: 0.0001030898856697604 Epsilon: 0.10097074809542754\n",
      "855/1000 Total steps: 67360 Episode reward: 217.0 Average reward: 78.69158878504673 Loss: 0.0008085449226200581 Epsilon: 0.10094990977523746\n",
      "856/1000 Total steps: 67378 Episode reward: 18.0 Average reward: 78.62077012835472 Loss: 0.0002719115582294762 Epsilon: 0.10094820147557297\n",
      "857/1000 Total steps: 67445 Episode reward: 67.0 Average reward: 78.60722610722611 Loss: 0.0008709304966032505 Epsilon: 0.1009418697606176\n",
      "858/1000 Total steps: 67591 Episode reward: 146.0 Average reward: 78.68568102444704 Loss: 0.000822672969661653 Epsilon: 0.10092821835983184\n",
      "859/1000 Total steps: 67747 Episode reward: 156.0 Average reward: 78.77558139534884 Loss: 0.0009829401969909668 Epsilon: 0.10091385051399474\n",
      "860/1000 Total steps: 67880 Episode reward: 133.0 Average reward: 78.83855981416957 Loss: 0.0008985645836219192 Epsilon: 0.10090177677052917\n",
      "861/1000 Total steps: 68009 Episode reward: 129.0 Average reward: 78.89675174013921 Loss: 0.0002097086689900607 Epsilon: 0.10089021856092431\n",
      "862/1000 Total steps: 68317 Episode reward: 308.0 Average reward: 79.162224797219 Loss: 0.0006065715570002794 Epsilon: 0.10086321777580697\n",
      "863/1000 Total steps: 68666 Episode reward: 349.0 Average reward: 79.47453703703704 Loss: 0.00011097706737928092 Epsilon: 0.10083361111667277\n",
      "864/1000 Total steps: 68781 Episode reward: 115.0 Average reward: 79.51560693641619 Loss: 0.0002641894097905606 Epsilon: 0.10082407950066917\n",
      "865/1000 Total steps: 68930 Episode reward: 149.0 Average reward: 79.5958429561201 Loss: 0.0007184499991126359 Epsilon: 0.10081189174040606\n",
      "866/1000 Total steps: 69052 Episode reward: 122.0 Average reward: 79.64475201845444 Loss: 0.0002676564618013799 Epsilon: 0.10080204683719202\n",
      "867/1000 Total steps: 69069 Episode reward: 17.0 Average reward: 79.5725806451613 Loss: 0.0016243711579591036 Epsilon: 0.10080068451587\n",
      "868/1000 Total steps: 69128 Episode reward: 59.0 Average reward: 79.54890678941312 Loss: 0.0002538615954108536 Epsilon: 0.10079597438577345\n",
      "869/1000 Total steps: 69194 Episode reward: 66.0 Average reward: 79.53333333333333 Loss: 7.308542262762785e-05 Epsilon: 0.10079073825307241\n",
      "870/1000 Total steps: 69317 Episode reward: 123.0 Average reward: 79.58323765786453 Loss: 0.0012683565728366375 Epsilon: 0.10078107174346393\n",
      "871/1000 Total steps: 69473 Episode reward: 156.0 Average reward: 79.67087155963303 Loss: 0.00015690534200984985 Epsilon: 0.10076898157278485\n",
      "872/1000 Total steps: 69642 Episode reward: 169.0 Average reward: 79.77319587628865 Loss: 0.0009421951253898442 Epsilon: 0.10075609498260196\n",
      "873/1000 Total steps: 69831 Episode reward: 189.0 Average reward: 79.89816933638444 Loss: 0.0010382136097177863 Epsilon: 0.1007419389830131\n",
      "874/1000 Total steps: 69983 Episode reward: 152.0 Average reward: 79.98057142857142 Loss: 0.00016987220442388207 Epsilon: 0.10073074678664992\n",
      "875/1000 Total steps: 70031 Episode reward: 48.0 Average reward: 79.94406392694064 Loss: 0.00011678138980641961 Epsilon: 0.10072724760682401\n",
      "876/1000 Total steps: 70292 Episode reward: 261.0 Average reward: 80.15051311288484 Loss: 0.0007589320885017514 Epsilon: 0.1007085120074192\n",
      "877/1000 Total steps: 70452 Episode reward: 160.0 Average reward: 80.24145785876993 Loss: 3.1174306059256196e-05 Epsilon: 0.10069726602308844\n",
      "878/1000 Total steps: 70549 Episode reward: 97.0 Average reward: 80.26052332195677 Loss: 0.0009758381056599319 Epsilon: 0.1006905352397386\n",
      "879/1000 Total steps: 70674 Episode reward: 125.0 Average reward: 80.31136363636364 Loss: 0.00015823164721950889 Epsilon: 0.10068195727322457\n",
      "880/1000 Total steps: 70745 Episode reward: 71.0 Average reward: 80.30079455164585 Loss: 0.0003819696430582553 Epsilon: 0.10067713252470985\n",
      "881/1000 Total steps: 70899 Episode reward: 154.0 Average reward: 80.3843537414966 Loss: 0.0003117282467428595 Epsilon: 0.10066678456760832\n",
      "882/1000 Total steps: 70994 Episode reward: 95.0 Average reward: 80.400906002265 Loss: 0.00012652282021008432 Epsilon: 0.10066048010781477\n",
      "883/1000 Total steps: 71082 Episode reward: 88.0 Average reward: 80.40950226244344 Loss: 0.0019471602281555533 Epsilon: 0.10065469338180408\n",
      "884/1000 Total steps: 71152 Episode reward: 70.0 Average reward: 80.39774011299436 Loss: 0.00035679666325449944 Epsilon: 0.10065012653075807\n",
      "885/1000 Total steps: 71189 Episode reward: 37.0 Average reward: 80.34875846501129 Loss: 0.0003535448922775686 Epsilon: 0.10064772550722696\n",
      "886/1000 Total steps: 71441 Episode reward: 252.0 Average reward: 80.54227733934611 Loss: 0.0020210433285683393 Epsilon: 0.10063160677348436\n",
      "887/1000 Total steps: 71485 Episode reward: 44.0 Average reward: 80.50112612612612 Loss: 0.0005802963860332966 Epsilon: 0.10062883380867732\n",
      "888/1000 Total steps: 71596 Episode reward: 111.0 Average reward: 80.53543307086615 Loss: 0.00017871307500172406 Epsilon: 0.10062189234976922\n",
      "889/1000 Total steps: 71610 Episode reward: 14.0 Average reward: 80.46067415730337 Loss: 0.0003791509079746902 Epsilon: 0.10062102230964974\n",
      "890/1000 Total steps: 71696 Episode reward: 86.0 Average reward: 80.4668911335578 Loss: 0.0005841668462380767 Epsilon: 0.1006157044174989\n",
      "891/1000 Total steps: 71708 Episode reward: 12.0 Average reward: 80.39013452914799 Loss: 0.00011899931268999353 Epsilon: 0.10061496601532781\n",
      "892/1000 Total steps: 71811 Episode reward: 103.0 Average reward: 80.41545352743562 Loss: 0.00046757113886997104 Epsilon: 0.1006086643745317\n",
      "893/1000 Total steps: 71843 Episode reward: 32.0 Average reward: 80.36129753914989 Loss: 0.0004875294107478112 Epsilon: 0.10060671976157333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894/1000 Total steps: 71944 Episode reward: 101.0 Average reward: 80.38435754189945 Loss: 8.352266740985215e-05 Epsilon: 0.10060062273380141\n",
      "895/1000 Total steps: 71957 Episode reward: 13.0 Average reward: 80.30915178571429 Loss: 0.0010943150846287608 Epsilon: 0.10059984243155383\n",
      "896/1000 Total steps: 71981 Episode reward: 24.0 Average reward: 80.2463768115942 Loss: 0.001581057091243565 Epsilon: 0.10059840453588309\n",
      "897/1000 Total steps: 72081 Episode reward: 100.0 Average reward: 80.2683741648107 Loss: 0.00045182203757576644 Epsilon: 0.1005924503112658\n",
      "898/1000 Total steps: 72171 Episode reward: 90.0 Average reward: 80.27919911012236 Loss: 0.0011286602821201086 Epsilon: 0.10058714218088097\n",
      "899/1000 Total steps: 72278 Episode reward: 107.0 Average reward: 80.30888888888889 Loss: 0.0004147194267716259 Epsilon: 0.10058089325094062\n",
      "900/1000 Total steps: 72505 Episode reward: 227.0 Average reward: 80.47169811320755 Loss: 0.0002530404017306864 Epsilon: 0.10056785551232411\n",
      "901/1000 Total steps: 72570 Episode reward: 65.0 Average reward: 80.45454545454545 Loss: 0.00011294300202280283 Epsilon: 0.10056417642149267\n",
      "902/1000 Total steps: 72757 Episode reward: 187.0 Average reward: 80.57253599114064 Loss: 0.0005161723238416016 Epsilon: 0.10055372435382365\n",
      "903/1000 Total steps: 72877 Episode reward: 120.0 Average reward: 80.61615044247787 Loss: 0.00047117937356233597 Epsilon: 0.10054711937073589\n",
      "904/1000 Total steps: 72912 Episode reward: 35.0 Average reward: 80.56574585635359 Loss: 0.00037950740079395473 Epsilon: 0.10054520780013826\n",
      "905/1000 Total steps: 73020 Episode reward: 108.0 Average reward: 80.59602649006622 Loss: 0.00012147962115705013 Epsilon: 0.1005393512382566\n",
      "906/1000 Total steps: 73067 Episode reward: 47.0 Average reward: 80.55898566703418 Loss: 0.0018748966977000237 Epsilon: 0.10053682223524933\n",
      "907/1000 Total steps: 73204 Episode reward: 137.0 Average reward: 80.62114537444934 Loss: 7.313401147257537e-05 Epsilon: 0.10052951791943497\n",
      "908/1000 Total steps: 73292 Episode reward: 88.0 Average reward: 80.62926292629263 Loss: 0.0003828518674708903 Epsilon: 0.10052487860466792\n",
      "909/1000 Total steps: 73435 Episode reward: 143.0 Average reward: 80.6978021978022 Loss: 0.00019703702128026634 Epsilon: 0.10051742625193706\n",
      "910/1000 Total steps: 73638 Episode reward: 203.0 Average reward: 80.83205268935237 Loss: 0.0006314925267361104 Epsilon: 0.10050702839434593\n",
      "911/1000 Total steps: 73817 Episode reward: 179.0 Average reward: 80.93969298245614 Loss: 0.00047197166713885963 Epsilon: 0.1004980333320689\n",
      "912/1000 Total steps: 73883 Episode reward: 66.0 Average reward: 80.92332968236583 Loss: 0.0002290585543960333 Epsilon: 0.10049475713541878\n",
      "913/1000 Total steps: 73982 Episode reward: 99.0 Average reward: 80.94310722100657 Loss: 0.0027876319363713264 Epsilon: 0.1004898832055388\n",
      "914/1000 Total steps: 73992 Episode reward: 10.0 Average reward: 80.8655737704918 Loss: 0.0003809585759881884 Epsilon: 0.10048939356719323\n",
      "915/1000 Total steps: 74015 Episode reward: 23.0 Average reward: 80.8024017467249 Loss: 0.0005376419285312295 Epsilon: 0.10048826925544284\n",
      "916/1000 Total steps: 74142 Episode reward: 127.0 Average reward: 80.85278080697928 Loss: 0.0002896985097322613 Epsilon: 0.10048210744620699\n",
      "917/1000 Total steps: 74335 Episode reward: 193.0 Average reward: 80.97494553376906 Loss: 0.0014085785951465368 Epsilon: 0.10047289198772329\n",
      "918/1000 Total steps: 74611 Episode reward: 276.0 Average reward: 81.18715995647443 Loss: 0.0015561537584289908 Epsilon: 0.10046001863827438\n",
      "919/1000 Total steps: 74708 Episode reward: 97.0 Average reward: 81.20434782608696 Loss: 0.000952099566347897 Epsilon: 0.10045557802925488\n",
      "920/1000 Total steps: 74982 Episode reward: 274.0 Average reward: 81.41368078175896 Loss: 0.00021147116785869002 Epsilon: 0.1004432646548389\n",
      "921/1000 Total steps: 75127 Episode reward: 145.0 Average reward: 81.4826464208243 Loss: 0.0007869374239817262 Epsilon: 0.10043688369113003\n",
      "922/1000 Total steps: 75283 Episode reward: 156.0 Average reward: 81.56338028169014 Loss: 0.0002853141922969371 Epsilon: 0.10043012119019863\n",
      "923/1000 Total steps: 75469 Episode reward: 186.0 Average reward: 81.67640692640693 Loss: 0.0007228220347315073 Epsilon: 0.10042219487926683\n",
      "924/1000 Total steps: 75834 Episode reward: 365.0 Average reward: 81.9827027027027 Loss: 0.0002591634402051568 Epsilon: 0.1004070626100467\n",
      "925/1000 Total steps: 75906 Episode reward: 72.0 Average reward: 81.9719222462203 Loss: 0.0003168639668729156 Epsilon: 0.10040414228504017\n",
      "926/1000 Total steps: 75979 Episode reward: 73.0 Average reward: 81.96224379719526 Loss: 9.687602141639218e-05 Epsilon: 0.10040120278857528\n",
      "927/1000 Total steps: 76220 Episode reward: 241.0 Average reward: 82.13362068965517 Loss: 0.0002178190043196082 Epsilon: 0.10039164938230453\n",
      "928/1000 Total steps: 76322 Episode reward: 102.0 Average reward: 82.15500538213132 Loss: 0.000660566845908761 Epsilon: 0.10038767486311193\n",
      "929/1000 Total steps: 76358 Episode reward: 36.0 Average reward: 82.10537634408603 Loss: 5.094370135338977e-05 Epsilon: 0.10038628174272599\n",
      "930/1000 Total steps: 76471 Episode reward: 113.0 Average reward: 82.13856068743287 Loss: 0.0007408671663142741 Epsilon: 0.10038194132855875\n",
      "931/1000 Total steps: 76671 Episode reward: 200.0 Average reward: 82.26502145922747 Loss: 0.0007847646484151483 Epsilon: 0.10037437838353432\n",
      "932/1000 Total steps: 76867 Episode reward: 196.0 Average reward: 82.38692390139336 Loss: 0.0002820052905008197 Epsilon: 0.10036711201029413\n",
      "933/1000 Total steps: 76881 Episode reward: 14.0 Average reward: 82.313704496788 Loss: 0.0005693087005056441 Epsilon: 0.10036659841308165\n",
      "934/1000 Total steps: 77129 Episode reward: 248.0 Average reward: 82.49090909090908 Loss: 0.0008985751192085445 Epsilon: 0.10035761858257702\n",
      "935/1000 Total steps: 77155 Episode reward: 26.0 Average reward: 82.43055555555556 Loss: 0.0003769767645280808 Epsilon: 0.10035668998196623\n",
      "936/1000 Total steps: 77280 Episode reward: 125.0 Average reward: 82.47598719316969 Loss: 0.0005884155980311334 Epsilon: 0.10035225910784841\n",
      "937/1000 Total steps: 77301 Episode reward: 21.0 Average reward: 82.41044776119404 Loss: 0.0004620318941306323 Epsilon: 0.10035152013990983\n",
      "938/1000 Total steps: 77621 Episode reward: 320.0 Average reward: 82.66347177848775 Loss: 0.0002438478550175205 Epsilon: 0.10034044956923607\n",
      "939/1000 Total steps: 77741 Episode reward: 120.0 Average reward: 82.7031914893617 Loss: 0.0008948854519985616 Epsilon: 0.1003363885890182\n",
      "940/1000 Total steps: 77885 Episode reward: 144.0 Average reward: 82.7683315621679 Loss: 0.0005201049498282373 Epsilon: 0.10033157930329768\n",
      "941/1000 Total steps: 78032 Episode reward: 147.0 Average reward: 82.83651804670913 Loss: 0.00041011357097886503 Epsilon: 0.1003267407381234\n",
      "942/1000 Total steps: 78150 Episode reward: 118.0 Average reward: 82.87380699893956 Loss: 0.0022014945279806852 Epsilon: 0.1003229078558928\n",
      "943/1000 Total steps: 78212 Episode reward: 62.0 Average reward: 82.85169491525424 Loss: 0.00010296310938429087 Epsilon: 0.10032091202066878\n",
      "944/1000 Total steps: 78251 Episode reward: 39.0 Average reward: 82.80529100529101 Loss: 0.0002885812136810273 Epsilon: 0.10031966290115449\n",
      "945/1000 Total steps: 78301 Episode reward: 50.0 Average reward: 82.77061310782241 Loss: 0.00011207324860151857 Epsilon: 0.10031806857578365\n",
      "946/1000 Total steps: 78345 Episode reward: 44.0 Average reward: 82.72967265047518 Loss: 0.0001430170377716422 Epsilon: 0.10031667214844325\n",
      "947/1000 Total steps: 78358 Episode reward: 13.0 Average reward: 82.65611814345992 Loss: 0.0003062368487007916 Epsilon: 0.10031626074212233\n",
      "948/1000 Total steps: 78380 Episode reward: 22.0 Average reward: 82.59220231822971 Loss: 0.00022613401233684272 Epsilon: 0.10031556573327971\n",
      "949/1000 Total steps: 78537 Episode reward: 157.0 Average reward: 82.67052631578947 Loss: 0.00015145540237426758 Epsilon: 0.1003106500404281\n",
      "950/1000 Total steps: 78677 Episode reward: 140.0 Average reward: 82.73080967402734 Loss: 0.00011063826968893409 Epsilon: 0.10030633124199131\n",
      "951/1000 Total steps: 78749 Episode reward: 72.0 Average reward: 82.71953781512605 Loss: 0.000806715281214565 Epsilon: 0.10030413357813277\n",
      "952/1000 Total steps: 78809 Episode reward: 60.0 Average reward: 82.69569779643231 Loss: 0.00017923585255630314 Epsilon: 0.10030231424013598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "953/1000 Total steps: 79085 Episode reward: 276.0 Average reward: 82.89832285115304 Loss: 0.0006195589085109532 Epsilon: 0.10029408446048715\n",
      "954/1000 Total steps: 79158 Episode reward: 73.0 Average reward: 82.88795811518325 Loss: 0.00031499340548180044 Epsilon: 0.10029194546077348\n",
      "955/1000 Total steps: 79358 Episode reward: 200.0 Average reward: 83.01046025104603 Loss: 0.0012238650815561414 Epsilon: 0.1002861645533281\n",
      "956/1000 Total steps: 79463 Episode reward: 105.0 Average reward: 83.03343782654127 Loss: 0.0002559675194788724 Epsilon: 0.10028317554527191\n",
      "957/1000 Total steps: 79562 Episode reward: 99.0 Average reward: 83.05010438413362 Loss: 0.00023515280918218195 Epsilon: 0.10028038593871028\n",
      "958/1000 Total steps: 79706 Episode reward: 144.0 Average reward: 83.11366006256517 Loss: 0.0007745113107375801 Epsilon: 0.10027637731256989\n",
      "959/1000 Total steps: 79790 Episode reward: 84.0 Average reward: 83.11458333333333 Loss: 0.00031520123593509197 Epsilon: 0.10027406546649147\n",
      "960/1000 Total steps: 79972 Episode reward: 182.0 Average reward: 83.21748178980229 Loss: 0.0002349636924918741 Epsilon: 0.10026912259160188\n",
      "961/1000 Total steps: 80000 Episode reward: 28.0 Average reward: 83.16008316008316 Loss: 0.000781507755164057 Epsilon: 0.10026837010232202\n",
      "962/1000 Total steps: 80297 Episode reward: 297.0 Average reward: 83.38213914849429 Loss: 0.0006058871513232589 Epsilon: 0.1002605167104274\n",
      "963/1000 Total steps: 80331 Episode reward: 34.0 Average reward: 83.33091286307054 Loss: 0.0009452656959183514 Epsilon: 0.10025963245769343\n",
      "964/1000 Total steps: 80496 Episode reward: 165.0 Average reward: 83.41554404145077 Loss: 0.00014567493053618819 Epsilon: 0.1002553836710254\n",
      "965/1000 Total steps: 80521 Episode reward: 25.0 Average reward: 83.35507246376811 Loss: 0.0002951760543510318 Epsilon: 0.10025474600925717\n",
      "966/1000 Total steps: 80567 Episode reward: 46.0 Average reward: 83.31644260599793 Loss: 0.0002577923296485096 Epsilon: 0.10025357686869946\n",
      "967/1000 Total steps: 80649 Episode reward: 82.0 Average reward: 83.3150826446281 Loss: 6.275483610806987e-05 Epsilon: 0.10025150604037578\n",
      "968/1000 Total steps: 80796 Episode reward: 147.0 Average reward: 83.38080495356037 Loss: 0.00033469998743385077 Epsilon: 0.10024783594288783\n",
      "969/1000 Total steps: 81003 Episode reward: 207.0 Average reward: 83.50824742268041 Loss: 0.0002859177766367793 Epsilon: 0.10024275847199628\n",
      "970/1000 Total steps: 81070 Episode reward: 67.0 Average reward: 83.49124613800205 Loss: 0.00044963666005060077 Epsilon: 0.10024113742679937\n",
      "971/1000 Total steps: 81102 Episode reward: 32.0 Average reward: 83.43827160493827 Loss: 0.0008025123388506472 Epsilon: 0.10024036702034135\n",
      "972/1000 Total steps: 81118 Episode reward: 16.0 Average reward: 83.36896197327852 Loss: 0.00034431202220730484 Epsilon: 0.10023998274061457\n",
      "973/1000 Total steps: 81269 Episode reward: 151.0 Average reward: 83.43839835728953 Loss: 4.014066144009121e-05 Epsilon: 0.10023638622327379\n",
      "974/1000 Total steps: 81460 Episode reward: 191.0 Average reward: 83.54871794871795 Loss: 0.0008854406187310815 Epsilon: 0.10023191409122602\n",
      "975/1000 Total steps: 81650 Episode reward: 190.0 Average reward: 83.6577868852459 Loss: 0.0011367658153176308 Epsilon: 0.10022754932012427\n",
      "976/1000 Total steps: 81753 Episode reward: 103.0 Average reward: 83.6775844421699 Loss: 0.0001978004875127226 Epsilon: 0.10022521759114562\n",
      "977/1000 Total steps: 81798 Episode reward: 45.0 Average reward: 83.63803680981596 Loss: 5.344911915017292e-05 Epsilon: 0.10022420638889692\n",
      "978/1000 Total steps: 81856 Episode reward: 58.0 Average reward: 83.61184882533198 Loss: 0.0002760807692538947 Epsilon: 0.10022290975571245\n",
      "979/1000 Total steps: 81891 Episode reward: 35.0 Average reward: 83.56224489795919 Loss: 9.149077231995761e-05 Epsilon: 0.10022213093529823\n",
      "980/1000 Total steps: 81909 Episode reward: 18.0 Average reward: 83.4954128440367 Loss: 0.0030978634022176266 Epsilon: 0.10022173145925099\n",
      "981/1000 Total steps: 81944 Episode reward: 35.0 Average reward: 83.44602851323829 Loss: 0.00014087039744481444 Epsilon: 0.10022095675566574\n",
      "982/1000 Total steps: 81967 Episode reward: 23.0 Average reward: 83.38453713123093 Loss: 0.0004259175912011415 Epsilon: 0.10022044913911052\n",
      "983/1000 Total steps: 82182 Episode reward: 215.0 Average reward: 83.51829268292683 Loss: 0.0005535780219361186 Epsilon: 0.10021576007073016\n",
      "984/1000 Total steps: 82335 Episode reward: 153.0 Average reward: 83.58883248730965 Loss: 0.0002008865849347785 Epsilon: 0.10021248406698305\n",
      "985/1000 Total steps: 82518 Episode reward: 183.0 Average reward: 83.6896551724138 Loss: 0.00013213357306085527 Epsilon: 0.10020863097190685\n",
      "986/1000 Total steps: 82535 Episode reward: 17.0 Average reward: 83.62208713272543 Loss: 0.00010842202755156904 Epsilon: 0.1002082766005556\n",
      "987/1000 Total steps: 82756 Episode reward: 221.0 Average reward: 83.76113360323886 Loss: 0.0002714272995945066 Epsilon: 0.1002037241772468\n",
      "988/1000 Total steps: 82768 Episode reward: 12.0 Average reward: 83.68857431749241 Loss: 0.0018128113588318229 Epsilon: 0.10020347985485686\n",
      "989/1000 Total steps: 82857 Episode reward: 89.0 Average reward: 83.69393939393939 Loss: 0.0008808382553979754 Epsilon: 0.10020167691911355\n",
      "990/1000 Total steps: 82905 Episode reward: 48.0 Average reward: 83.65792129162462 Loss: 0.00012859230628237128 Epsilon: 0.10020071118950706\n",
      "991/1000 Total steps: 82968 Episode reward: 63.0 Average reward: 83.63709677419355 Loss: 0.0009170602425001562 Epsilon: 0.10019945068377534\n",
      "992/1000 Total steps: 83035 Episode reward: 67.0 Average reward: 83.62034239677745 Loss: 0.0002514235966373235 Epsilon: 0.10019811883088348\n",
      "993/1000 Total steps: 83071 Episode reward: 36.0 Average reward: 83.57243460764587 Loss: 7.763950270600617e-05 Epsilon: 0.10019740688536313\n",
      "994/1000 Total steps: 83411 Episode reward: 340.0 Average reward: 83.83015075376885 Loss: 8.6327621829696e-05 Epsilon: 0.10019080787021124\n",
      "995/1000 Total steps: 83423 Episode reward: 12.0 Average reward: 83.75803212851406 Loss: 0.001530442968942225 Epsilon: 0.10019057903809372\n",
      "996/1000 Total steps: 83552 Episode reward: 129.0 Average reward: 83.80341023069208 Loss: 0.0003560546028893441 Epsilon: 0.10018813635766485\n",
      "997/1000 Total steps: 83743 Episode reward: 191.0 Average reward: 83.91082164328657 Loss: 0.0009696462075226009 Epsilon: 0.1001845770528001\n",
      "998/1000 Total steps: 83830 Episode reward: 87.0 Average reward: 83.91391391391392 Loss: 0.00019972443988081068 Epsilon: 0.10018297819754585\n",
      "999/1000 Total steps: 83894 Episode reward: 64.0 Average reward: 83.894 Loss: 0.00021228451805654913 Epsilon: 0.10018181087649337\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "state_space_dimensions = env.observation_space.shape[0]\n",
    "\n",
    "target_net = CartPoleDQN()\n",
    "policy_net = CartPoleDQN()\n",
    "\n",
    "#policy_net.load_state_dict(target_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters())\n",
    "\n",
    "memory = ReplayBuffer(100000)\n",
    "try:\n",
    "    train_dqn(env, policy_net, target_net, optimizer, memory, gamma=0.5, batch_size=128, episodes=1000, epsilon_steps=10000, epsilon_end=0.1)\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
