{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alessandropacielli/autonomous_systems_project/blob/master/Pytorch_DQN_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eL-UO54fZx3X"
   },
   "source": [
    "Install dependencies for virtual display to render the environment\n",
    "\n",
    "See [this medium post.](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymmGP3TuZ3B9"
   },
   "outputs": [],
   "source": [
    "# This installs a \n",
    "!apt-get install -y xvfb x11-utils\n",
    "\n",
    "!pip install pyvirtualdisplay==0.2.* \\\n",
    "             PyOpenGL==3.1.* \\\n",
    "             PyOpenGL-accelerate==3.1.*\n",
    "\n",
    "# This starts the display\n",
    "import pyvirtualdisplay\n",
    "\n",
    "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cedZKEAZDast"
   },
   "outputs": [],
   "source": [
    "from gym.wrappers.frame_stack import FrameStack\n",
    "from gym.wrappers.gray_scale_observation import GrayScaleObservation\n",
    "from collections import namedtuple\n",
    "import gym\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFeNLFFuDims"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "frames = 4\n",
    "env_name = 'Breakout-v0'\n",
    "env = FrameStack(GrayScaleObservation(gym.make(env_name)), frames)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ju6R5xsI3ta"
   },
   "source": [
    "# Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yoyJzxuwI7Bg"
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cX470chbIr7o"
   },
   "source": [
    "# DQN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GmuSz5GhEhMZ"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, h, w, outputs, frame_stack=4, rgb=False):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Construct a new DQN object.\n",
    "        \n",
    "        :param h: The height of the image.\n",
    "        :param w: The width of the image.\n",
    "        :param outputs: The number of outputs.\n",
    "        \"\"\"\n",
    "\n",
    "        if rgb:\n",
    "          color_channels = 3\n",
    "        else:\n",
    "          color_channels = 1\n",
    "        \n",
    "        self.input_channels = color_channels * frame_stack\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.input_channels, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # (Size - Kernel size + 2 * Padding) // Stride --> see https://cs231n.github.io/convolutional-networks/\n",
    "        def conv2d_size_out(size, kernel=5, stride=2):\n",
    "            return (size - kernel) // stride + 1\n",
    "        \n",
    "        # Compute convolution output dimensions\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        \n",
    "        # Conv output width * conv output height * conv output channels\n",
    "        self.linear_input_size = convw * convh * 32\n",
    "                 \n",
    "        # A fully connected layer for the output\n",
    "        self.head = nn.Linear(self.linear_input_size, outputs)\n",
    "        \n",
    "    # NN forward pass    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x))) # TODO should we use maxpooling? or any other pooling?\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dajfV_DTJdKV"
   },
   "outputs": [],
   "source": [
    "class DQNTraining():\n",
    "\n",
    "  def __init__(self, env, batch_size=128, gamma=0.999,\n",
    "                 eps_start=0.9, eps_end=0.05, eps_decay=200, \n",
    "                 update_every=10, memory_size=10000):\n",
    "    \"\"\"\n",
    "    The training support object has two DQNs: policy and target, see \n",
    "    https://greentec.github.io/reinforcement-learning-third-en/#soft-update-target-network\n",
    "\n",
    "    :param env: gym environment\n",
    "    :param batch_size: how many transitions are sampled from the replay memory for training.\n",
    "    :param gamma: discount.\n",
    "    :param eps_start: initial exploration rate (for epsilon-greedy policy).\n",
    "    :param eps_decay: controls the rate of decay (for epsilon-greedy policy).\n",
    "    :param eps_end: final exploration rate (for epsilon-greedy policy).\n",
    "    :param update_every: how often the target net is updated with weights from the policy network.\n",
    "    :param memory_size: replay buffer size\n",
    "    \"\"\"\n",
    "\n",
    "    self.env = env\n",
    "    self.batch_size = batch_size\n",
    "    self.gamma = gamma\n",
    "    self.eps_start = eps_start\n",
    "    self.eps_end = eps_end\n",
    "    self.eps_decay = eps_decay\n",
    "    self.update_every = update_every\n",
    "\n",
    "    # Might be useful later on\n",
    "    self.n_actions = env.action_space.n\n",
    "\n",
    "    # State dimensions\n",
    "    # TODO move outside!!!\n",
    "    self.state_h = env.observation_space.shape[1]\n",
    "    self.state_w = env.observation_space.shape[2]\n",
    "    self.channels = 4\n",
    "\n",
    "    # Policy & target nets\n",
    "    self.policy_network = DQN(self.state_h, self.state_w, self.n_actions, rgb=False).to(device)\n",
    "    self.target_network = DQN(self.state_h, self.state_w, self.n_actions, rgb=False).to(device)\n",
    "    self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "    self.target_network.eval()\n",
    "\n",
    "    # Optimizer\n",
    "    self.optimizer = optim.RMSprop(self.policy_network.parameters())\n",
    "\n",
    "    # Replay memory\n",
    "    self.memory = ReplayMemory(memory_size)\n",
    "\n",
    "    self.total_steps = 0\n",
    "    \n",
    "    # Performance\n",
    "\n",
    "  def select_action(self, state):\n",
    "    sample = random.random()\n",
    "    eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\n",
    "                    math.exp(-1. * self.total_steps / self.eps_decay)\n",
    "    self.total_steps += 1\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return self.policy_network(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "  def optimize_model(self):\n",
    "\n",
    "    if len(self.memory) < self.batch_size:\n",
    "        return\n",
    "    \n",
    "    # Optimization steps sample the replay buffer\n",
    "    transitions = self.memory.sample(self.batch_size)\n",
    "    \n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Convert arrays to tensors\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.tensor(batch.reward, device=device)\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = [s for s in batch.next_state if s is not None]\n",
    "    non_final_len = torch.sum(non_final_mask.long())\n",
    "    non_final_next_states = torch.stack(non_final_next_states)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    print(self.policy_network(state_batch).shape)\n",
    "    state_action_values = self.policy_network(state_batch).gather(1, action_batch)[non_final_mask]\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "    next_state_values = self.target_network(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * self.gamma) + reward_batch[non_final_mask]\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Optimize the model\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in self.policy_network.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    self.optimizer.step()\n",
    "\n",
    "  def training_loop(self, num_episodes, render=False):\n",
    "    if render:\n",
    "      _, ax = plt.subplots(1, 1)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "      print('Episode: %d/%d' % (episode+1, num_episodes))\n",
    "      # Reset env\n",
    "      state = torch.tensor(env.reset(), device=device).view(self.channels, self.state_h, self.state_w).float()\n",
    "      done = False\n",
    "      if render:\n",
    "        img = ax.imshow(env.render(mode='rgb_array'))      \n",
    "      while not done:\n",
    "\n",
    "        if render:\n",
    "          img.set_data(env.render(mode='rgb_array')) \n",
    "          ax.axis('off')\n",
    "          display.display(plt.gcf())\n",
    "          display.clear_output(wait=True)\n",
    "        \n",
    "        # Let agent perform an action according to an eps-greedy policy\n",
    "        action = self.select_action(state.view(1, self.channels, self.state_h, self.state_w))\n",
    "        \n",
    "        # Take step\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if not done:\n",
    "          next_state = torch.from_numpy(np.array(next_state)).to(device).view(self.channels, self.state_h, self.state_w).float()\n",
    "        else:\n",
    "          next_state = None\n",
    "        \n",
    "        # Remember transition\n",
    "        self.memory.push(state, action, reward, next_state)\n",
    "\n",
    "        # Run optimization step\n",
    "        self.optimize_model()\n",
    "        \n",
    "      if episode % self.update_every == 0:\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k1nG8TutLRIo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/100\n",
      "torch.Size([128, 4])\n",
      "torch.Size([128, 4])\n",
      "torch.Size([128, 4])\n",
      "torch.Size([128, 4])\n",
      "torch.Size([128, 4])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fecfb65be763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-8d032b82fc46>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(self, num_episodes, render)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# Run optimization step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8d032b82fc46>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# for each batch state according to policy_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mstate_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_final_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Compute V(s_{t+1}) for all next states.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/uni/autonomous/.venv/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6fc1fc236854>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# NN forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO should we use maxpooling? or any other pooling?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/uni/autonomous/.venv/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/uni/autonomous/.venv/lib64/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/uni/autonomous/.venv/lib64/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    345\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 346\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = DQNTraining(env)\n",
    "agent.training_loop(100, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HpmUg02wUtks"
   },
   "outputs": [],
   "source": [
    "# TODO checkpointing\n",
    "torch.save({\n",
    "            'model_state_dict': agent.policy_network.state_dict(),\n",
    "            'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "            }, 'policy.pkl')\n",
    "\n",
    "torch.save({\n",
    "            'model_state_dict': agent.target_network.state_dict(),\n",
    "            }, 'target.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WNj86NaOWQUA"
   },
   "outputs": [],
   "source": [
    "old_agent = DQNTraining(env, eps_start=0.05)\n",
    "policy_checkpoint = torch.load('policy.pkl')\n",
    "target_checkpoint = torch.load('target.pkl')\n",
    "old_agent.policy_network.load_state_dict(policy_checkpoint['model_state_dict'])\n",
    "old_agent.target_network.load_state_dict(target_checkpoint['model_state_dict'])\n",
    "old_agent.optimizer.load_state_dict(policy_checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "ADm8cc0ZXRiA",
    "outputId": "a6a439c7-c743-482a-e404-a0c07bb4f723"
   },
   "outputs": [],
   "source": [
    "old_agent.training_loop(1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f13650bfdd0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(torch.from_numpy(np.array(env.step(env.action_space.sample())[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMqnCT/lIHPuaY8BTKUQJaN",
   "include_colab_link": true,
   "name": "Pytorch DQN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
