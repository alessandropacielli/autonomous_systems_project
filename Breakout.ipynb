{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing, FrameStack\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 4 -- State space dimensions: (210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutNoFrameskip-v0')\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "state_shape = env.observation_space.shape\n",
    "\n",
    "print('Actions: {} -- State space dimensions: {}'.format(num_actions, state_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We can preprocess the frames using gym wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = 4\n",
    "\n",
    "env = gym.make('BreakoutNoFrameskip-v0')\n",
    "\n",
    "# Grayscale, frame resize and frame rescale\n",
    "env = AtariPreprocessing(env, scale_obs=True)\n",
    "\n",
    "# Frame stack \n",
    "env = FrameStack(env, frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakoutDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_actions, frame_h, frame_w, frame_stack=4):\n",
    "        super(BreakoutDQN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(frame_stack, 32, kernel_size=6, stride=3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=4, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel=5, stride=2) -> int:\n",
    "            return (size - kernel) // stride + 1\n",
    "\n",
    "        convh = conv2d_size_out(frame_h, kernel=6, stride=3)\n",
    "        convh = conv2d_size_out(convh, kernel=4, stride=2)\n",
    "        convh = conv2d_size_out(convh, kernel=4, stride=1)\n",
    "\n",
    "        \n",
    "        convw = conv2d_size_out(frame_w, kernel=6, stride=3)\n",
    "        convw = conv2d_size_out(convw, kernel=4, stride=2)\n",
    "        convw = conv2d_size_out(convw, kernel=4, stride=1)\n",
    "        \n",
    "        linear_input = convh * convw * 64\n",
    "        \n",
    "        self.fc1 = nn.Linear(linear_input, 256)\n",
    "        self.head = nn.Linear(256, num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = F.leaky_relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        x = self.head(x) * 15\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"reward\", \"next_state\"))\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state_sample, action_sample, reward_sample, next_state_sample = Transition(*zip(*random.sample(self.memory, batch_size)))\n",
    "        return torch.stack(state_sample), \\\n",
    "               torch.stack(action_sample), \\\n",
    "               torch.stack(reward_sample), \\\n",
    "               next_state_sample\n",
    "               \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, env, model, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return model(state).argmax().item()\n",
    "    else:\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_epsilon(epsilon_start, epsilon_end, epsilon_steps, total_steps):\n",
    "    return epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * total_steps / epsilon_steps)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(policy_net, target_net, optimizer, memory, batch_size, gamma, frame_stack=4, frame_h=84, frame_w=84):\n",
    "    state_batch, action_batch, reward_batch, next_state_batch = memory.sample(batch_size)\n",
    "    \n",
    "    state_batch = state_batch.view((batch_size, frame_stack, frame_h, frame_w))\n",
    "    \n",
    "    non_final_next_states = torch.cat([s for s in next_state_batch if s is not None])\n",
    "    non_final_mask = torch.tensor(list(map(lambda s: s is not None, next_state_batch)), dtype=torch.bool)\n",
    "        \n",
    "    state_action_values = policy_net(state_batch)\n",
    "    state_action_values = state_action_values.gather(1, action_batch.reshape((batch_size, 1)))\n",
    "    \n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(dim=1)[0].float().detach()\n",
    "        \n",
    "    expected_state_action_values = reward_batch + gamma * next_state_values\n",
    "    \n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_dqn(env, policy_net, target_net, optimizer, memory, frame_h=84, frame_w=84, frame_stack=4, target_update=10, batch_size=32, episodes=100, gamma=0.99, epsilon_start=0.9, epsilon_end=0.05, epsilon_steps=1000000):\n",
    "    \n",
    "    total_rewards = []\n",
    "    total_steps = 0\n",
    "    \n",
    "    epsilon = epsilon_start\n",
    "        \n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        done = False\n",
    "        state = torch.tensor(env.reset(), device=device).float().view(1, frame_stack, frame_h, frame_w)\n",
    "        \n",
    "        total_rewards.append(0)\n",
    "        loss = 0\n",
    "        \n",
    "        while not done:\n",
    "                        \n",
    "            # env.render()\n",
    "            action = select_action(state, env, policy_net, epsilon)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            total_rewards[episode] += reward\n",
    "            \n",
    "            action_tensor = torch.tensor(action, device=device, dtype=torch.int64)\n",
    "            reward_tensor = torch.tensor(reward, device=device, dtype=torch.float)\n",
    "            next_state = torch.tensor(next_state, device=device, dtype=torch.float).view(1, frame_stack, frame_h, frame_w)\n",
    "            if done:\n",
    "                next_state = None\n",
    "                                            \n",
    "            memory.push(state, action_tensor, reward_tensor, next_state)\n",
    "            \n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if len(memory) >= batch_size:                 \n",
    "                loss = optimize_model(policy_net, target_net, optimizer, memory, batch_size, gamma)\n",
    "        \n",
    "            if total_steps % target_update == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "                \n",
    "            total_steps += 1\n",
    "            epsilon = update_epsilon(epsilon_start, epsilon_end, epsilon_steps, total_steps)\n",
    "            \n",
    "        \n",
    "        print('{}/{} Total steps: {} Episode reward: {} Average reward: {} Loss: {} Epsilon: {}'.format(episode, episodes, total_steps, total_rewards[episode], np.mean(total_rewards), loss, epsilon))   \n",
    "                   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1000 Total steps: 261 Episode reward: 3.0 Average reward: 3.0 Loss: 0.017512204125523567 Epsilon: 0.8977843926253687\n",
      "1/1000 Total steps: 493 Episode reward: 2.0 Average reward: 2.5 Loss: 0.001796769560314715 Epsilon: 0.8958198126284538\n",
      "2/1000 Total steps: 627 Episode reward: 0.0 Average reward: 1.6666666666666667 Loss: 0.0005694336141459644 Epsilon: 0.8946871731174847\n",
      "3/1000 Total steps: 876 Episode reward: 3.0 Average reward: 2.0 Loss: 0.0027269867714494467 Epsilon: 0.8925865184568302\n",
      "4/1000 Total steps: 1160 Episode reward: 4.0 Average reward: 2.4 Loss: 0.0007982361712493002 Epsilon: 0.8901969675128499\n",
      "5/1000 Total steps: 1457 Episode reward: 4.0 Average reward: 2.6666666666666665 Loss: 0.0018873510416597128 Epsilon: 0.8877052845001843\n",
      "6/1000 Total steps: 1614 Episode reward: 1.0 Average reward: 2.4285714285714284 Loss: 0.020840086042881012 Epsilon: 0.8863911190933038\n",
      "7/1000 Total steps: 1800 Episode reward: 1.0 Average reward: 2.25 Loss: 0.002523174975067377 Epsilon: 0.8848368775045556\n",
      "8/1000 Total steps: 1987 Episode reward: 1.0 Average reward: 2.111111111111111 Loss: 0.01431548036634922 Epsilon: 0.8832771913047245\n",
      "9/1000 Total steps: 2222 Episode reward: 2.0 Average reward: 2.1 Loss: 0.0012600241461768746 Epsilon: 0.8813212889905001\n",
      "10/1000 Total steps: 2397 Episode reward: 1.0 Average reward: 2.0 Loss: 0.02146921679377556 Epsilon: 0.8798677489532548\n",
      "11/1000 Total steps: 2590 Episode reward: 2.0 Average reward: 2.0 Loss: 0.014347361400723457 Epsilon: 0.8782676487911159\n",
      "12/1000 Total steps: 2719 Episode reward: 0.0 Average reward: 1.8461538461538463 Loss: 0.00527161592617631 Epsilon: 0.8771998723880292\n",
      "13/1000 Total steps: 2929 Episode reward: 2.0 Average reward: 1.8571428571428572 Loss: 0.0017234212718904018 Epsilon: 0.87546457535562\n",
      "14/1000 Total steps: 3141 Episode reward: 2.0 Average reward: 1.8666666666666667 Loss: 0.001099183689802885 Epsilon: 0.873716444129699\n",
      "15/1000 Total steps: 3280 Episode reward: 0.0 Average reward: 1.75 Loss: 0.02619415894150734 Epsilon: 0.8725722736550595\n",
      "16/1000 Total steps: 3420 Episode reward: 0.0 Average reward: 1.6470588235294117 Loss: 0.0014960865955799818 Epsilon: 0.8714214782167125\n",
      "17/1000 Total steps: 3603 Episode reward: 1.0 Average reward: 1.6111111111111112 Loss: 0.0203222818672657 Epsilon: 0.869919651502142\n",
      "18/1000 Total steps: 3919 Episode reward: 5.0 Average reward: 1.7894736842105263 Loss: 0.011642947793006897 Epsilon: 0.8673327947896103\n",
      "19/1000 Total steps: 4073 Episode reward: 1.0 Average reward: 1.75 Loss: 0.0019940005149692297 Epsilon: 0.8660750709815348\n",
      "20/1000 Total steps: 4342 Episode reward: 3.0 Average reward: 1.8095238095238095 Loss: 0.000580448773689568 Epsilon: 0.8638827789952861\n",
      "21/1000 Total steps: 4531 Episode reward: 1.0 Average reward: 1.7727272727272727 Loss: 0.005131370387971401 Epsilon: 0.8623459932629647\n",
      "22/1000 Total steps: 4727 Episode reward: 1.0 Average reward: 1.7391304347826086 Loss: 0.005345583893358707 Epsilon: 0.860755354451421\n",
      "23/1000 Total steps: 4865 Episode reward: 0.0 Average reward: 1.6666666666666667 Loss: 0.0019461425254121423 Epsilon: 0.8596372837085285\n",
      "24/1000 Total steps: 5182 Episode reward: 4.0 Average reward: 1.76 Loss: 0.0008096303790807724 Epsilon: 0.857074797206126\n",
      "25/1000 Total steps: 5368 Episode reward: 1.0 Average reward: 1.7307692307692308 Loss: 0.006763335317373276 Epsilon: 0.8555750332961408\n",
      "26/1000 Total steps: 5499 Episode reward: 0.0 Average reward: 1.6666666666666667 Loss: 0.0007158985827118158 Epsilon: 0.8545204209244447\n",
      "27/1000 Total steps: 5755 Episode reward: 3.0 Average reward: 1.7142857142857142 Loss: 0.05976700782775879 Epsilon: 0.8524634826512303\n",
      "28/1000 Total steps: 5935 Episode reward: 1.0 Average reward: 1.6896551724137931 Loss: 0.003045927733182907 Epsilon: 0.8510203475936563\n",
      "29/1000 Total steps: 6144 Episode reward: 2.0 Average reward: 1.7 Loss: 0.0038414262235164642 Epsilon: 0.8493479633175158\n",
      "30/1000 Total steps: 6288 Episode reward: 0.0 Average reward: 1.6451612903225807 Loss: 0.00036961110890842974 Epsilon: 0.8481977306166435\n",
      "31/1000 Total steps: 6424 Episode reward: 0.0 Average reward: 1.59375 Loss: 0.014452639035880566 Epsilon: 0.8471129195417414\n",
      "32/1000 Total steps: 6552 Episode reward: 0.0 Average reward: 1.5454545454545454 Loss: 0.005827666725963354 Epsilon: 0.8460932677211096\n",
      "33/1000 Total steps: 6684 Episode reward: 0.0 Average reward: 1.5 Loss: 0.05353868752717972 Epsilon: 0.8450431178591084\n",
      "34/1000 Total steps: 6811 Episode reward: 0.0 Average reward: 1.457142857142857 Loss: 0.004082391504198313 Epsilon: 0.8440340539906104\n",
      "35/1000 Total steps: 6948 Episode reward: 0.0 Average reward: 1.4166666666666667 Loss: 0.0014037464279681444 Epsilon: 0.8429469721577275\n",
      "36/1000 Total steps: 7082 Episode reward: 0.0 Average reward: 1.3783783783783783 Loss: 0.0020314359571784735 Epsilon: 0.8418851348049488\n",
      "37/1000 Total steps: 7247 Episode reward: 1.0 Average reward: 1.368421052631579 Loss: 0.0003884684119839221 Epsilon: 0.8405796016935303\n",
      "38/1000 Total steps: 7501 Episode reward: 3.0 Average reward: 1.4102564102564104 Loss: 0.00016104087990242988 Epsilon: 0.8385740775990652\n",
      "39/1000 Total steps: 7791 Episode reward: 4.0 Average reward: 1.475 Loss: 0.01535984966903925 Epsilon: 0.8362905255249246\n",
      "40/1000 Total steps: 7968 Episode reward: 1.0 Average reward: 1.4634146341463414 Loss: 0.0001829039683798328 Epsilon: 0.8349000222531665\n",
      "41/1000 Total steps: 8111 Episode reward: 0.0 Average reward: 1.4285714285714286 Loss: 0.01724124886095524 Epsilon: 0.833778417359974\n",
      "42/1000 Total steps: 8352 Episode reward: 2.0 Average reward: 1.441860465116279 Loss: 0.017756789922714233 Epsilon: 0.8318917856784581\n",
      "43/1000 Total steps: 8528 Episode reward: 1.0 Average reward: 1.4318181818181819 Loss: 0.01698736473917961 Epsilon: 0.8305168664195243\n",
      "44/1000 Total steps: 8752 Episode reward: 2.0 Average reward: 1.4444444444444444 Loss: 0.0002722802455537021 Epsilon: 0.8287704653381841\n",
      "45/1000 Total steps: 8887 Episode reward: 0.0 Average reward: 1.4130434782608696 Loss: 0.0026077949441969395 Epsilon: 0.8277198345453273\n",
      "46/1000 Total steps: 9019 Episode reward: 0.0 Average reward: 1.3829787234042554 Loss: 0.00021940158330835402 Epsilon: 0.8266939216152239\n",
      "47/1000 Total steps: 9244 Episode reward: 2.0 Average reward: 1.3958333333333333 Loss: 0.0018782452680170536 Epsilon: 0.824948324824403\n",
      "48/1000 Total steps: 9382 Episode reward: 0.0 Average reward: 1.3673469387755102 Loss: 0.01619136519730091 Epsilon: 0.8238796337026205\n",
      "49/1000 Total steps: 9636 Episode reward: 3.0 Average reward: 1.4 Loss: 0.029846832156181335 Epsilon: 0.8219164737016772\n",
      "50/1000 Total steps: 9766 Episode reward: 0.0 Average reward: 1.3725490196078431 Loss: 0.0021900967694818974 Epsilon: 0.8209136342727271\n",
      "51/1000 Total steps: 10040 Episode reward: 3.0 Average reward: 1.4038461538461537 Loss: 0.00012324436102062464 Epsilon: 0.8188042221291748\n",
      "52/1000 Total steps: 10290 Episode reward: 3.0 Average reward: 1.4339622641509433 Loss: 5.200585292186588e-05 Epsilon: 0.8168846120862024\n",
      "53/1000 Total steps: 10422 Episode reward: 0.0 Average reward: 1.4074074074074074 Loss: 0.0001905662502394989 Epsilon: 0.8158729922142514\n",
      "54/1000 Total steps: 10557 Episode reward: 0.0 Average reward: 1.3818181818181818 Loss: 0.0015105678467079997 Epsilon: 0.8148397612625764\n",
      "55/1000 Total steps: 10746 Episode reward: 2.0 Average reward: 1.3928571428571428 Loss: 0.0009302911348640919 Epsilon: 0.8133955792956457\n",
      "56/1000 Total steps: 10922 Episode reward: 1.0 Average reward: 1.3859649122807018 Loss: 0.0002900061081163585 Epsilon: 0.81205318472982\n",
      "57/1000 Total steps: 11105 Episode reward: 1.0 Average reward: 1.3793103448275863 Loss: 0.016961917281150818 Epsilon: 0.8106599026437035\n",
      "58/1000 Total steps: 11324 Episode reward: 2.0 Average reward: 1.3898305084745763 Loss: 0.0017870933515951037 Epsilon: 0.8089958802265287\n",
      "59/1000 Total steps: 11545 Episode reward: 2.0 Average reward: 1.4 Loss: 0.0012871555518358946 Epsilon: 0.8073203514724554\n",
      "60/1000 Total steps: 11728 Episode reward: 1.0 Average reward: 1.3934426229508197 Loss: 0.01525658555328846 Epsilon: 0.8059357225511391\n",
      "61/1000 Total steps: 11900 Episode reward: 1.0 Average reward: 1.3870967741935485 Loss: 3.9126371120801196e-05 Epsilon: 0.8046366306476576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/1000 Total steps: 12064 Episode reward: 1.0 Average reward: 1.380952380952381 Loss: 0.015500007197260857 Epsilon: 0.803400040854187\n",
      "63/1000 Total steps: 12192 Episode reward: 0.0 Average reward: 1.359375 Loss: 6.808726175222546e-05 Epsilon: 0.8024363057239591\n",
      "64/1000 Total steps: 12346 Episode reward: 1.0 Average reward: 1.353846153846154 Loss: 0.00035163311986252666 Epsilon: 0.8012784455942757\n",
      "65/1000 Total steps: 12601 Episode reward: 3.0 Average reward: 1.378787878787879 Loss: 0.015256588347256184 Epsilon: 0.7993651260771745\n",
      "66/1000 Total steps: 12755 Episode reward: 0.0 Average reward: 1.3582089552238805 Loss: 0.01586870476603508 Epsilon: 0.7982119919242111\n",
      "67/1000 Total steps: 12890 Episode reward: 0.0 Average reward: 1.338235294117647 Loss: 3.5427932743914425e-05 Epsilon: 0.797202587236581\n",
      "68/1000 Total steps: 13068 Episode reward: 1.0 Average reward: 1.3333333333333333 Loss: 2.7581034373724833e-05 Epsilon: 0.7958737496476115\n",
      "69/1000 Total steps: 13296 Episode reward: 2.0 Average reward: 1.3428571428571427 Loss: 0.000593088916502893 Epsilon: 0.7941750947009114\n",
      "70/1000 Total steps: 13455 Episode reward: 0.0 Average reward: 1.323943661971831 Loss: 0.015429573133587837 Epsilon: 0.792992796476506\n",
      "71/1000 Total steps: 13707 Episode reward: 3.0 Average reward: 1.3472222222222223 Loss: 0.0008235780405811965 Epsilon: 0.7911228117996738\n",
      "72/1000 Total steps: 13852 Episode reward: 0.0 Average reward: 1.3287671232876712 Loss: 9.854988456936553e-05 Epsilon: 0.790048962451489\n",
      "73/1000 Total steps: 13991 Episode reward: 0.0 Average reward: 1.3108108108108107 Loss: 0.015342414379119873 Epsilon: 0.7890210089868485\n",
      "74/1000 Total steps: 14115 Episode reward: 0.0 Average reward: 1.2933333333333332 Loss: 0.00016435905126854777 Epsilon: 0.7881051908602901\n",
      "75/1000 Total steps: 14302 Episode reward: 2.0 Average reward: 1.3026315789473684 Loss: 0.00015688251005485654 Epsilon: 0.7867262238893415\n",
      "76/1000 Total steps: 14557 Episode reward: 3.0 Average reward: 1.3246753246753247 Loss: 0.00010455532901687548 Epsilon: 0.7848499652648676\n",
      "77/1000 Total steps: 14694 Episode reward: 0.0 Average reward: 1.3076923076923077 Loss: 0.00011735012958524749 Epsilon: 0.7838439101175859\n",
      "78/1000 Total steps: 14851 Episode reward: 1.0 Average reward: 1.3037974683544304 Loss: 0.0009459020802751184 Epsilon: 0.7826926791314979\n",
      "79/1000 Total steps: 15064 Episode reward: 2.0 Average reward: 1.3125 Loss: 0.0009355774964205921 Epsilon: 0.7811337046222094\n",
      "80/1000 Total steps: 15302 Episode reward: 2.0 Average reward: 1.3209876543209877 Loss: 0.0011091675842180848 Epsilon: 0.779395675480295\n",
      "81/1000 Total steps: 15574 Episode reward: 3.0 Average reward: 1.3414634146341464 Loss: 6.0076108638895676e-05 Epsilon: 0.7774144149787837\n",
      "82/1000 Total steps: 15708 Episode reward: 0.0 Average reward: 1.3253012048192772 Loss: 0.00011630247172433883 Epsilon: 0.7764403324437658\n",
      "83/1000 Total steps: 15966 Episode reward: 3.0 Average reward: 1.3452380952380953 Loss: 0.030623937025666237 Epsilon: 0.7745685320468605\n",
      "84/1000 Total steps: 16304 Episode reward: 5.0 Average reward: 1.388235294117647 Loss: 0.0010864471551030874 Epsilon: 0.77212362462971\n",
      "85/1000 Total steps: 16510 Episode reward: 2.0 Average reward: 1.3953488372093024 Loss: 0.031043188646435738 Epsilon: 0.7706375811133092\n",
      "86/1000 Total steps: 16685 Episode reward: 1.0 Average reward: 1.3908045977011494 Loss: 0.00043901370372623205 Epsilon: 0.769377568179244\n",
      "87/1000 Total steps: 16834 Episode reward: 0.0 Average reward: 1.375 Loss: 0.017012687399983406 Epsilon: 0.7683064937512635\n",
      "88/1000 Total steps: 16972 Episode reward: 0.0 Average reward: 1.3595505617977528 Loss: 0.0007280229474417865 Epsilon: 0.7673159144468116\n",
      "89/1000 Total steps: 17156 Episode reward: 1.0 Average reward: 1.3555555555555556 Loss: 0.00010275986278429627 Epsilon: 0.7659972666921983\n",
      "90/1000 Total steps: 17291 Episode reward: 0.0 Average reward: 1.3406593406593406 Loss: 2.9107384762028232e-05 Epsilon: 0.7650313225411685\n",
      "91/1000 Total steps: 17560 Episode reward: 3.0 Average reward: 1.358695652173913 Loss: 0.00012468817294575274 Epsilon: 0.7631104729844747\n",
      "92/1000 Total steps: 17703 Episode reward: 0.0 Average reward: 1.3440860215053763 Loss: 0.01572243496775627 Epsilon: 0.762091453780487\n",
      "93/1000 Total steps: 17856 Episode reward: 0.0 Average reward: 1.3297872340425532 Loss: 0.01649441011250019 Epsilon: 0.7610027868987391\n",
      "94/1000 Total steps: 17982 Episode reward: 0.0 Average reward: 1.3157894736842106 Loss: 0.00010161077079828829 Epsilon: 0.760107487544288\n",
      "95/1000 Total steps: 18295 Episode reward: 4.0 Average reward: 1.34375 Loss: 0.0003510057576932013 Epsilon: 0.7578883259079771\n",
      "96/1000 Total steps: 18522 Episode reward: 2.0 Average reward: 1.3505154639175259 Loss: 0.015262185595929623 Epsilon: 0.756283241867788\n",
      "97/1000 Total steps: 18650 Episode reward: 0.0 Average reward: 1.336734693877551 Loss: 5.16327127115801e-05 Epsilon: 0.7553797776586441\n",
      "98/1000 Total steps: 18965 Episode reward: 4.0 Average reward: 1.3636363636363635 Loss: 0.00017425989790353924 Epsilon: 0.7531613272527894\n",
      "99/1000 Total steps: 19154 Episode reward: 1.0 Average reward: 1.36 Loss: 0.015330548398196697 Epsilon: 0.7518336074347387\n",
      "100/1000 Total steps: 19437 Episode reward: 3.0 Average reward: 1.3762376237623761 Loss: 0.0006660617073066533 Epsilon: 0.749850226133964\n",
      "101/1000 Total steps: 19738 Episode reward: 4.0 Average reward: 1.4019607843137254 Loss: 7.594929775223136e-05 Epsilon: 0.7477468441312854\n",
      "102/1000 Total steps: 19869 Episode reward: 0.0 Average reward: 1.3883495145631068 Loss: 4.5560318540083244e-05 Epsilon: 0.7468333942058056\n",
      "103/1000 Total steps: 20134 Episode reward: 3.0 Average reward: 1.4038461538461537 Loss: 6.90590386511758e-05 Epsilon: 0.7449892303075454\n",
      "104/1000 Total steps: 20381 Episode reward: 2.0 Average reward: 1.4095238095238096 Loss: 0.0010669119656085968 Epsilon: 0.7432747251941694\n",
      "105/1000 Total steps: 20564 Episode reward: 1.0 Average reward: 1.4056603773584906 Loss: 0.00026507797883823514 Epsilon: 0.7420071925931306\n",
      "106/1000 Total steps: 20705 Episode reward: 0.0 Average reward: 1.3925233644859814 Loss: 4.55027366115246e-05 Epsilon: 0.7410321500181299\n",
      "107/1000 Total steps: 20894 Episode reward: 1.0 Average reward: 1.3888888888888888 Loss: 0.01569829322397709 Epsilon: 0.7397273326953772\n",
      "108/1000 Total steps: 21022 Episode reward: 0.0 Average reward: 1.3761467889908257 Loss: 0.00013119624054525048 Epsilon: 0.738845046493158\n",
      "109/1000 Total steps: 21165 Episode reward: 0.0 Average reward: 1.3636363636363635 Loss: 3.545183062669821e-05 Epsilon: 0.7378607020506895\n",
      "110/1000 Total steps: 21376 Episode reward: 2.0 Average reward: 1.3693693693693694 Loss: 0.00010550559818511829 Epsilon: 0.7364108461052936\n",
      "111/1000 Total steps: 21581 Episode reward: 2.0 Average reward: 1.375 Loss: 4.805826029041782e-05 Epsilon: 0.7350051452064871\n",
      "112/1000 Total steps: 21756 Episode reward: 1.0 Average reward: 1.3716814159292035 Loss: 0.0008558949921280146 Epsilon: 0.7338074345049054\n",
      "113/1000 Total steps: 21913 Episode reward: 1.0 Average reward: 1.368421052631579 Loss: 0.0017363886581733823 Epsilon: 0.7327346991503348\n",
      "114/1000 Total steps: 22055 Episode reward: 0.0 Average reward: 1.3565217391304347 Loss: 6.10364368185401e-05 Epsilon: 0.7317659038849696\n",
      "115/1000 Total steps: 22188 Episode reward: 0.0 Average reward: 1.3448275862068966 Loss: 0.029261238873004913 Epsilon: 0.7308597579534205\n",
      "116/1000 Total steps: 22319 Episode reward: 0.0 Average reward: 1.3333333333333333 Loss: 2.625844172143843e-05 Epsilon: 0.7299684156271946\n",
      "117/1000 Total steps: 22513 Episode reward: 1.0 Average reward: 1.3305084745762712 Loss: 0.00010342524183215573 Epsilon: 0.7286505556383918\n",
      "118/1000 Total steps: 22812 Episode reward: 5.0 Average reward: 1.361344537815126 Loss: 0.0003332355699967593 Epsilon: 0.7266244210577181\n",
      "119/1000 Total steps: 23019 Episode reward: 2.0 Average reward: 1.3666666666666667 Loss: 0.015443554148077965 Epsilon: 0.7252252571403894\n",
      "120/1000 Total steps: 23327 Episode reward: 4.0 Average reward: 1.3884297520661157 Loss: 5.4087082389742136e-05 Epsilon: 0.7231487627912325\n",
      "121/1000 Total steps: 23458 Episode reward: 0.0 Average reward: 1.3770491803278688 Loss: 0.015728116035461426 Epsilon: 0.7222675152551379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/1000 Total steps: 23631 Episode reward: 1.0 Average reward: 1.3739837398373984 Loss: 0.0002495630760677159 Epsilon: 0.7211054978885854\n",
      "123/1000 Total steps: 23849 Episode reward: 2.0 Average reward: 1.3790322580645162 Loss: 0.00040044443449005485 Epsilon: 0.7196440814259021\n",
      "124/1000 Total steps: 24123 Episode reward: 3.0 Average reward: 1.392 Loss: 0.0005169283249415457 Epsilon: 0.7178117680584647\n",
      "125/1000 Total steps: 24259 Episode reward: 0.0 Average reward: 1.380952380952381 Loss: 0.0009648395935073495 Epsilon: 0.7169041613663483\n",
      "126/1000 Total steps: 24476 Episode reward: 2.0 Average reward: 1.3858267716535433 Loss: 0.0012485700426623225 Epsilon: 0.7154585483935294\n",
      "127/1000 Total steps: 24650 Episode reward: 1.0 Average reward: 1.3828125 Loss: 0.00012436654651537538 Epsilon: 0.714301657306454\n",
      "128/1000 Total steps: 24835 Episode reward: 1.0 Average reward: 1.37984496124031 Loss: 7.765452755847946e-05 Epsilon: 0.713073835325954\n",
      "129/1000 Total steps: 25040 Episode reward: 2.0 Average reward: 1.3846153846153846 Loss: 4.473822264117189e-05 Epsilon: 0.7117159262958427\n",
      "130/1000 Total steps: 25176 Episode reward: 0.0 Average reward: 1.3740458015267176 Loss: 0.030380617827177048 Epsilon: 0.7108166043136438\n",
      "131/1000 Total steps: 25362 Episode reward: 2.0 Average reward: 1.378787878787879 Loss: 0.0323377288877964 Epsilon: 0.709588627801802\n",
      "132/1000 Total steps: 25518 Episode reward: 1.0 Average reward: 1.3759398496240602 Loss: 6.755081267328933e-05 Epsilon: 0.7085604717126908\n",
      "133/1000 Total steps: 25701 Episode reward: 1.0 Average reward: 1.373134328358209 Loss: 0.00045573542593047023 Epsilon: 0.7073564081036827\n",
      "134/1000 Total steps: 25932 Episode reward: 2.0 Average reward: 1.3777777777777778 Loss: 0.00020941506954841316 Epsilon: 0.7058396673110355\n",
      "135/1000 Total steps: 26107 Episode reward: 1.0 Average reward: 1.375 Loss: 9.359318937640637e-05 Epsilon: 0.7046929515621728\n",
      "136/1000 Total steps: 26363 Episode reward: 3.0 Average reward: 1.3868613138686132 Loss: 0.00012634128506761044 Epsilon: 0.7030190810745541\n",
      "137/1000 Total steps: 26616 Episode reward: 3.0 Average reward: 1.3985507246376812 Loss: 0.0001244943414349109 Epsilon: 0.7013690309929391\n",
      "138/1000 Total steps: 26756 Episode reward: 0.0 Average reward: 1.3884892086330936 Loss: 5.642497490043752e-05 Epsilon: 0.7004577523934108\n",
      "139/1000 Total steps: 26884 Episode reward: 0.0 Average reward: 1.3785714285714286 Loss: 0.000109394735773094 Epsilon: 0.6996256990980593\n",
      "140/1000 Total steps: 27028 Episode reward: 0.0 Average reward: 1.3687943262411348 Loss: 0.0007179586100392044 Epsilon: 0.698690911300104\n",
      "141/1000 Total steps: 27232 Episode reward: 2.0 Average reward: 1.3732394366197183 Loss: 0.0001339545560767874 Epsilon: 0.6973689307197066\n",
      "142/1000 Total steps: 27473 Episode reward: 3.0 Average reward: 1.3846153846153846 Loss: 5.8475055993767455e-05 Epsilon: 0.6958106500790648\n",
      "143/1000 Total steps: 27762 Episode reward: 4.0 Average reward: 1.4027777777777777 Loss: 5.185142072150484e-05 Epsilon: 0.6939469516417277\n",
      "144/1000 Total steps: 28004 Episode reward: 3.0 Average reward: 1.4137931034482758 Loss: 0.00014098848623689264 Epsilon: 0.6923904841040833\n",
      "145/1000 Total steps: 28269 Episode reward: 4.0 Average reward: 1.4315068493150684 Loss: 0.0001104317489080131 Epsilon: 0.6906904029236731\n",
      "146/1000 Total steps: 28508 Episode reward: 2.0 Average reward: 1.435374149659864 Loss: 0.0005301306373439729 Epsilon: 0.6891609812476057\n",
      "147/1000 Total steps: 28648 Episode reward: 0.0 Average reward: 1.4256756756756757 Loss: 0.0009037909330800176 Epsilon: 0.6882667819594134\n",
      "148/1000 Total steps: 28808 Episode reward: 1.0 Average reward: 1.4228187919463087 Loss: 0.030775753781199455 Epsilon: 0.6872463716542101\n",
      "149/1000 Total steps: 28950 Episode reward: 0.0 Average reward: 1.4133333333333333 Loss: 0.01468697376549244 Epsilon: 0.6863421239742576\n",
      "150/1000 Total steps: 29090 Episode reward: 0.0 Average reward: 1.403973509933775 Loss: 0.01594400592148304 Epsilon: 0.6854518683250564\n",
      "151/1000 Total steps: 29219 Episode reward: 0.0 Average reward: 1.394736842105263 Loss: 0.00033169996459037066 Epsilon: 0.6846326639153646\n",
      "152/1000 Total steps: 29427 Episode reward: 2.0 Average reward: 1.3986928104575163 Loss: 3.135840597678907e-05 Epsilon: 0.68331399986046\n",
      "153/1000 Total steps: 29569 Episode reward: 0.0 Average reward: 1.3896103896103895 Loss: 0.029447879642248154 Epsilon: 0.6824153321857134\n",
      "154/1000 Total steps: 29739 Episode reward: 1.0 Average reward: 1.3870967741935485 Loss: 1.963014801731333e-05 Epsilon: 0.6813411394435299\n",
      "155/1000 Total steps: 29925 Episode reward: 1.0 Average reward: 1.3846153846153846 Loss: 0.01650063693523407 Epsilon: 0.6801679363412845\n",
      "156/1000 Total steps: 30222 Episode reward: 3.0 Average reward: 1.394904458598726 Loss: 0.01614808477461338 Epsilon: 0.6782991141450365\n",
      "157/1000 Total steps: 30494 Episode reward: 3.0 Average reward: 1.4050632911392404 Loss: 0.00014970450138207525 Epsilon: 0.6765924626527989\n",
      "158/1000 Total steps: 30629 Episode reward: 0.0 Average reward: 1.3962264150943395 Loss: 2.8374128305586055e-05 Epsilon: 0.6757471335537438\n",
      "159/1000 Total steps: 30818 Episode reward: 1.0 Average reward: 1.39375 Loss: 0.014878501184284687 Epsilon: 0.6745655883832299\n",
      "160/1000 Total steps: 30990 Episode reward: 1.0 Average reward: 1.391304347826087 Loss: 1.6316496839863248e-05 Epsilon: 0.6734922588991784\n",
      "161/1000 Total steps: 31151 Episode reward: 1.0 Average reward: 1.3888888888888888 Loss: 0.030550338327884674 Epsilon: 0.6724892440059993\n",
      "162/1000 Total steps: 31386 Episode reward: 2.0 Average reward: 1.392638036809816 Loss: 6.29904170637019e-05 Epsilon: 0.6710281117853697\n",
      "163/1000 Total steps: 31726 Episode reward: 5.0 Average reward: 1.4146341463414633 Loss: 0.0001556915376568213 Epsilon: 0.668920201683093\n",
      "164/1000 Total steps: 32072 Episode reward: 5.0 Average reward: 1.4363636363636363 Loss: 0.00015406800957862288 Epsilon: 0.6667824382487145\n",
      "165/1000 Total steps: 32293 Episode reward: 1.0 Average reward: 1.4337349397590362 Loss: 0.000254984013736248 Epsilon: 0.6654208541647737\n",
      "166/1000 Total steps: 32440 Episode reward: 0.0 Average reward: 1.4251497005988023 Loss: 2.2507758330903016e-05 Epsilon: 0.6645168501149166\n",
      "167/1000 Total steps: 32584 Episode reward: 0.0 Average reward: 1.4166666666666667 Loss: 0.030915167182683945 Epsilon: 0.6636325826761085\n",
      "168/1000 Total steps: 32840 Episode reward: 3.0 Average reward: 1.4260355029585798 Loss: 7.937150076031685e-05 Epsilon: 0.6620636923009611\n",
      "169/1000 Total steps: 33234 Episode reward: 6.0 Average reward: 1.4529411764705882 Loss: 0.00012382320710457861 Epsilon: 0.6596569058361293\n",
      "170/1000 Total steps: 33371 Episode reward: 0.0 Average reward: 1.4444444444444444 Loss: 2.664071143954061e-05 Epsilon: 0.6588222477464727\n",
      "171/1000 Total steps: 33599 Episode reward: 2.0 Average reward: 1.447674418604651 Loss: 0.0007168324082158506 Epsilon: 0.6574357142704198\n",
      "172/1000 Total steps: 33806 Episode reward: 2.0 Average reward: 1.4508670520231215 Loss: 0.015411562286317348 Epsilon: 0.6561796228450241\n",
      "173/1000 Total steps: 33956 Episode reward: 0.0 Average reward: 1.4425287356321839 Loss: 0.000505062926094979 Epsilon: 0.655271035021984\n",
      "174/1000 Total steps: 34168 Episode reward: 1.0 Average reward: 1.44 Loss: 7.291607471415773e-05 Epsilon: 0.6539892196321332\n",
      "175/1000 Total steps: 34300 Episode reward: 0.0 Average reward: 1.4318181818181819 Loss: 7.52491905586794e-05 Epsilon: 0.6531924798261775\n",
      "176/1000 Total steps: 34438 Episode reward: 0.0 Average reward: 1.423728813559322 Loss: 8.491058542858809e-05 Epsilon: 0.6523606482997821\n",
      "177/1000 Total steps: 34608 Episode reward: 1.0 Average reward: 1.4213483146067416 Loss: 0.001474074088037014 Epsilon: 0.6513375051157858\n",
      "178/1000 Total steps: 34814 Episode reward: 2.0 Average reward: 1.4245810055865922 Loss: 4.600388274411671e-05 Epsilon: 0.6501000248974864\n",
      "179/1000 Total steps: 34955 Episode reward: 0.0 Average reward: 1.4166666666666667 Loss: 0.015159562230110168 Epsilon: 0.6492544801115407\n",
      "180/1000 Total steps: 35136 Episode reward: 1.0 Average reward: 1.4143646408839778 Loss: 0.00011890068708453327 Epsilon: 0.6481708105193705\n",
      "181/1000 Total steps: 35394 Episode reward: 3.0 Average reward: 1.4230769230769231 Loss: 0.0007885899394750595 Epsilon: 0.6466295189493103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/1000 Total steps: 35648 Episode reward: 3.0 Average reward: 1.4316939890710383 Loss: 0.000917077821213752 Epsilon: 0.6451160029502144\n",
      "183/1000 Total steps: 35785 Episode reward: 0.0 Average reward: 1.423913043478261 Loss: 8.23110094643198e-05 Epsilon: 0.6443012522578307\n",
      "184/1000 Total steps: 35971 Episode reward: 1.0 Average reward: 1.4216216216216215 Loss: 0.0011378306662663817 Epsilon: 0.6431968793138597\n",
      "185/1000 Total steps: 36160 Episode reward: 1.0 Average reward: 1.4193548387096775 Loss: 0.015347982756793499 Epsilon: 0.6420767960240862\n",
      "186/1000 Total steps: 36418 Episode reward: 3.0 Average reward: 1.427807486631016 Loss: 5.8239675126969814e-05 Epsilon: 0.640551206746756\n",
      "187/1000 Total steps: 36607 Episode reward: 1.0 Average reward: 1.425531914893617 Loss: 0.016347568482160568 Epsilon: 0.6394361190558063\n",
      "188/1000 Total steps: 36750 Episode reward: 0.0 Average reward: 1.417989417989418 Loss: 1.646095552132465e-05 Epsilon: 0.638593827787347\n",
      "189/1000 Total steps: 36958 Episode reward: 2.0 Average reward: 1.4210526315789473 Loss: 0.015566453337669373 Epsilon: 0.6373708249893923\n",
      "190/1000 Total steps: 37093 Episode reward: 0.0 Average reward: 1.4136125654450262 Loss: 0.0008956938399933279 Epsilon: 0.6365784093765435\n",
      "191/1000 Total steps: 37215 Episode reward: 0.0 Average reward: 1.40625 Loss: 7.003735663602129e-05 Epsilon: 0.6358632200712876\n",
      "192/1000 Total steps: 37427 Episode reward: 2.0 Average reward: 1.4093264248704662 Loss: 0.00016665758448652923 Epsilon: 0.6346225056666942\n",
      "193/1000 Total steps: 37563 Episode reward: 0.0 Average reward: 1.402061855670103 Loss: 0.00012965474161319435 Epsilon: 0.6338279594728653\n",
      "194/1000 Total steps: 37713 Episode reward: 0.0 Average reward: 1.3948717948717948 Loss: 0.00010166617721552029 Epsilon: 0.6329528740118303\n",
      "195/1000 Total steps: 37896 Episode reward: 1.0 Average reward: 1.3928571428571428 Loss: 6.334564386634156e-05 Epsilon: 0.6318870457826643\n",
      "196/1000 Total steps: 38039 Episode reward: 0.0 Average reward: 1.385786802030457 Loss: 1.769945265550632e-05 Epsilon: 0.6310555419741135\n",
      "197/1000 Total steps: 38221 Episode reward: 1.0 Average reward: 1.3838383838383839 Loss: 0.0005296929739415646 Epsilon: 0.6299989826483526\n",
      "198/1000 Total steps: 38392 Episode reward: 1.0 Average reward: 1.3819095477386936 Loss: 0.0004267472540959716 Epsilon: 0.6290080318923902\n",
      "199/1000 Total steps: 38541 Episode reward: 0.0 Average reward: 1.375 Loss: 0.014925472438335419 Epsilon: 0.6281459523336337\n",
      "200/1000 Total steps: 38691 Episode reward: 0.0 Average reward: 1.3681592039800996 Loss: 0.0008709468529559672 Epsilon: 0.6272793834942445\n",
      "201/1000 Total steps: 38894 Episode reward: 1.0 Average reward: 1.3663366336633664 Loss: 5.482562119141221e-05 Epsilon: 0.6261086949966004\n",
      "202/1000 Total steps: 39055 Episode reward: 1.0 Average reward: 1.3645320197044335 Loss: 0.00011469375749584287 Epsilon: 0.6251819062627807\n",
      "203/1000 Total steps: 39231 Episode reward: 1.0 Average reward: 1.3627450980392157 Loss: 0.015307013876736164 Epsilon: 0.6241704764270973\n",
      "204/1000 Total steps: 39361 Episode reward: 0.0 Average reward: 1.3560975609756099 Loss: 0.0002385346160735935 Epsilon: 0.6234245397716209\n",
      "205/1000 Total steps: 39613 Episode reward: 3.0 Average reward: 1.3640776699029127 Loss: 0.01575285568833351 Epsilon: 0.6219813291405386\n",
      "206/1000 Total steps: 39826 Episode reward: 2.0 Average reward: 1.3671497584541064 Loss: 0.001359546440653503 Epsilon: 0.6207643054997728\n",
      "207/1000 Total steps: 40035 Episode reward: 2.0 Average reward: 1.3701923076923077 Loss: 7.155866478569806e-05 Epsilon: 0.6195726538110642\n",
      "208/1000 Total steps: 40172 Episode reward: 0.0 Average reward: 1.3636363636363635 Loss: 0.00011615121184149757 Epsilon: 0.618792873546788\n",
      "209/1000 Total steps: 40303 Episode reward: 0.0 Average reward: 1.3571428571428572 Loss: 7.077479676809162e-05 Epsilon: 0.6180482427221203\n",
      "210/1000 Total steps: 40547 Episode reward: 3.0 Average reward: 1.3649289099526067 Loss: 0.015986638143658638 Epsilon: 0.6166638946014068\n",
      "211/1000 Total steps: 40689 Episode reward: 0.0 Average reward: 1.3584905660377358 Loss: 0.00021082465536892414 Epsilon: 0.6158598029112871\n",
      "212/1000 Total steps: 40870 Episode reward: 1.0 Average reward: 1.3568075117370892 Loss: 0.015703827142715454 Epsilon: 0.614836523015687\n",
      "213/1000 Total steps: 41009 Episode reward: 0.0 Average reward: 1.3504672897196262 Loss: 0.00020378868794068694 Epsilon: 0.6140519456562835\n",
      "214/1000 Total steps: 41239 Episode reward: 2.0 Average reward: 1.3534883720930233 Loss: 0.0001009801053442061 Epsilon: 0.6127561169555243\n",
      "215/1000 Total steps: 41487 Episode reward: 3.0 Average reward: 1.3611111111111112 Loss: 0.01623675785958767 Epsilon: 0.611362210943353\n",
      "216/1000 Total steps: 41685 Episode reward: 2.0 Average reward: 1.3640552995391706 Loss: 0.015557070262730122 Epsilon: 0.6102518134219981\n",
      "217/1000 Total steps: 41917 Episode reward: 2.0 Average reward: 1.3669724770642202 Loss: 2.486055382178165e-05 Epsilon: 0.6089535357992222\n",
      "218/1000 Total steps: 42056 Episode reward: 0.0 Average reward: 1.360730593607306 Loss: 9.304258128395304e-05 Epsilon: 0.6081771301114222\n",
      "219/1000 Total steps: 42264 Episode reward: 2.0 Average reward: 1.3636363636363635 Loss: 0.00012861554569099098 Epsilon: 0.607017328292829\n",
      "220/1000 Total steps: 42422 Episode reward: 1.0 Average reward: 1.3619909502262444 Loss: 0.015137198381125927 Epsilon: 0.6061379358171249\n",
      "221/1000 Total steps: 42561 Episode reward: 0.0 Average reward: 1.3558558558558558 Loss: 0.0010561046656221151 Epsilon: 0.6053654410945495\n",
      "222/1000 Total steps: 42766 Episode reward: 2.0 Average reward: 1.358744394618834 Loss: 5.821805461891927e-05 Epsilon: 0.6042281081049234\n",
      "223/1000 Total steps: 43108 Episode reward: 5.0 Average reward: 1.375 Loss: 0.00017507544544059783 Epsilon: 0.6023358855201735\n",
      "224/1000 Total steps: 43391 Episode reward: 3.0 Average reward: 1.3822222222222222 Loss: 5.069952749181539e-06 Epsilon: 0.6007749846805975\n",
      "225/1000 Total steps: 43575 Episode reward: 1.0 Average reward: 1.3805309734513274 Loss: 6.648738781223074e-05 Epsilon: 0.5997624904890997\n",
      "226/1000 Total steps: 43841 Episode reward: 3.0 Average reward: 1.3876651982378854 Loss: 5.9962429077131674e-05 Epsilon: 0.5983020654907617\n",
      "227/1000 Total steps: 44024 Episode reward: 1.0 Average reward: 1.3859649122807018 Loss: 0.0008734129369258881 Epsilon: 0.5972995902555196\n",
      "228/1000 Total steps: 44284 Episode reward: 3.0 Average reward: 1.3930131004366813 Loss: 2.366708213230595e-05 Epsilon: 0.5958784595912889\n",
      "229/1000 Total steps: 44458 Episode reward: 1.0 Average reward: 1.391304347826087 Loss: 0.015778006985783577 Epsilon: 0.5949294569433372\n",
      "230/1000 Total steps: 44669 Episode reward: 2.0 Average reward: 1.393939393939394 Loss: 0.0005585638573393226 Epsilon: 0.5937808679766826\n",
      "231/1000 Total steps: 44823 Episode reward: 0.0 Average reward: 1.3879310344827587 Loss: 0.015332765877246857 Epsilon: 0.592944089924474\n",
      "232/1000 Total steps: 44956 Episode reward: 0.0 Average reward: 1.3819742489270386 Loss: 8.532252832083032e-05 Epsilon: 0.5922224542789538\n",
      "233/1000 Total steps: 45092 Episode reward: 0.0 Average reward: 1.376068376068376 Loss: 0.0001245256862603128 Epsilon: 0.5914855329612145\n",
      "234/1000 Total steps: 45262 Episode reward: 1.0 Average reward: 1.374468085106383 Loss: 0.0010575167834758759 Epsilon: 0.5905657895585776\n",
      "235/1000 Total steps: 45515 Episode reward: 3.0 Average reward: 1.38135593220339 Loss: 7.181301043601707e-05 Epsilon: 0.589199886706686\n",
      "236/1000 Total steps: 45646 Episode reward: 0.0 Average reward: 1.3755274261603376 Loss: 4.310587246436626e-05 Epsilon: 0.5884939973136007\n",
      "237/1000 Total steps: 45785 Episode reward: 0.0 Average reward: 1.3697478991596639 Loss: 0.0011005684500560164 Epsilon: 0.5877460106285131\n",
      "238/1000 Total steps: 46313 Episode reward: 12.0 Average reward: 1.4142259414225942 Loss: 0.015281816013157368 Epsilon: 0.584914194266463\n",
      "239/1000 Total steps: 46561 Episode reward: 2.0 Average reward: 1.4166666666666667 Loss: 8.987257024273276e-05 Epsilon: 0.5835892506738147\n",
      "240/1000 Total steps: 46745 Episode reward: 1.0 Average reward: 1.4149377593360997 Loss: 5.3006955567980185e-05 Epsilon: 0.5826083491587138\n",
      "241/1000 Total steps: 47208 Episode reward: 10.0 Average reward: 1.4504132231404958 Loss: 0.0001234861701959744 Epsilon: 0.5801480724377752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/1000 Total steps: 47501 Episode reward: 3.0 Average reward: 1.4567901234567902 Loss: 1.6943979062489234e-05 Epsilon: 0.5785970119987173\n",
      "243/1000 Total steps: 47627 Episode reward: 0.0 Average reward: 1.4508196721311475 Loss: 0.015435711480677128 Epsilon: 0.5779313991877303\n",
      "244/1000 Total steps: 47845 Episode reward: 2.0 Average reward: 1.453061224489796 Loss: 0.0001501801743870601 Epsilon: 0.5767817622970064\n",
      "245/1000 Total steps: 48054 Episode reward: 2.0 Average reward: 1.4552845528455285 Loss: 0.00010929422569461167 Epsilon: 0.5756819381304049\n",
      "246/1000 Total steps: 48194 Episode reward: 0.0 Average reward: 1.4493927125506072 Loss: 6.32812298135832e-05 Epsilon: 0.5749464983449939\n",
      "247/1000 Total steps: 48357 Episode reward: 0.0 Average reward: 1.4435483870967742 Loss: 3.05540525005199e-05 Epsilon: 0.5740915325391199\n",
      "248/1000 Total steps: 48601 Episode reward: 2.0 Average reward: 1.4457831325301205 Loss: 6.476070120697841e-05 Epsilon: 0.5728143080472782\n",
      "249/1000 Total steps: 48786 Episode reward: 0.0 Average reward: 1.44 Loss: 0.00010853380808839574 Epsilon: 0.5718479956919197\n",
      "250/1000 Total steps: 48932 Episode reward: 0.0 Average reward: 1.4342629482071714 Loss: 0.015216397121548653 Epsilon: 0.5710866535332251\n",
      "251/1000 Total steps: 49089 Episode reward: 1.0 Average reward: 1.4325396825396826 Loss: 0.00023352210700977594 Epsilon: 0.5702691893644644\n",
      "252/1000 Total steps: 49231 Episode reward: 0.0 Average reward: 1.4268774703557312 Loss: 0.00011359241034369916 Epsilon: 0.5695309314027716\n",
      "253/1000 Total steps: 49444 Episode reward: 2.0 Average reward: 1.4291338582677164 Loss: 0.03473486751317978 Epsilon: 0.568425508212514\n",
      "254/1000 Total steps: 49756 Episode reward: 4.0 Average reward: 1.4392156862745098 Loss: 0.00022093640291132033 Epsilon: 0.5668105412853583\n",
      "255/1000 Total steps: 49896 Episode reward: 0.0 Average reward: 1.43359375 Loss: 0.01491988729685545 Epsilon: 0.5660875127656173\n",
      "256/1000 Total steps: 50092 Episode reward: 1.0 Average reward: 1.4319066147859922 Loss: 0.015100421383976936 Epsilon: 0.5650769718941586\n",
      "257/1000 Total steps: 50272 Episode reward: 1.0 Average reward: 1.430232558139535 Loss: 0.00010001460032071918 Epsilon: 0.564150667269014\n",
      "258/1000 Total steps: 50401 Episode reward: 0.0 Average reward: 1.4247104247104247 Loss: 9.362761920783669e-05 Epsilon: 0.5634878405234054\n",
      "259/1000 Total steps: 50637 Episode reward: 3.0 Average reward: 1.4307692307692308 Loss: 0.00021662851213477552 Epsilon: 0.5622774380564692\n",
      "260/1000 Total steps: 50776 Episode reward: 0.0 Average reward: 1.4252873563218391 Loss: 0.000201494520297274 Epsilon: 0.5615658670739724\n",
      "261/1000 Total steps: 51001 Episode reward: 2.0 Average reward: 1.4274809160305344 Loss: 1.8398131942376494e-05 Epsilon: 0.5604161378035272\n",
      "262/1000 Total steps: 51154 Episode reward: 0.0 Average reward: 1.4220532319391634 Loss: 0.00014423907850869 Epsilon: 0.5596357982246902\n",
      "263/1000 Total steps: 51315 Episode reward: 1.0 Average reward: 1.4204545454545454 Loss: 0.01600014790892601 Epsilon: 0.5588159447486919\n",
      "264/1000 Total steps: 51611 Episode reward: 4.0 Average reward: 1.430188679245283 Loss: 3.576926974346861e-05 Epsilon: 0.5573120763754525\n",
      "265/1000 Total steps: 51790 Episode reward: 1.0 Average reward: 1.4285714285714286 Loss: 0.030487895011901855 Epsilon: 0.5564048000133348\n",
      "266/1000 Total steps: 52010 Episode reward: 2.0 Average reward: 1.4307116104868913 Loss: 0.00026953493943437934 Epsilon: 0.5552919340547159\n",
      "267/1000 Total steps: 52234 Episode reward: 2.0 Average reward: 1.4328358208955223 Loss: 0.015270350500941277 Epsilon: 0.5541613468528356\n",
      "268/1000 Total steps: 52379 Episode reward: 0.0 Average reward: 1.4275092936802973 Loss: 4.2159244912909344e-05 Epsilon: 0.5534308426434412\n",
      "269/1000 Total steps: 52551 Episode reward: 1.0 Average reward: 1.4259259259259258 Loss: 0.0001271292712772265 Epsilon: 0.5525656858422334\n",
      "270/1000 Total steps: 52729 Episode reward: 1.0 Average reward: 1.4243542435424354 Loss: 0.01665361225605011 Epsilon: 0.551671914613813\n",
      "271/1000 Total steps: 52902 Episode reward: 1.0 Average reward: 1.4227941176470589 Loss: 0.015520299784839153 Epsilon: 0.5508047724957357\n",
      "272/1000 Total steps: 53119 Episode reward: 2.0 Average reward: 1.424908424908425 Loss: 0.00011327719403197989 Epsilon: 0.5497192044067823\n",
      "273/1000 Total steps: 53281 Episode reward: 1.0 Average reward: 1.4233576642335766 Loss: 0.00012828987382818013 Epsilon: 0.5489103146732317\n",
      "274/1000 Total steps: 53454 Episode reward: 1.0 Average reward: 1.4218181818181819 Loss: 0.0014683480840176344 Epsilon: 0.5480479459928375\n",
      "275/1000 Total steps: 53739 Episode reward: 3.0 Average reward: 1.4275362318840579 Loss: 0.015134504996240139 Epsilon: 0.5466305301237845\n",
      "276/1000 Total steps: 54074 Episode reward: 7.0 Average reward: 1.447653429602888 Loss: 0.00016239208343904465 Epsilon: 0.5449696014567013\n",
      "277/1000 Total steps: 54393 Episode reward: 5.0 Average reward: 1.460431654676259 Loss: 6.488717917818576e-05 Epsilon: 0.5433931641823387\n",
      "278/1000 Total steps: 54533 Episode reward: 0.0 Average reward: 1.4551971326164874 Loss: 0.0007695513195358217 Epsilon: 0.5427028970522182\n",
      "279/1000 Total steps: 54697 Episode reward: 0.0 Average reward: 1.45 Loss: 0.00023898160725366324 Epsilon: 0.5418955265258428\n",
      "280/1000 Total steps: 54931 Episode reward: 2.0 Average reward: 1.4519572953736655 Loss: 0.0005635865963995457 Epsilon: 0.540745836655524\n",
      "281/1000 Total steps: 55126 Episode reward: 1.0 Average reward: 1.450354609929078 Loss: 0.00031227050931192935 Epsilon: 0.5397898146983934\n",
      "282/1000 Total steps: 55262 Episode reward: 0.0 Average reward: 1.4452296819787986 Loss: 7.338268187595531e-05 Epsilon: 0.5391241533027532\n",
      "283/1000 Total steps: 55493 Episode reward: 2.0 Average reward: 1.4471830985915493 Loss: 0.0001412419369444251 Epsilon: 0.5379955805120452\n",
      "284/1000 Total steps: 55670 Episode reward: 1.0 Average reward: 1.4456140350877194 Loss: 0.015244117937982082 Epsilon: 0.5371325923044072\n",
      "285/1000 Total steps: 56028 Episode reward: 5.0 Average reward: 1.4580419580419581 Loss: 0.000281146087218076 Epsilon: 0.5353917755452064\n",
      "286/1000 Total steps: 56215 Episode reward: 2.0 Average reward: 1.4599303135888502 Loss: 5.980984860798344e-05 Epsilon: 0.5344849410794215\n",
      "287/1000 Total steps: 56423 Episode reward: 2.0 Average reward: 1.4618055555555556 Loss: 0.000861639273352921 Epsilon: 0.5334782597135389\n",
      "288/1000 Total steps: 56710 Episode reward: 3.0 Average reward: 1.467128027681661 Loss: 0.00019419330055825412 Epsilon: 0.5320926663856692\n",
      "289/1000 Total steps: 56872 Episode reward: 1.0 Average reward: 1.4655172413793103 Loss: 3.826848114840686e-05 Epsilon: 0.5313123085266545\n",
      "290/1000 Total steps: 57087 Episode reward: 2.0 Average reward: 1.4673539518900343 Loss: 0.0001956648047780618 Epsilon: 0.5302785986995799\n",
      "291/1000 Total steps: 57290 Episode reward: 2.0 Average reward: 1.4691780821917808 Loss: 5.786743349744938e-05 Epsilon: 0.5293046220649756\n",
      "292/1000 Total steps: 57439 Episode reward: 0.0 Average reward: 1.4641638225255973 Loss: 2.232648512290325e-05 Epsilon: 0.5285909899660404\n",
      "293/1000 Total steps: 57659 Episode reward: 2.0 Average reward: 1.465986394557823 Loss: 0.0009066196507774293 Epsilon: 0.5275392471294382\n",
      "294/1000 Total steps: 57820 Episode reward: 1.0 Average reward: 1.464406779661017 Loss: 3.515883145155385e-05 Epsilon: 0.5267710275242838\n",
      "295/1000 Total steps: 57980 Episode reward: 1.0 Average reward: 1.462837837837838 Loss: 4.9292553740087897e-05 Epsilon: 0.5260088038218147\n",
      "296/1000 Total steps: 58232 Episode reward: 3.0 Average reward: 1.468013468013468 Loss: 0.00010651720367604867 Epsilon: 0.5248107717905417\n",
      "297/1000 Total steps: 58429 Episode reward: 1.0 Average reward: 1.4664429530201342 Loss: 5.700641486328095e-05 Epsilon: 0.5238763153119567\n",
      "298/1000 Total steps: 58557 Episode reward: 0.0 Average reward: 1.4615384615384615 Loss: 1.3922624930273741e-05 Epsilon: 0.523270141662256\n",
      "299/1000 Total steps: 58689 Episode reward: 0.0 Average reward: 1.4566666666666668 Loss: 4.337620339356363e-05 Epsilon: 0.5226458372068513\n",
      "300/1000 Total steps: 58899 Episode reward: 2.0 Average reward: 1.458471760797342 Loss: 0.0009636491304263473 Epsilon: 0.5216543224036421\n",
      "301/1000 Total steps: 59049 Episode reward: 0.0 Average reward: 1.4536423841059603 Loss: 0.00016387546202167869 Epsilon: 0.5209473712659433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302/1000 Total steps: 59318 Episode reward: 3.0 Average reward: 1.4587458745874586 Loss: 0.030636392533779144 Epsilon: 0.519682225221561\n",
      "303/1000 Total steps: 59467 Episode reward: 0.0 Average reward: 1.4539473684210527 Loss: 0.00029597265529446304 Epsilon: 0.518982919817884\n",
      "304/1000 Total steps: 59614 Episode reward: 0.0 Average reward: 1.4491803278688524 Loss: 5.994707316858694e-05 Epsilon: 0.5182940213901495\n",
      "305/1000 Total steps: 59972 Episode reward: 5.0 Average reward: 1.4607843137254901 Loss: 8.112948125926778e-05 Epsilon: 0.5166205261374234\n",
      "306/1000 Total steps: 60103 Episode reward: 0.0 Average reward: 1.4560260586319218 Loss: 0.015269852243363857 Epsilon: 0.5160096534571489\n",
      "307/1000 Total steps: 60315 Episode reward: 2.0 Average reward: 1.4577922077922079 Loss: 0.0005281114717945457 Epsilon: 0.5150227594690717\n",
      "308/1000 Total steps: 60487 Episode reward: 1.0 Average reward: 1.4563106796116505 Loss: 0.00011990615166723728 Epsilon: 0.5142236077902462\n",
      "309/1000 Total steps: 60618 Episode reward: 0.0 Average reward: 1.4516129032258065 Loss: 9.290096204495057e-05 Epsilon: 0.5136158730172284\n",
      "310/1000 Total steps: 60778 Episode reward: 1.0 Average reward: 1.45016077170418 Loss: 8.145294123096392e-05 Epsilon: 0.5128746807323498\n",
      "311/1000 Total steps: 61001 Episode reward: 2.0 Average reward: 1.4519230769230769 Loss: 0.001110207405872643 Epsilon: 0.51184362025403\n",
      "312/1000 Total steps: 61208 Episode reward: 2.0 Average reward: 1.4536741214057507 Loss: 7.328268111450598e-05 Epsilon: 0.5108885927545825\n",
      "313/1000 Total steps: 61384 Episode reward: 1.0 Average reward: 1.4522292993630572 Loss: 0.0003106338554061949 Epsilon: 0.5100781422369942\n",
      "314/1000 Total steps: 61538 Episode reward: 1.0 Average reward: 1.4507936507936507 Loss: 7.313424430321902e-05 Epsilon: 0.5093701671786636\n",
      "315/1000 Total steps: 61675 Episode reward: 0.0 Average reward: 1.4462025316455696 Loss: 0.0001050856735673733 Epsilon: 0.5087412609487625\n",
      "316/1000 Total steps: 61850 Episode reward: 1.0 Average reward: 1.444794952681388 Loss: 3.8097205106168985e-05 Epsilon: 0.5079391657800761\n",
      "317/1000 Total steps: 62123 Episode reward: 4.0 Average reward: 1.4528301886792452 Loss: 7.048507541185245e-05 Epsilon: 0.5066906967930566\n",
      "318/1000 Total steps: 62260 Episode reward: 0.0 Average reward: 1.4482758620689655 Loss: 0.0001222523715114221 Epsilon: 0.5060654589241826\n",
      "319/1000 Total steps: 62390 Episode reward: 0.0 Average reward: 1.44375 Loss: 3.933765401598066e-05 Epsilon: 0.5054729590359524\n",
      "320/1000 Total steps: 62576 Episode reward: 1.0 Average reward: 1.442367601246106 Loss: 0.00023683921608608216 Epsilon: 0.5046265667210132\n",
      "321/1000 Total steps: 62754 Episode reward: 1.0 Average reward: 1.4409937888198758 Loss: 6.815327651565894e-05 Epsilon: 0.5038180512245168\n",
      "322/1000 Total steps: 62963 Episode reward: 2.0 Average reward: 1.4427244582043344 Loss: 0.00018178064783569425 Epsilon: 0.5028705619686239\n",
      "323/1000 Total steps: 63160 Episode reward: 1.0 Average reward: 1.441358024691358 Loss: 0.0004981847596354783 Epsilon: 0.501979285157451\n",
      "324/1000 Total steps: 63382 Episode reward: 1.0 Average reward: 1.44 Loss: 0.0008271459955722094 Epsilon: 0.5009770040880254\n",
      "325/1000 Total steps: 63547 Episode reward: 1.0 Average reward: 1.4386503067484662 Loss: 0.015623332001268864 Epsilon: 0.5002335055862253\n",
      "326/1000 Total steps: 63746 Episode reward: 2.0 Average reward: 1.4403669724770642 Loss: 0.01575317233800888 Epsilon: 0.4993384318039039\n",
      "327/1000 Total steps: 63889 Episode reward: 0.0 Average reward: 1.4359756097560976 Loss: 3.862983430735767e-05 Epsilon: 0.4986963370535891\n",
      "328/1000 Total steps: 64065 Episode reward: 1.0 Average reward: 1.4346504559270516 Loss: 0.015311937779188156 Epsilon: 0.49790732603374227\n",
      "329/1000 Total steps: 64346 Episode reward: 3.0 Average reward: 1.4393939393939394 Loss: 4.3157964682905003e-05 Epsilon: 0.49665047315290456\n",
      "330/1000 Total steps: 64516 Episode reward: 1.0 Average reward: 1.4380664652567976 Loss: 0.00020151841454207897 Epsilon: 0.4958918123929015\n",
      "331/1000 Total steps: 64672 Episode reward: 1.0 Average reward: 1.4367469879518073 Loss: 6.614000449189916e-05 Epsilon: 0.49519676344470404\n",
      "332/1000 Total steps: 64908 Episode reward: 1.0 Average reward: 1.4354354354354355 Loss: 0.015523511916399002 Epsilon: 0.4941473378921999\n",
      "333/1000 Total steps: 65125 Episode reward: 2.0 Average reward: 1.437125748502994 Loss: 5.5653061281191185e-05 Epsilon: 0.49318458313567753\n",
      "334/1000 Total steps: 65402 Episode reward: 3.0 Average reward: 1.4417910447761193 Loss: 8.576387335779145e-05 Epsilon: 0.49195866052706966\n",
      "335/1000 Total steps: 65608 Episode reward: 2.0 Average reward: 1.443452380952381 Loss: 0.00018367893062531948 Epsilon: 0.4910491627906811\n",
      "336/1000 Total steps: 65743 Episode reward: 0.0 Average reward: 1.4391691394658754 Loss: 0.00014341597852762789 Epsilon: 0.49045414814616656\n",
      "337/1000 Total steps: 65907 Episode reward: 1.0 Average reward: 1.4378698224852071 Loss: 0.0005669149104505777 Epsilon: 0.48973239534227486\n",
      "338/1000 Total steps: 66084 Episode reward: 1.0 Average reward: 1.4365781710914454 Loss: 0.015321808867156506 Epsilon: 0.4889547574151064\n",
      "339/1000 Total steps: 66245 Episode reward: 1.0 Average reward: 1.4352941176470588 Loss: 0.030474791303277016 Epsilon: 0.4882486088577907\n",
      "340/1000 Total steps: 66430 Episode reward: 1.0 Average reward: 1.434017595307918 Loss: 0.0009638070478104055 Epsilon: 0.48743859842207854\n",
      "341/1000 Total steps: 66562 Episode reward: 0.0 Average reward: 1.4298245614035088 Loss: 0.0007032725843600929 Epsilon: 0.48686156040104117\n",
      "342/1000 Total steps: 66802 Episode reward: 2.0 Average reward: 1.4314868804664722 Loss: 0.03139742836356163 Epsilon: 0.4858143498114473\n",
      "343/1000 Total steps: 67028 Episode reward: 2.0 Average reward: 1.433139534883721 Loss: 0.0004875552258454263 Epsilon: 0.4848305215255865\n",
      "344/1000 Total steps: 67214 Episode reward: 1.0 Average reward: 1.4318840579710146 Loss: 0.00012489993241615593 Epsilon: 0.4840224884592565\n",
      "345/1000 Total steps: 67390 Episode reward: 1.0 Average reward: 1.430635838150289 Loss: 1.4666746210423298e-05 Epsilon: 0.4832592806994062\n",
      "346/1000 Total steps: 67547 Episode reward: 1.0 Average reward: 1.4293948126801153 Loss: 0.0018999413587152958 Epsilon: 0.4825795973197738\n",
      "347/1000 Total steps: 67728 Episode reward: 1.0 Average reward: 1.4281609195402298 Loss: 5.4083291615825146e-05 Epsilon: 0.48179733640831357\n",
      "348/1000 Total steps: 67993 Episode reward: 3.0 Average reward: 1.4326647564469914 Loss: 0.014843471348285675 Epsilon: 0.48065458827685137\n",
      "349/1000 Total steps: 68125 Episode reward: 0.0 Average reward: 1.4285714285714286 Loss: 0.00011532322969287634 Epsilon: 0.48008649924157576\n",
      "350/1000 Total steps: 68311 Episode reward: 1.0 Average reward: 1.4273504273504274 Loss: 1.956660344148986e-05 Epsilon: 0.4792872818555698\n",
      "351/1000 Total steps: 68468 Episode reward: 0.0 Average reward: 1.4232954545454546 Loss: 0.0002736393653322011 Epsilon: 0.4786138296213931\n",
      "352/1000 Total steps: 68641 Episode reward: 1.0 Average reward: 1.4220963172804533 Loss: 6.0109170590294525e-05 Epsilon: 0.4778729687255998\n",
      "353/1000 Total steps: 68897 Episode reward: 3.0 Average reward: 1.426553672316384 Loss: 0.0006816966342739761 Epsilon: 0.47677901478415197\n",
      "354/1000 Total steps: 69139 Episode reward: 2.0 Average reward: 1.4281690140845071 Loss: 3.1626434065401554e-05 Epsilon: 0.47574745825520826\n",
      "355/1000 Total steps: 69469 Episode reward: 4.0 Average reward: 1.4353932584269662 Loss: 0.00010621853289194405 Epsilon: 0.4743448072899642\n",
      "356/1000 Total steps: 69691 Episode reward: 2.0 Average reward: 1.4369747899159664 Loss: 0.0003227170091122389 Epsilon: 0.47340380671488774\n",
      "357/1000 Total steps: 69829 Episode reward: 0.0 Average reward: 1.4329608938547487 Loss: 2.1965059204376303e-05 Epsilon: 0.472819912441334\n",
      "358/1000 Total steps: 69961 Episode reward: 0.0 Average reward: 1.4289693593314763 Loss: 4.811581675312482e-05 Epsilon: 0.47226215835559393\n",
      "359/1000 Total steps: 70148 Episode reward: 1.0 Average reward: 1.4277777777777778 Loss: 0.0005638820002786815 Epsilon: 0.4714732659637451\n",
      "360/1000 Total steps: 70288 Episode reward: 0.0 Average reward: 1.4238227146814404 Loss: 0.0007263473235070705 Epsilon: 0.4708836162425102\n",
      "361/1000 Total steps: 70524 Episode reward: 3.0 Average reward: 1.4281767955801106 Loss: 0.015868715941905975 Epsilon: 0.4698915020633824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362/1000 Total steps: 70691 Episode reward: 1.0 Average reward: 1.4269972451790633 Loss: 0.00014020627713762224 Epsilon: 0.4691908684468395\n",
      "363/1000 Total steps: 70864 Episode reward: 1.0 Average reward: 1.4258241758241759 Loss: 0.0007776610436849296 Epsilon: 0.46846629518101596\n",
      "364/1000 Total steps: 71039 Episode reward: 1.0 Average reward: 1.4246575342465753 Loss: 1.6588328435318545e-05 Epsilon: 0.4677346195673409\n",
      "365/1000 Total steps: 71227 Episode reward: 1.0 Average reward: 1.4234972677595628 Loss: 0.00026355450972914696 Epsilon: 0.4669500162407731\n",
      "366/1000 Total steps: 71378 Episode reward: 0.0 Average reward: 1.4196185286103542 Loss: 0.031210558488965034 Epsilon: 0.46632089682094946\n",
      "367/1000 Total steps: 71640 Episode reward: 3.0 Average reward: 1.423913043478261 Loss: 0.0004244451702106744 Epsilon: 0.4652315637207745\n",
      "368/1000 Total steps: 71779 Episode reward: 0.0 Average reward: 1.4200542005420054 Loss: 0.00012562649499159306 Epsilon: 0.46465479279586036\n",
      "369/1000 Total steps: 71945 Episode reward: 1.0 Average reward: 1.4189189189189189 Loss: 0.00014582906442228705 Epsilon: 0.46396703683519824\n",
      "370/1000 Total steps: 72160 Episode reward: 1.0 Average reward: 1.417789757412399 Loss: 2.9908436772529967e-05 Epsilon: 0.4630779638019916\n",
      "371/1000 Total steps: 72361 Episode reward: 1.0 Average reward: 1.4166666666666667 Loss: 0.10936961323022842 Epsilon: 0.4622485109740977\n",
      "372/1000 Total steps: 72521 Episode reward: 1.0 Average reward: 1.415549597855228 Loss: 0.0010329863289371133 Epsilon: 0.46158944075331737\n",
      "373/1000 Total steps: 72766 Episode reward: 3.0 Average reward: 1.4197860962566844 Loss: 0.00018582350458018482 Epsilon: 0.4605822808980841\n",
      "374/1000 Total steps: 72999 Episode reward: 3.0 Average reward: 1.424 Loss: 0.01579020358622074 Epsilon: 0.459626737823569\n",
      "375/1000 Total steps: 73329 Episode reward: 5.0 Average reward: 1.4335106382978724 Loss: 0.00018416601233184338 Epsilon: 0.4582771975549021\n",
      "376/1000 Total steps: 73547 Episode reward: 1.0 Average reward: 1.4323607427055702 Loss: 8.271307160612196e-05 Epsilon: 0.4573881227079189\n",
      "377/1000 Total steps: 73734 Episode reward: 1.0 Average reward: 1.4312169312169312 Loss: 0.00016537368355784565 Epsilon: 0.4566270187724268\n",
      "378/1000 Total steps: 74118 Episode reward: 7.0 Average reward: 1.445910290237467 Loss: 0.0007799094310030341 Epsilon: 0.45506856516629174\n",
      "379/1000 Total steps: 74261 Episode reward: 0.0 Average reward: 1.4421052631578948 Loss: 0.00020830373978242278 Epsilon: 0.4544897310831116\n",
      "380/1000 Total steps: 74434 Episode reward: 0.0 Average reward: 1.4383202099737533 Loss: 0.0001181849220301956 Epsilon: 0.4537905687980912\n",
      "381/1000 Total steps: 74594 Episode reward: 0.0 Average reward: 1.4345549738219896 Loss: 0.0010013777064159513 Epsilon: 0.4531450204643982\n",
      "382/1000 Total steps: 74807 Episode reward: 1.0 Average reward: 1.433420365535248 Loss: 0.12527747452259064 Epsilon: 0.4522872354361711\n",
      "383/1000 Total steps: 74972 Episode reward: 0.0 Average reward: 1.4296875 Loss: 0.0005598913412541151 Epsilon: 0.4516240088101375\n",
      "384/1000 Total steps: 75202 Episode reward: 2.0 Average reward: 1.431168831168831 Loss: 5.913589848205447e-05 Epsilon: 0.45070133507141896\n",
      "385/1000 Total steps: 75491 Episode reward: 3.0 Average reward: 1.4352331606217616 Loss: 0.0007803540211170912 Epsilon: 0.44954497995104425\n",
      "386/1000 Total steps: 75719 Episode reward: 2.0 Average reward: 1.4366925064599483 Loss: 5.6627999583724886e-05 Epsilon: 0.4486350551052595\n",
      "387/1000 Total steps: 75967 Episode reward: 3.0 Average reward: 1.440721649484536 Loss: 0.00020704290363937616 Epsilon: 0.4476476650383517\n",
      "388/1000 Total steps: 76158 Episode reward: 2.0 Average reward: 1.442159383033419 Loss: 0.0005964330630376935 Epsilon: 0.44688888286577927\n",
      "389/1000 Total steps: 76288 Episode reward: 0.0 Average reward: 1.4384615384615385 Loss: 0.00015418618568219244 Epsilon: 0.44637326254387955\n",
      "390/1000 Total steps: 76419 Episode reward: 0.0 Average reward: 1.434782608695652 Loss: 6.990651309024543e-05 Epsilon: 0.4458543535295597\n",
      "391/1000 Total steps: 76664 Episode reward: 2.0 Average reward: 1.4362244897959184 Loss: 0.00016317940026056021 Epsilon: 0.44488569745163753\n",
      "392/1000 Total steps: 76929 Episode reward: 3.0 Average reward: 1.440203562340967 Loss: 0.03089185059070587 Epsilon: 0.44384063567182774\n",
      "393/1000 Total steps: 77092 Episode reward: 1.0 Average reward: 1.4390862944162437 Loss: 0.01643521711230278 Epsilon: 0.44319919834912025\n",
      "394/1000 Total steps: 77276 Episode reward: 1.0 Average reward: 1.4379746835443037 Loss: 5.984129529679194e-05 Epsilon: 0.44247637702370923\n",
      "395/1000 Total steps: 77457 Episode reward: 1.0 Average reward: 1.4368686868686869 Loss: 0.00015826169692445546 Epsilon: 0.4417666372895206\n",
      "396/1000 Total steps: 77614 Episode reward: 1.0 Average reward: 1.4357682619647356 Loss: 9.727560973260552e-05 Epsilon: 0.4411520462491848\n",
      "397/1000 Total steps: 77790 Episode reward: 1.0 Average reward: 1.434673366834171 Loss: 1.8298023860552348e-05 Epsilon: 0.44046422410881964\n",
      "398/1000 Total steps: 77939 Episode reward: 0.0 Average reward: 1.431077694235589 Loss: 3.659976937342435e-05 Epsilon: 0.43988286563451695\n",
      "399/1000 Total steps: 78295 Episode reward: 5.0 Average reward: 1.44 Loss: 0.000825270137283951 Epsilon: 0.43849735031341824\n",
      "400/1000 Total steps: 78445 Episode reward: 0.0 Average reward: 1.4364089775561097 Loss: 0.0008286935626529157 Epsilon: 0.4379150411290194\n",
      "401/1000 Total steps: 78630 Episode reward: 1.0 Average reward: 1.435323383084577 Loss: 0.0007287879125215113 Epsilon: 0.43719806171337866\n",
      "402/1000 Total steps: 78949 Episode reward: 4.0 Average reward: 1.4416873449131513 Loss: 0.015954071655869484 Epsilon: 0.43596486788642547\n",
      "403/1000 Total steps: 79160 Episode reward: 2.0 Average reward: 1.443069306930693 Loss: 9.953604603651911e-05 Epsilon: 0.43515134058831\n",
      "404/1000 Total steps: 79308 Episode reward: 0.0 Average reward: 1.4395061728395062 Loss: 0.015290237963199615 Epsilon: 0.4345817382139677\n",
      "405/1000 Total steps: 79469 Episode reward: 1.0 Average reward: 1.438423645320197 Loss: 2.9009348509134725e-05 Epsilon: 0.43396305978521804\n",
      "406/1000 Total steps: 79726 Episode reward: 2.0 Average reward: 1.4398034398034398 Loss: 7.739294233033434e-05 Epsilon: 0.432977541654805\n",
      "407/1000 Total steps: 79895 Episode reward: 1.0 Average reward: 1.4387254901960784 Loss: 0.015184316784143448 Epsilon: 0.4323308562125236\n",
      "408/1000 Total steps: 80276 Episode reward: 5.0 Average reward: 1.4474327628361858 Loss: 0.00020159893028903753 Epsilon: 0.4308769471059589\n",
      "409/1000 Total steps: 80443 Episode reward: 1.0 Average reward: 1.446341463414634 Loss: 6.511608080472797e-05 Epsilon: 0.43024141342262096\n",
      "410/1000 Total steps: 80598 Episode reward: 0.0 Average reward: 1.4428223844282237 Loss: 1.9402848920435645e-05 Epsilon: 0.4296524957609099\n",
      "411/1000 Total steps: 80807 Episode reward: 2.0 Average reward: 1.4441747572815533 Loss: 7.91452985140495e-05 Epsilon: 0.4288598506474426\n",
      "412/1000 Total steps: 80984 Episode reward: 1.0 Average reward: 1.4430992736077481 Loss: 0.0001337199646513909 Epsilon: 0.4281898618268202\n",
      "413/1000 Total steps: 81135 Episode reward: 0.0 Average reward: 1.4396135265700483 Loss: 0.00016481446800753474 Epsilon: 0.42761922607388064\n",
      "414/1000 Total steps: 81340 Episode reward: 1.0 Average reward: 1.4385542168674699 Loss: 5.341965879779309e-05 Epsilon: 0.42684589959089964\n",
      "415/1000 Total steps: 81573 Episode reward: 2.0 Average reward: 1.4399038461538463 Loss: 0.00010321758600184694 Epsilon: 0.42596887078019247\n",
      "416/1000 Total steps: 81728 Episode reward: 0.0 Average reward: 1.4364508393285371 Loss: 4.199149043415673e-05 Epsilon: 0.425386570429836\n",
      "417/1000 Total steps: 81920 Episode reward: 1.0 Average reward: 1.4354066985645932 Loss: 0.015722503885626793 Epsilon: 0.42466651968452584\n",
      "418/1000 Total steps: 82063 Episode reward: 0.0 Average reward: 1.431980906921241 Loss: 0.00015928206266835332 Epsilon: 0.42413112945662484\n",
      "419/1000 Total steps: 82369 Episode reward: 4.0 Average reward: 1.438095238095238 Loss: 0.015557355247437954 Epsilon: 0.42298803802233614\n",
      "420/1000 Total steps: 82590 Episode reward: 1.0 Average reward: 1.4370546318289785 Loss: 0.0009168793330900371 Epsilon: 0.42216464464311876\n",
      "421/1000 Total steps: 82823 Episode reward: 2.0 Average reward: 1.438388625592417 Loss: 0.01493785995990038 Epsilon: 0.42129851045927075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/1000 Total steps: 83001 Episode reward: 1.0 Average reward: 1.4373522458628842 Loss: 0.00045226470683701336 Epsilon: 0.42063818697290356\n",
      "423/1000 Total steps: 83165 Episode reward: 0.0 Average reward: 1.4339622641509433 Loss: 0.00010023333743447438 Epsilon: 0.4200308385081361\n",
      "424/1000 Total steps: 83498 Episode reward: 4.0 Average reward: 1.44 Loss: 0.00014970141637604684 Epsilon: 0.41880068515798574\n",
      "425/1000 Total steps: 83708 Episode reward: 2.0 Average reward: 1.4413145539906103 Loss: 0.000366375083103776 Epsilon: 0.41802701635571965\n",
      "426/1000 Total steps: 83891 Episode reward: 1.0 Average reward: 1.440281030444965 Loss: 0.01598259061574936 Epsilon: 0.41735414278288996\n",
      "427/1000 Total steps: 84027 Episode reward: 0.0 Average reward: 1.4369158878504673 Loss: 3.036432826775126e-05 Epsilon: 0.41685488072385835\n",
      "428/1000 Total steps: 84204 Episode reward: 1.0 Average reward: 1.435897435897436 Loss: 3.5496021155267954e-05 Epsilon: 0.41620612190590567\n",
      "429/1000 Total steps: 84337 Episode reward: 0.0 Average reward: 1.4325581395348836 Loss: 3.0521485314238816e-05 Epsilon: 0.41571939151123144\n",
      "430/1000 Total steps: 84614 Episode reward: 3.0 Average reward: 1.4361948955916473 Loss: 0.015018854290246964 Epsilon: 0.4147077505663056\n",
      "431/1000 Total steps: 84773 Episode reward: 0.0 Average reward: 1.4328703703703705 Loss: 0.0009299020748585463 Epsilon: 0.41412832600749966\n",
      "432/1000 Total steps: 84932 Episode reward: 1.0 Average reward: 1.4318706697459584 Loss: 7.813765114406124e-05 Epsilon: 0.4135498220017087\n",
      "433/1000 Total steps: 85138 Episode reward: 2.0 Average reward: 1.433179723502304 Loss: 0.00036788475699722767 Epsilon: 0.4128016802189892\n",
      "434/1000 Total steps: 85337 Episode reward: 1.0 Average reward: 1.432183908045977 Loss: 0.0010016713058575988 Epsilon: 0.41208042276454154\n",
      "435/1000 Total steps: 85510 Episode reward: 0.0 Average reward: 1.4288990825688073 Loss: 0.015213591046631336 Epsilon: 0.41145456515608436\n",
      "436/1000 Total steps: 85719 Episode reward: 2.0 Average reward: 1.4302059496567505 Loss: 0.0001948210410773754 Epsilon: 0.41069991400006545\n",
      "437/1000 Total steps: 85920 Episode reward: 1.0 Average reward: 1.4292237442922375 Loss: 2.0863075405941345e-05 Epsilon: 0.40997563531684844\n",
      "438/1000 Total steps: 86057 Episode reward: 0.0 Average reward: 1.4259681093394077 Loss: 0.0006330391624942422 Epsilon: 0.40948280636138146\n",
      "439/1000 Total steps: 86290 Episode reward: 2.0 Average reward: 1.4272727272727272 Loss: 3.83786900783889e-05 Epsilon: 0.4086461864632345\n",
      "440/1000 Total steps: 86433 Episode reward: 0.0 Average reward: 1.4240362811791383 Loss: 0.00022402314061764628 Epsilon: 0.4081336889396553\n",
      "441/1000 Total steps: 86565 Episode reward: 0.0 Average reward: 1.420814479638009 Loss: 0.00024621968623250723 Epsilon: 0.40766126433908734\n",
      "442/1000 Total steps: 86771 Episode reward: 2.0 Average reward: 1.4221218961625282 Loss: 0.015445212833583355 Epsilon: 0.4069252404993863\n",
      "443/1000 Total steps: 87009 Episode reward: 2.0 Average reward: 1.4234234234234233 Loss: 0.00016728139598853886 Epsilon: 0.40607676850917307\n"
     ]
    }
   ],
   "source": [
    "target_net = BreakoutDQN(num_actions, 84, 84).to(device)\n",
    "policy_net = BreakoutDQN(num_actions, 84, 84).to(device)\n",
    "\n",
    "#policy_net.load_state_dict(target_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters())\n",
    "\n",
    "memory = ReplayBuffer(100000)\n",
    "try:\n",
    "    train_dqn(env, policy_net, target_net, optimizer, memory, gamma=0.9, batch_size=32, episodes=1000, epsilon_steps=100000, epsilon_end=0.05)\n",
    "finally:\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
