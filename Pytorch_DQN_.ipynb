{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch DQN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMqnCT/lIHPuaY8BTKUQJaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandropacielli/autonomous_systems_project/blob/master/Pytorch_DQN_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYQV0uUZZnsk",
        "colab_type": "text"
      },
      "source": [
        "Install openai baselines for frame stacks and other goodies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPts_QP65B5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m pip install git+https://github.com/openai/baselines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL-UO54fZx3X",
        "colab_type": "text"
      },
      "source": [
        "Install dependencies for virtual display to render the environment\n",
        "\n",
        "See [this medium post.](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymmGP3TuZ3B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This installs a \n",
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "!pip install pyvirtualdisplay==0.2.* \\\n",
        "             PyOpenGL==3.1.* \\\n",
        "             PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "# This starts the display\n",
        "import pyvirtualdisplay\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cedZKEAZDast",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from baselines.common.atari_wrappers import FrameStack\n",
        "from collections import namedtuple\n",
        "import gym\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFeNLFFuDims",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = 4\n",
        "env_name = 'Breakout-v0'\n",
        "env = FrameStack(gym.make(env_name), frames)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ju6R5xsI3ta",
        "colab_type": "text"
      },
      "source": [
        "# Replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoyJzxuwI7Bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'reward', 'next_state'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX470chbIr7o",
        "colab_type": "text"
      },
      "source": [
        "# DQN module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmuSz5GhEhMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    \n",
        "    def __init__(self, h, w, outputs, frame_stack=4, rgb=False):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        Construct a new DQN object.\n",
        "        \n",
        "        :param h: The height of the image.\n",
        "        :param w: The width of the image.\n",
        "        :param outputs: The number of outputs.\n",
        "        \"\"\"\n",
        "\n",
        "        if rgb:\n",
        "          color_channels = 3\n",
        "        else:\n",
        "          color_channels = 1\n",
        "        \n",
        "        self.input_channels = color_channels * frame_stack\n",
        "\n",
        "        self.conv1 = nn.Conv2d(self.input_channels, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        # (Size - Kernel size + 2 * Padding) // Stride --> see https://cs231n.github.io/convolutional-networks/\n",
        "        def conv2d_size_out(size, kernel=5, stride=2):\n",
        "            return (size - kernel) // stride + 1\n",
        "        \n",
        "        # Compute convolution output dimensions\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        \n",
        "        # Conv output width * conv output height * conv output channels\n",
        "        self.linear_input_size = convw * convh * 32\n",
        "                 \n",
        "        # A fully connected layer for the output\n",
        "        self.head = nn.Linear(self.linear_input_size, outputs)\n",
        "        \n",
        "    # NN forward pass    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x))) # TODO should we use maxpooling? or any other pooling?\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dajfV_DTJdKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNTraining():\n",
        "\n",
        "  def __init__(self, env, batch_size=128, gamma=0.999, \n",
        "                 eps_start=0.9, eps_end=0.05, eps_decay=200, \n",
        "                 update_every=10, memory_size=10000):\n",
        "    \"\"\"\n",
        "    The training support object has two DQNs: policy and target, see \n",
        "    https://greentec.github.io/reinforcement-learning-third-en/#soft-update-target-network\n",
        "\n",
        "    :param env: gym environment\n",
        "    :param batch_size: how many transitions are sampled from the replay memory for training.\n",
        "    :param gamma: discount.\n",
        "    :param eps_start: initial exploration rate (for epsilon-greedy policy).\n",
        "    :param eps_decay: controls the rate of decay (for epsilon-greedy policy).\n",
        "    :param eps_end: final exploration rate (for epsilon-greedy policy).\n",
        "    :param update_every: how often the target net is updated with weights from the policy network.\n",
        "    :param memory_size: replay buffer size\n",
        "    \"\"\"\n",
        "\n",
        "    self.env = env\n",
        "    self.batch_size = batch_size\n",
        "    self.gamma = gamma\n",
        "    self.eps_start = eps_start\n",
        "    self.eps_end = eps_end\n",
        "    self.eps_decay = eps_decay\n",
        "    self.update_every = update_every\n",
        "\n",
        "    # Might be useful later on\n",
        "    self.n_actions = env.action_space.n\n",
        "\n",
        "    # State dimensions\n",
        "    self.state_h = env.observation_space.shape[0]\n",
        "    self.state_w = env.observation_space.shape[1]\n",
        "\n",
        "    # Policy & target nets\n",
        "    self.policy_network = DQN(self.state_h, self.state_w, self.n_actions, rgb=True).to(device)\n",
        "    self.target_network = DQN(self.state_h, self.state_w, self.n_actions, rgb=True).to(device)\n",
        "    self.target_network.load_state_dict(self.policy_network.state_dict())\n",
        "    self.target_network.eval()\n",
        "\n",
        "    # Optimizer\n",
        "    self.optimizer = optim.RMSprop(self.policy_network.parameters())\n",
        "\n",
        "    # Replay memory\n",
        "    self.memory = ReplayMemory(memory_size)\n",
        "\n",
        "    self.total_steps = 0\n",
        "\n",
        "  def select_action(self, state):\n",
        "    sample = random.random()\n",
        "    eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\n",
        "                    math.exp(-1. * self.total_steps / self.eps_decay)\n",
        "    self.total_steps += 1\n",
        "    \n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return self.policy_network(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "  def optimize_model(self):\n",
        "\n",
        "    if len(self.memory) < self.batch_size:\n",
        "        return\n",
        "    \n",
        "    # Optimization steps sample the replay buffer\n",
        "    transitions = self.memory.sample(self.batch_size)\n",
        "    \n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Convert arrays to tensors\n",
        "    state_batch = torch.cat(batch.state).view(self.batch_size, 12, self.state_h, self.state_w).float()\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.tensor(batch.reward, device=device)\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = [s for s in batch.next_state if s is not None]\n",
        "    non_final_len = torch.sum(non_final_mask.long())\n",
        "    non_final_next_states = torch.cat(non_final_next_states).view(non_final_len, 12, self.state_h, self.state_w).float()\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = self.policy_network(state_batch).gather(1, action_batch)[non_final_mask]\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(self.batch_size, device=device)\n",
        "    next_state_values = self.target_network(non_final_next_states).max(1)[0].detach()\n",
        "    \n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * self.gamma) + reward_batch[non_final_mask]\n",
        "    \n",
        "    # Compute Huber loss\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    \n",
        "    # Optimize the model\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in self.policy_network.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    self.optimizer.step()\n",
        "\n",
        "  def training_loop(self, num_episodes, render=False):\n",
        "    if render:\n",
        "      _, ax = plt.subplots(1, 1)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "      print('Episode: %d/%d' % (episode+1, num_episodes))\n",
        "      # Reset env\n",
        "      state = torch.tensor(env.reset(), device=device)\n",
        "      done = False\n",
        "      if render:\n",
        "        img = ax.imshow(env.render(mode='rgb_array'))      \n",
        "      while not done:\n",
        "\n",
        "        if render:\n",
        "          img.set_data(env.render(mode='rgb_array')) \n",
        "          ax.axis('off')\n",
        "          display.display(plt.gcf())\n",
        "          display.clear_output(wait=True)\n",
        "        \n",
        "        # Let agent perform an action according to an eps-greedy policy\n",
        "        action = self.select_action(state.view((1, 12, self.state_h, self.state_w)).float()) # TODO avoid explicit reference to frame stack\n",
        "        \n",
        "        # Take step\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "        # Convert to tensors\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        \n",
        "        if not done:\n",
        "          next_state = torch.tensor(next_state._force(), device=device)\n",
        "        else:\n",
        "          next_state = None\n",
        "        \n",
        "        # Remember transition\n",
        "        self.memory.push(state, action, reward, next_state)\n",
        "\n",
        "        # Run optimization step\n",
        "        self.optimize_model()\n",
        "\n",
        "        # Update state \n",
        "        if not done:\n",
        "          state = torch.tensor(next_state, device=device)\n",
        "        \n",
        "      if episode % self.update_every == 0:\n",
        "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1nG8TutLRIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent = DQNTraining(env)\n",
        "agent.training_loop(100, render=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpmUg02wUtks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO checkpointing\n",
        "torch.save({\n",
        "            'model_state_dict': agent.policy_network.state_dict(),\n",
        "            'optimizer_state_dict': agent.optimizer.state_dict(),\n",
        "            }, 'policy.pkl')\n",
        "\n",
        "torch.save({\n",
        "            'model_state_dict': agent.target_network.state_dict(),\n",
        "            }, 'target.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNj86NaOWQUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "old_agent = DQNTraining(env, eps_start=0.05)\n",
        "policy_checkpoint = torch.load('policy.pkl')\n",
        "target_checkpoint = torch.load('target.pkl')\n",
        "old_agent.policy_network.load_state_dict(policy_checkpoint['model_state_dict'])\n",
        "old_agent.target_network.load_state_dict(target_checkpoint['model_state_dict'])\n",
        "old_agent.optimizer.load_state_dict(policy_checkpoint['optimizer_state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADm8cc0ZXRiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "a6a439c7-c743-482a-e404-a0c07bb4f723"
      },
      "source": [
        "old_agent.training_loop(1, render=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}